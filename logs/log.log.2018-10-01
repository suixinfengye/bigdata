[INFO ] 2018-10-01 22:05:20,119 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:05:20,751 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:05:20,900 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:05:20,900 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:05:20,933 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:05:20,952 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:05:20,953 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:05:20,953 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:05:20,954 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:05:20,954 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:05:21,286 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 35035.
[INFO ] 2018-10-01 22:05:21,331 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:05:21,354 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:05:21,357 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:05:21,357 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:05:21,372 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-95f374c5-fd41-456a-b857-ac1a424daba5
[INFO ] 2018-10-01 22:05:21,420 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:05:21,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:05:21,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:05:21,732 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:05:21,856 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:05:21,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43959.
[INFO ] 2018-10-01 22:05:21,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:43959
[INFO ] 2018-10-01 22:05:21,883 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:05:21,886 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 43959, None)
[INFO ] 2018-10-01 22:05:21,890 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:43959 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 43959, None)
[INFO ] 2018-10-01 22:05:21,893 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 43959, None)
[INFO ] 2018-10-01 22:05:21,893 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 43959, None)
[INFO ] 2018-10-01 22:05:22,408 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:05:22,417 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:05:23,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:05:23,287 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:05:23,556 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:05:23,557 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:05:23,557 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:05:23,557 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:05:23,563 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[ERROR] 2018-10-01 22:05:23,565 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error starting the context, marking it as stopped
java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.streaming.DStreamGraph.validate(DStreamGraph.scala:168)
	at org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:513)
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at spark.dataProcess.StoreMovieEssay$.main(StoreMovieEssay.scala:62)
	at spark.dataProcess.StoreMovieEssay.main(StoreMovieEssay.scala)
[INFO ] 2018-10-01 22:05:23,585 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 22:05:23,594 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:05:23,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 22:05:23,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 22:05:23,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 22:05:23,641 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 22:05:23,650 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 22:05:23,652 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 22:05:23,652 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 22:05:23,653 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-993ddd6f-e4a5-4410-b2b4-e430e079ac63
[INFO ] 2018-10-01 22:14:03,549 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:14:03,980 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:14:04,104 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:14:04,104 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:14:04,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:14:04,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:14:04,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:14:04,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:14:04,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:14:04,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:14:04,481 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 42445.
[INFO ] 2018-10-01 22:14:04,500 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:14:04,513 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:14:04,515 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:14:04,515 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:14:04,527 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-27890d95-8e25-419f-9465-88f414bdce3e
[INFO ] 2018-10-01 22:14:04,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:14:04,616 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:14:04,760 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:14:04,811 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:14:04,892 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:14:04,915 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38459.
[INFO ] 2018-10-01 22:14:04,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:38459
[INFO ] 2018-10-01 22:14:04,917 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:14:04,918 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 38459, None)
[INFO ] 2018-10-01 22:14:04,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:38459 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 38459, None)
[INFO ] 2018-10-01 22:14:04,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 38459, None)
[INFO ] 2018-10-01 22:14:04,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 38459, None)
[INFO ] 2018-10-01 22:14:05,168 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:14:05,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:14:05,816 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:14:05,826 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:14:06,040 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:14:06,041 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:14:06,041 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:14:06,042 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:14:06,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:14:06,448 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:14:06,471 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:14:06,472 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:14:06,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:14:06,475 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:14:06,475 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4f9a6c2d
[INFO ] 2018-10-01 22:14:06,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:14:06,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:14:06,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:14:06,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:14:06,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@529c2a9a
[INFO ] 2018-10-01 22:14:06,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:14:06,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:14:06,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:14:06,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:14:06,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@f88bfbe
[INFO ] 2018-10-01 22:14:06,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:14:06,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:14:06,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:14:06,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:14:06,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6ae62c7e
[INFO ] 2018-10-01 22:14:06,558 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:14:06,619 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:14:06,637 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:14:06,638 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:14:06,770 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:14:06,771 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:14:06,772 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:14:06,885 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 1
[INFO ] 2018-10-01 22:14:06,886 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:14:06,926 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538403250000
[INFO ] 2018-10-01 22:14:06,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538403250000 ms
[INFO ] 2018-10-01 22:14:06,928 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:14:06,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:14:10,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403250000 ms
[INFO ] 2018-10-01 22:14:10,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403250000 ms.0 from job set of time 1538403250000 ms
[INFO ] 2018-10-01 22:14:10,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403250000 ms
[INFO ] 2018-10-01 22:14:10,116 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403250000 ms
[INFO ] 2018-10-01 22:14:10,134 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403250000 ms
[INFO ] 2018-10-01 22:14:10,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403250000 ms to writer queue
[INFO ] 2018-10-01 22:14:10,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403250000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403250000'
[INFO ] 2018-10-01 22:14:10,145 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:10,186 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:10,187 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:10,187 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:10,197 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:10,204 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:10,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:10,328 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2014.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:10,330 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:38459 (size: 2014.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:10,340 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:10,371 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:10,372 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:14:10,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:10,434 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000.bk
[INFO ] 2018-10-01 22:14:10,435 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403250000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403250000', took 4658 bytes and 293 ms
[INFO ] 2018-10-01 22:14:10,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:14:10,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:10,520 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:10,527 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 110 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:10,528 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:10,532 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (reduce at StoreMovieEssay.scala:74) finished in 0.143 s
[INFO ] 2018-10-01 22:14:10,536 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: reduce at StoreMovieEssay.scala:74, took 0.390721 s
[INFO ] 2018-10-01 22:14:10,542 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403250000 ms.0 from job set of time 1538403250000 ms
[ERROR] 2018-10-01 22:14:10,543 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403250000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:10,611 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:38459 in memory (size: 2014.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:15,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403255000 ms.0 from job set of time 1538403255000 ms
[INFO ] 2018-10-01 22:14:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:15,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403255000 ms
[INFO ] 2018-10-01 22:14:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403255000 ms
[INFO ] 2018-10-01 22:14:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403255000 ms
[INFO ] 2018-10-01 22:14:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403255000 ms
[INFO ] 2018-10-01 22:14:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403255000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403255000'
[INFO ] 2018-10-01 22:14:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2015.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:15,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:38459 (size: 2015.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:15,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:15,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403255000 ms to writer queue
[INFO ] 2018-10-01 22:14:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:14:15,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:15,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:14:15,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:15,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:15,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:15,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:15,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:74) finished in 0.011 s
[INFO ] 2018-10-01 22:14:15,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:74, took 0.035338 s
[INFO ] 2018-10-01 22:14:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403255000 ms.0 from job set of time 1538403255000 ms
[ERROR] 2018-10-01 22:14:15,069 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403255000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:15,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000
[INFO ] 2018-10-01 22:14:15,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403255000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403255000', took 4695 bytes and 30 ms
[INFO ] 2018-10-01 22:14:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403260000 ms.0 from job set of time 1538403260000 ms
[INFO ] 2018-10-01 22:14:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403260000 ms
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403260000 ms
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403260000 ms
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403260000 ms
[INFO ] 2018-10-01 22:14:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403260000 ms to writer queue
[INFO ] 2018-10-01 22:14:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403260000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403260000'
[INFO ] 2018-10-01 22:14:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2015.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:38459 (size: 2015.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:14:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 22:14:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000.bk
[INFO ] 2018-10-01 22:14:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403260000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403260000', took 4710 bytes and 15 ms
[INFO ] 2018-10-01 22:14:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (reduce at StoreMovieEssay.scala:74) finished in 0.008 s
[INFO ] 2018-10-01 22:14:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: reduce at StoreMovieEssay.scala:74, took 0.023658 s
[INFO ] 2018-10-01 22:14:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403260000 ms.0 from job set of time 1538403260000 ms
[ERROR] 2018-10-01 22:14:20,044 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403260000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:25,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403265000 ms.0 from job set of time 1538403265000 ms
[INFO ] 2018-10-01 22:14:25,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403265000 ms
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403265000 ms
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403265000 ms
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403265000 ms
[INFO ] 2018-10-01 22:14:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:25,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403265000 ms to writer queue
[INFO ] 2018-10-01 22:14:25,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403265000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403265000'
[INFO ] 2018-10-01 22:14:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:25,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2015.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:25,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:38459 (size: 2015.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:25,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:25,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:25,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:14:25,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:25,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:14:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:25,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (reduce at StoreMovieEssay.scala:74) finished in 0.008 s
[INFO ] 2018-10-01 22:14:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: reduce at StoreMovieEssay.scala:74, took 0.020405 s
[INFO ] 2018-10-01 22:14:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403265000 ms.0 from job set of time 1538403265000 ms
[ERROR] 2018-10-01 22:14:25,039 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403265000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:25,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000
[INFO ] 2018-10-01 22:14:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403265000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403265000', took 4719 bytes and 22 ms
[INFO ] 2018-10-01 22:14:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403270000 ms.0 from job set of time 1538403270000 ms
[INFO ] 2018-10-01 22:14:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:30,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2015.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403270000 ms
[INFO ] 2018-10-01 22:14:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403270000 ms
[INFO ] 2018-10-01 22:14:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403270000 ms
[INFO ] 2018-10-01 22:14:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:38459 (size: 2015.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 22:14:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403270000 ms
[INFO ] 2018-10-01 22:14:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:30,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 22:14:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403270000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403270000'
[INFO ] 2018-10-01 22:14:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (reduce at StoreMovieEssay.scala:74) finished in 0.006 s
[INFO ] 2018-10-01 22:14:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403270000 ms to writer queue
[INFO ] 2018-10-01 22:14:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: reduce at StoreMovieEssay.scala:74, took 0.016597 s
[INFO ] 2018-10-01 22:14:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403270000 ms.0 from job set of time 1538403270000 ms
[ERROR] 2018-10-01 22:14:30,037 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403270000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:30,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000.bk
[INFO ] 2018-10-01 22:14:30,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403270000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403270000', took 4728 bytes and 20 ms
[INFO ] 2018-10-01 22:14:35,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403275000 ms.0 from job set of time 1538403275000 ms
[INFO ] 2018-10-01 22:14:35,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:14:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403275000 ms
[INFO ] 2018-10-01 22:14:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403275000 ms
[INFO ] 2018-10-01 22:14:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403275000 ms
[INFO ] 2018-10-01 22:14:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403275000 ms
[INFO ] 2018-10-01 22:14:35,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403275000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403275000'
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:72), which has no missing parents
[INFO ] 2018-10-01 22:14:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403275000 ms to writer queue
[INFO ] 2018-10-01 22:14:35,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:35,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2014.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:14:35,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:38459 (size: 2014.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:14:35,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:14:35,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:72) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:14:35,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-10-01 22:14:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:14:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-10-01 22:14:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:14:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000
[INFO ] 2018-10-01 22:14:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403275000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403275000', took 4750 bytes and 14 ms
[INFO ] 2018-10-01 22:14:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:14:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:14:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:14:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (reduce at StoreMovieEssay.scala:74) finished in 0.009 s
[INFO ] 2018-10-01 22:14:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: reduce at StoreMovieEssay.scala:74, took 0.024734 s
[INFO ] 2018-10-01 22:14:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403275000 ms.0 from job set of time 1538403275000 ms
[ERROR] 2018-10-01 22:14:35,038 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403275000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:14:35,563 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:14:35,565 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538403275565.
[WARN ] 2018-10-01 22:14:35,566 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:14:35,567 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:14:35,567 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:14:35,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:14:35,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:14:35,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:14:35,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:18:16,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:18:16,535 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:18:16,649 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:18:16,650 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:18:16,682 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:18:16,697 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:18:16,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:18:16,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:18:16,699 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:18:16,699 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:18:16,962 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 40445.
[INFO ] 2018-10-01 22:18:16,985 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:18:17,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:18:17,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:18:17,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:18:17,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-8c03b723-9b68-47bd-8673-dea10c24fcdc
[INFO ] 2018-10-01 22:18:17,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:18:17,198 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:18:17,416 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:18:17,469 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:18:17,564 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:18:17,590 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42767.
[INFO ] 2018-10-01 22:18:17,590 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:42767
[INFO ] 2018-10-01 22:18:17,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:18:17,592 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 42767, None)
[INFO ] 2018-10-01 22:18:17,595 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:42767 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 42767, None)
[INFO ] 2018-10-01 22:18:17,598 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 42767, None)
[INFO ] 2018-10-01 22:18:17,598 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 42767, None)
[INFO ] 2018-10-01 22:18:17,791 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:18:17,791 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:18:18,358 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:18:18,368 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:18:18,592 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:18:18,593 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:18:18,593 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:18:18,594 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:18:18,599 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:18:18,763 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:18:18,773 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:18:18,774 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:18:18,775 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:18:18,775 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:18:18,776 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c87fdf2
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:18:18,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:18:18,778 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-01 22:18:18,831 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:18:18,927 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:18:18,952 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:18:18,952 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:18:19,088 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:18:19,089 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:18:19,090 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:18:19,102 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 3
[INFO ] 2018-10-01 22:18:19,103 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:18:19,113 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538403500000
[INFO ] 2018-10-01 22:18:19,114 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538403500000 ms
[INFO ] 2018-10-01 22:18:19,114 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:18:19,119 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:18:20,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403500000 ms
[INFO ] 2018-10-01 22:18:20,575 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403500000 ms
[INFO ] 2018-10-01 22:18:20,576 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403500000 ms
[INFO ] 2018-10-01 22:18:20,577 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403500000 ms.0 from job set of time 1538403500000 ms
[INFO ] 2018-10-01 22:18:20,584 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403500000 ms
[INFO ] 2018-10-01 22:18:20,601 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:18:20,604 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403500000 ms to writer queue
[INFO ] 2018-10-01 22:18:20,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403500000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403500000'
[INFO ] 2018-10-01 22:18:20,615 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:18:20,615 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:18:20,616 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:18:20,618 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:18:20,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:74), which has no missing parents
[INFO ] 2018-10-01 22:18:20,683 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000.bk
[INFO ] 2018-10-01 22:18:20,684 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403500000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403500000', took 4659 bytes and 79 ms
[INFO ] 2018-10-01 22:18:20,705 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:20,716 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2046.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:20,718 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:42767 (size: 2046.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:18:20,721 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:18:20,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:18:20,737 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:18:20,771 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:18:20,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:18:20,854 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:18:20,867 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:18:20,877 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:18:20,879 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:18:20,888 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (reduce at StoreMovieEssay.scala:74) finished in 0.139 s
[INFO ] 2018-10-01 22:18:20,898 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: reduce at StoreMovieEssay.scala:74, took 0.297134 s
[INFO ] 2018-10-01 22:18:20,903 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403500000 ms.0 from job set of time 1538403500000 ms
[ERROR] 2018-10-01 22:18:20,905 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403500000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:18:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403505000 ms
[INFO ] 2018-10-01 22:18:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403505000 ms
[INFO ] 2018-10-01 22:18:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403505000 ms
[INFO ] 2018-10-01 22:18:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403505000 ms
[INFO ] 2018-10-01 22:18:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403505000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403505000'
[INFO ] 2018-10-01 22:18:25,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403505000 ms to writer queue
[INFO ] 2018-10-01 22:18:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403505000 ms.0 from job set of time 1538403505000 ms
[INFO ] 2018-10-01 22:18:25,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:18:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:18:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:18:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:18:25,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:18:25,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[7] at filter at StoreMovieEssay.scala:74), which has no missing parents
[INFO ] 2018-10-01 22:18:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:42767 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:18:25,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:18:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at filter at StoreMovieEssay.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:18:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:18:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:18:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:18:25,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:18:25,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:42767 in memory (size: 2046.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:18:25,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 766 bytes result sent to driver
[INFO ] 2018-10-01 22:18:25,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:18:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000
[INFO ] 2018-10-01 22:18:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:74) finished in 0.013 s
[INFO ] 2018-10-01 22:18:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:18:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403505000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403505000', took 4697 bytes and 47 ms
[INFO ] 2018-10-01 22:18:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:74, took 0.043516 s
[INFO ] 2018-10-01 22:18:25,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403505000 ms.0 from job set of time 1538403505000 ms
[ERROR] 2018-10-01 22:18:25,074 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403505000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:18:25,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:18:25,883 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538403505883.
[WARN ] 2018-10-01 22:18:25,884 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:18:25,885 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:18:25,885 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:18:25,886 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:18:25,887 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:18:25,887 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:18:25,888 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:18:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403510000 ms.0 from job set of time 1538403510000 ms
[INFO ] 2018-10-01 22:18:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403510000 ms
[INFO ] 2018-10-01 22:18:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403510000 ms
[INFO ] 2018-10-01 22:18:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403510000 ms
[INFO ] 2018-10-01 22:18:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403510000 ms
[INFO ] 2018-10-01 22:18:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:18:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:18:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:18:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:18:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:18:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:74), which has no missing parents
[INFO ] 2018-10-01 22:18:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403510000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403510000'
[INFO ] 2018-10-01 22:18:30,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403510000 ms to writer queue
[INFO ] 2018-10-01 22:18:30,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:42767 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:18:30,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:18:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:18:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:18:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:18:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 22:18:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:18:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:18:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000.bk
[INFO ] 2018-10-01 22:18:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:18:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403510000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403510000', took 4711 bytes and 23 ms
[INFO ] 2018-10-01 22:18:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:18:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (reduce at StoreMovieEssay.scala:74) finished in 0.009 s
[INFO ] 2018-10-01 22:18:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: reduce at StoreMovieEssay.scala:74, took 0.026133 s
[INFO ] 2018-10-01 22:18:30,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403510000 ms.0 from job set of time 1538403510000 ms
[ERROR] 2018-10-01 22:18:30,054 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403510000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:18:35,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538403515000
[INFO ] 2018-10-01 22:18:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403515000 ms
[INFO ] 2018-10-01 22:18:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403515000 ms
[INFO ] 2018-10-01 22:18:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403515000 ms
[INFO ] 2018-10-01 22:18:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403515000 ms.0 from job set of time 1538403515000 ms
[INFO ] 2018-10-01 22:18:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:74
[INFO ] 2018-10-01 22:18:35,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (reduce at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-10-01 22:18:35,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 22:18:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403515000 ms
[INFO ] 2018-10-01 22:18:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:18:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (reduce at StoreMovieEssay.scala:74)
[INFO ] 2018-10-01 22:18:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:18:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:18:35,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:74), which has no missing parents
[INFO ] 2018-10-01 22:18:35,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403515000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403515000'
[INFO ] 2018-10-01 22:18:35,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403515000 ms to writer queue
[INFO ] 2018-10-01 22:18:35,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:18:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:42767 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:18:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:18:35,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:18:35,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:18:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:18:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:18:35,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000
[INFO ] 2018-10-01 22:18:35,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403515000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403515000', took 4723 bytes and 19 ms
[INFO ] 2018-10-01 22:18:35,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:18:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:18:35,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:18:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (reduce at StoreMovieEssay.scala:74) finished in 0.009 s
[INFO ] 2018-10-01 22:18:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:18:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: reduce at StoreMovieEssay.scala:74, took 0.027774 s
[INFO ] 2018-10-01 22:18:35,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403515000 ms.0 from job set of time 1538403515000 ms
[ERROR] 2018-10-01 22:18:35,051 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403515000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:73)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-10-01 22:19:15,980 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Timed out while stopping the job generator (timeout = 50000)
[INFO ] 2018-10-01 22:19:15,981 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:19:15,982 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 22:19:15,983 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 22:19:15,985 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 22:19:15,995 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 22:19:15,995 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 22:19:16,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:19:16,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 22:19:16,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 22:19:16,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 22:19:16,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 22:19:16,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 22:19:16,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 22:19:16,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 22:19:16,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-4bb0c63c-d7a4-48c0-b6f6-10094c7e78e6
[INFO ] 2018-10-01 22:25:28,472 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:25:28,894 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:25:29,026 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:25:29,027 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:25:29,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:25:29,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:25:29,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:25:29,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:25:29,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:25:29,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:25:29,278 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 42017.
[INFO ] 2018-10-01 22:25:29,295 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:25:29,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:25:29,310 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:25:29,310 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:25:29,317 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-6da2201e-7004-48d5-a977-9a8664d388d7
[INFO ] 2018-10-01 22:25:29,366 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:25:29,405 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:25:29,558 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:25:29,601 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:25:29,689 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:25:29,705 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39787.
[INFO ] 2018-10-01 22:25:29,706 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:39787
[INFO ] 2018-10-01 22:25:29,707 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:25:29,708 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 39787, None)
[INFO ] 2018-10-01 22:25:29,710 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:39787 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 39787, None)
[INFO ] 2018-10-01 22:25:29,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 39787, None)
[INFO ] 2018-10-01 22:25:29,713 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 39787, None)
[INFO ] 2018-10-01 22:25:29,897 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:25:29,898 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:25:30,488 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:25:30,497 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:25:30,705 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:25:30,705 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:25:30,706 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:25:30,706 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:25:30,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:25:30,879 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:25:30,897 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:25:30,898 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:25:30,899 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:25:30,899 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@68f0f72c
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:25:30,900 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c98781a
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@446626a7
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:25:30,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@617389a
[INFO ] 2018-10-01 22:25:30,952 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:25:31,025 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:25:31,049 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:25:31,049 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:25:31,194 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:25:31,195 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:25:31,195 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:25:31,211 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 5
[INFO ] 2018-10-01 22:25:31,212 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:25:31,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538403935000
[INFO ] 2018-10-01 22:25:31,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538403935000 ms
[INFO ] 2018-10-01 22:25:31,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:25:31,241 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:25:35,567 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403935000 ms
[INFO ] 2018-10-01 22:25:35,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403935000 ms
[INFO ] 2018-10-01 22:25:35,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403935000 ms
[INFO ] 2018-10-01 22:25:35,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403935000 ms.0 from job set of time 1538403935000 ms
[INFO ] 2018-10-01 22:25:35,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403935000 ms
[INFO ] 2018-10-01 22:25:35,584 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403935000 ms to writer queue
[INFO ] 2018-10-01 22:25:35,585 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403935000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403935000'
[INFO ] 2018-10-01 22:25:35,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:75
[INFO ] 2018-10-01 22:25:35,603 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (reduce at StoreMovieEssay.scala:75) with 1 output partitions
[INFO ] 2018-10-01 22:25:35,605 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (reduce at StoreMovieEssay.scala:75)
[INFO ] 2018-10-01 22:25:35,606 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:25:35,607 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:25:35,611 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:75), which has no missing parents
[INFO ] 2018-10-01 22:25:35,631 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403250000
[INFO ] 2018-10-01 22:25:35,632 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403935000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403935000', took 4660 bytes and 48 ms
[INFO ] 2018-10-01 22:25:35,662 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:35,674 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2046.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:35,676 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:39787 (size: 2046.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:35,678 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:25:35,691 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:75) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:25:35,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:25:35,724 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:25:35,749 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:25:35,807 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:25:35,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:25:35,823 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 108 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:25:35,825 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:25:35,828 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (reduce at StoreMovieEssay.scala:75) finished in 0.124 s
[INFO ] 2018-10-01 22:25:35,832 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: reduce at StoreMovieEssay.scala:75, took 0.240820 s
[INFO ] 2018-10-01 22:25:35,837 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403935000 ms.0 from job set of time 1538403935000 ms
[ERROR] 2018-10-01 22:25:35,839 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403935000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:75)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:25:40,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403940000 ms
[INFO ] 2018-10-01 22:25:40,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403940000 ms
[INFO ] 2018-10-01 22:25:40,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403940000 ms
[INFO ] 2018-10-01 22:25:40,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403940000 ms
[INFO ] 2018-10-01 22:25:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403940000 ms.0 from job set of time 1538403940000 ms
[INFO ] 2018-10-01 22:25:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403940000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403940000'
[INFO ] 2018-10-01 22:25:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403940000 ms to writer queue
[INFO ] 2018-10-01 22:25:40,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:75
[INFO ] 2018-10-01 22:25:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:75) with 1 output partitions
[INFO ] 2018-10-01 22:25:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:75)
[INFO ] 2018-10-01 22:25:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:25:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:25:40,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[7] at filter at StoreMovieEssay.scala:75), which has no missing parents
[INFO ] 2018-10-01 22:25:40,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:40,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:40,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:39787 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:40,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403255000
[INFO ] 2018-10-01 22:25:40,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403940000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403940000', took 4697 bytes and 40 ms
[INFO ] 2018-10-01 22:25:40,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:25:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at filter at StoreMovieEssay.scala:75) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:25:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:25:40,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:25:40,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:25:40,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:39787 in memory (size: 2046.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:40,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:25:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:25:40,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 11 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:25:40,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:25:40,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:75) finished in 0.012 s
[INFO ] 2018-10-01 22:25:40,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:75, took 0.061868 s
[INFO ] 2018-10-01 22:25:40,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403940000 ms.0 from job set of time 1538403940000 ms
[ERROR] 2018-10-01 22:25:40,083 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403940000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:75)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:25:45,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403945000 ms.0 from job set of time 1538403945000 ms
[INFO ] 2018-10-01 22:25:45,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:75
[INFO ] 2018-10-01 22:25:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (reduce at StoreMovieEssay.scala:75) with 1 output partitions
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (reduce at StoreMovieEssay.scala:75)
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403945000 ms
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403945000 ms
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403945000 ms
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403945000 ms
[INFO ] 2018-10-01 22:25:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:75), which has no missing parents
[INFO ] 2018-10-01 22:25:45,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403945000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403945000'
[INFO ] 2018-10-01 22:25:45,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403945000 ms to writer queue
[INFO ] 2018-10-01 22:25:45,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:45,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:39787 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:45,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:25:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:75) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:25:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:25:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:25:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 22:25:45,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403260000
[INFO ] 2018-10-01 22:25:45,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403945000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403945000', took 4712 bytes and 16 ms
[INFO ] 2018-10-01 22:25:45,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:25:45,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:25:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:25:45,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:25:45,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (reduce at StoreMovieEssay.scala:75) finished in 0.010 s
[INFO ] 2018-10-01 22:25:45,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: reduce at StoreMovieEssay.scala:75, took 0.029527 s
[INFO ] 2018-10-01 22:25:45,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403945000 ms.0 from job set of time 1538403945000 ms
[ERROR] 2018-10-01 22:25:45,046 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403945000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:75)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:25:48,789 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:25:48,791 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538403948791.
[WARN ] 2018-10-01 22:25:48,792 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:25:48,792 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:25:48,793 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:25:48,793 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:25:48,794 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:25:48,795 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:25:48,796 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:25:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403950000 ms
[INFO ] 2018-10-01 22:25:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403950000 ms.0 from job set of time 1538403950000 ms
[INFO ] 2018-10-01 22:25:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403950000 ms
[INFO ] 2018-10-01 22:25:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403950000 ms
[INFO ] 2018-10-01 22:25:50,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403950000 ms
[INFO ] 2018-10-01 22:25:50,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:75
[INFO ] 2018-10-01 22:25:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (reduce at StoreMovieEssay.scala:75) with 1 output partitions
[INFO ] 2018-10-01 22:25:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (reduce at StoreMovieEssay.scala:75)
[INFO ] 2018-10-01 22:25:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:25:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403950000 ms to writer queue
[INFO ] 2018-10-01 22:25:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:25:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403950000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403950000'
[INFO ] 2018-10-01 22:25:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:75), which has no missing parents
[INFO ] 2018-10-01 22:25:50,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2047.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:39787 (size: 2047.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:50,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:25:50,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:75) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:25:50,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:25:50,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:25:50,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:25:50,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:25:50,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403265000
[INFO ] 2018-10-01 22:25:50,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:25:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403950000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403950000', took 4720 bytes and 26 ms
[INFO ] 2018-10-01 22:25:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:25:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:25:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (reduce at StoreMovieEssay.scala:75) finished in 0.009 s
[INFO ] 2018-10-01 22:25:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: reduce at StoreMovieEssay.scala:75, took 0.029268 s
[INFO ] 2018-10-01 22:25:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403950000 ms.0 from job set of time 1538403950000 ms
[ERROR] 2018-10-01 22:25:50,052 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403950000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:75)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:25:55,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538403955000
[INFO ] 2018-10-01 22:25:55,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538403955000 ms.0 from job set of time 1538403955000 ms
[INFO ] 2018-10-01 22:25:55,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 22:25:55,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:25:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:75
[INFO ] 2018-10-01 22:25:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538403955000 ms
[INFO ] 2018-10-01 22:25:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (reduce at StoreMovieEssay.scala:75) with 1 output partitions
[INFO ] 2018-10-01 22:25:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538403955000 ms
[INFO ] 2018-10-01 22:25:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (reduce at StoreMovieEssay.scala:75)
[INFO ] 2018-10-01 22:25:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538403955000 ms
[INFO ] 2018-10-01 22:25:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:25:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:25:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538403955000 ms
[INFO ] 2018-10-01 22:25:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[19] at filter at StoreMovieEssay.scala:75), which has no missing parents
[INFO ] 2018-10-01 22:25:55,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538403955000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403955000'
[INFO ] 2018-10-01 22:25:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538403955000 ms to writer queue
[INFO ] 2018-10-01 22:25:55,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:55,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2044.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:25:55,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:39787 (size: 2044.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:25:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:25:55,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at filter at StoreMovieEssay.scala:75) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:25:55,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 22:25:55,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:25:55,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 22:25:55,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403270000
[INFO ] 2018-10-01 22:25:55,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538403955000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403955000', took 4730 bytes and 25 ms
[INFO ] 2018-10-01 22:25:55,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:25:55,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:25:55,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:25:55,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:25:55,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (reduce at StoreMovieEssay.scala:75) finished in 0.014 s
[INFO ] 2018-10-01 22:25:55,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: reduce at StoreMovieEssay.scala:75, took 0.032430 s
[INFO ] 2018-10-01 22:25:55,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538403955000 ms.0 from job set of time 1538403955000 ms
[ERROR] 2018-10-01 22:25:55,062 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538403955000 ms.0
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:75)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:74)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-10-01 22:26:38,865 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Timed out while stopping the job generator (timeout = 50000)
[INFO ] 2018-10-01 22:26:38,866 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:26:38,868 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 22:26:38,868 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 22:26:38,870 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 22:26:38,877 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 22:26:38,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 22:26:38,884 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:26:38,892 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 22:26:38,902 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 22:26:38,902 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 22:26:38,903 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 22:26:38,905 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 22:26:38,907 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 22:26:38,907 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 22:26:38,908 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-cc41f0cf-48c4-4514-8816-619360a2acc4
[INFO ] 2018-10-01 22:31:15,764 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:31:16,192 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:31:16,297 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:31:16,297 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:31:16,327 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:31:16,345 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:31:16,346 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:31:16,349 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:31:16,349 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:31:16,350 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:31:16,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 34515.
[INFO ] 2018-10-01 22:31:16,643 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:31:16,659 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:31:16,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:31:16,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:31:16,676 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-e1b35748-c0f4-4168-a406-7bb1322a6794
[INFO ] 2018-10-01 22:31:16,717 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:31:16,762 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:31:16,908 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:31:16,963 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:31:17,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:31:17,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36351.
[INFO ] 2018-10-01 22:31:17,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:36351
[INFO ] 2018-10-01 22:31:17,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:31:17,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 36351, None)
[INFO ] 2018-10-01 22:31:17,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:36351 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 36351, None)
[INFO ] 2018-10-01 22:31:17,105 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 36351, None)
[INFO ] 2018-10-01 22:31:17,105 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 36351, None)
[INFO ] 2018-10-01 22:31:17,313 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:31:17,314 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:31:17,864 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:31:17,872 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:31:18,077 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:31:18,077 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:31:18,078 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:31:18,078 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:31:18,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:31:18,216 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:31:18,226 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:31:18,227 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:31:18,227 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:31:18,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:31:18,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7e53339
[INFO ] 2018-10-01 22:31:18,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@67064bdc
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@4993febc
[INFO ] 2018-10-01 22:31:18,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-10-01 22:31:18,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:31:18,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:31:18,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-10-01 22:31:18,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@714f3da4
[INFO ] 2018-10-01 22:31:18,270 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:31:18,335 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:31:18,358 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:31:18,358 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:31:18,495 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:31:18,496 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:31:18,496 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:31:18,506 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 7
[INFO ] 2018-10-01 22:31:18,508 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:31:18,519 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538404280000
[INFO ] 2018-10-01 22:31:18,519 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538404280000 ms
[INFO ] 2018-10-01 22:31:18,520 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:31:18,524 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:31:20,573 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,577 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,578 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,579 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404280000 ms.0 from job set of time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,583 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,599 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404280000 ms to writer queue
[INFO ] 2018-10-01 22:31:20,601 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404280000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000'
[INFO ] 2018-10-01 22:31:20,639 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:20,659 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:20,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:20,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:20,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:20,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:20,672 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403275000
[INFO ] 2018-10-01 22:31:20,673 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404280000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000', took 4550 bytes and 74 ms
[INFO ] 2018-10-01 22:31:20,708 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:20,720 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2025.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:20,721 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:36351 (size: 2025.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:20,724 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:20,738 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:20,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:31:20,773 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:20,789 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:31:20,840 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:20,850 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 766 bytes result sent to driver
[INFO ] 2018-10-01 22:31:20,859 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 98 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:20,861 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:20,865 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:64) finished in 0.114 s
[INFO ] 2018-10-01 22:31:20,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:64, took 0.241785 s
[INFO ] 2018-10-01 22:31:20,886 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404280000 ms.0 from job set of time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,887 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.886 s for time 1538404280000 ms (execution: 0.308 s)
[INFO ] 2018-10-01 22:31:20,902 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,903 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,903 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,905 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404280000 ms to writer queue
[INFO ] 2018-10-01 22:31:20,905 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404280000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000'
[INFO ] 2018-10-01 22:31:20,936 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403500000
[INFO ] 2018-10-01 22:31:20,936 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404280000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000', took 4547 bytes and 31 ms
[INFO ] 2018-10-01 22:31:20,959 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,961 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404280000 ms
[INFO ] 2018-10-01 22:31:20,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:20,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:36351 in memory (size: 2025.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:20,985 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404275000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538242090732-1538242150732
[INFO ] 2018-10-01 22:31:20,999 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 22:31:21,000 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404275000
[INFO ] 2018-10-01 22:31:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404285000 ms to writer queue
[INFO ] 2018-10-01 22:31:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404285000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000'
[INFO ] 2018-10-01 22:31:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404285000 ms.0 from job set of time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:25,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:25,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:25,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:25,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403505000
[INFO ] 2018-10-01 22:31:25,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404285000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000', took 4585 bytes and 20 ms
[INFO ] 2018-10-01 22:31:25,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:31:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:31:25,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:25,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:25,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:25,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:25,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:64) finished in 0.008 s
[INFO ] 2018-10-01 22:31:25,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:64, took 0.023896 s
[INFO ] 2018-10-01 22:31:25,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404285000 ms.0 from job set of time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.055 s for time 1538404285000 ms (execution: 0.030 s)
[INFO ] 2018-10-01 22:31:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-01 22:31:25,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-01 22:31:25,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-01 22:31:25,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-01 22:31:25,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-01 22:31:25,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-01 22:31:25,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404285000 ms to writer queue
[INFO ] 2018-10-01 22:31:25,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404285000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000'
[INFO ] 2018-10-01 22:31:25,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403510000
[INFO ] 2018-10-01 22:31:25,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404285000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000', took 4547 bytes and 18 ms
[INFO ] 2018-10-01 22:31:25,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404285000 ms
[INFO ] 2018-10-01 22:31:25,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:25,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404280000: 
[INFO ] 2018-10-01 22:31:25,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 22:31:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404290000 ms.0 from job set of time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404290000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000'
[INFO ] 2018-10-01 22:31:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404290000 ms to writer queue
[INFO ] 2018-10-01 22:31:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:30,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:30,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:31:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 22:31:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:30,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 766 bytes result sent to driver
[INFO ] 2018-10-01 22:31:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:64) finished in 0.007 s
[INFO ] 2018-10-01 22:31:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:64, took 0.024806 s
[INFO ] 2018-10-01 22:31:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404290000 ms.0 from job set of time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.040 s for time 1538404290000 ms (execution: 0.031 s)
[INFO ] 2018-10-01 22:31:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-10-01 22:31:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-10-01 22:31:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-10-01 22:31:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-10-01 22:31:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-10-01 22:31:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-10-01 22:31:30,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404290000 ms to writer queue
[INFO ] 2018-10-01 22:31:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403515000
[INFO ] 2018-10-01 22:31:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404290000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000', took 4583 bytes and 31 ms
[INFO ] 2018-10-01 22:31:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404290000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000'
[INFO ] 2018-10-01 22:31:30,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403935000
[INFO ] 2018-10-01 22:31:30,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404290000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000', took 4547 bytes and 9 ms
[INFO ] 2018-10-01 22:31:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404290000 ms
[INFO ] 2018-10-01 22:31:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404285000: 
[INFO ] 2018-10-01 22:31:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404280000 ms
[INFO ] 2018-10-01 22:31:35,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404295000 ms to writer queue
[INFO ] 2018-10-01 22:31:35,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404295000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000'
[INFO ] 2018-10-01 22:31:35,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:35,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404295000 ms.0 from job set of time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:35,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:35,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:35,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:35,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:35,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:35,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:31:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:35,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:31:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403940000
[INFO ] 2018-10-01 22:31:35,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404295000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000', took 4585 bytes and 22 ms
[INFO ] 2018-10-01 22:31:35,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:35,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:35,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:35,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:64) finished in 0.011 s
[INFO ] 2018-10-01 22:31:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:64, took 0.020937 s
[INFO ] 2018-10-01 22:31:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404295000 ms.0 from job set of time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.033 s for time 1538404295000 ms (execution: 0.021 s)
[INFO ] 2018-10-01 22:31:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-01 22:31:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-01 22:31:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-01 22:31:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-01 22:31:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-01 22:31:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-01 22:31:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404295000 ms to writer queue
[INFO ] 2018-10-01 22:31:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404295000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000'
[INFO ] 2018-10-01 22:31:35,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403945000
[INFO ] 2018-10-01 22:31:35,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404295000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000', took 4547 bytes and 9 ms
[INFO ] 2018-10-01 22:31:35,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404295000 ms
[INFO ] 2018-10-01 22:31:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:35,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404290000: 
[INFO ] 2018-10-01 22:31:35,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404285000 ms
[INFO ] 2018-10-01 22:31:40,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404300000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000'
[INFO ] 2018-10-01 22:31:40,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404300000 ms to writer queue
[INFO ] 2018-10-01 22:31:40,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404300000 ms.0 from job set of time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:40,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:40,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 22:31:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403950000
[INFO ] 2018-10-01 22:31:40,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404300000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000', took 4583 bytes and 15 ms
[INFO ] 2018-10-01 22:31:40,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:40,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 22:31:40,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:40,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:40,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:40,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:40,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:64) finished in 0.005 s
[INFO ] 2018-10-01 22:31:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:64, took 0.017896 s
[INFO ] 2018-10-01 22:31:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404300000 ms.0 from job set of time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.033 s for time 1538404300000 ms (execution: 0.021 s)
[INFO ] 2018-10-01 22:31:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-10-01 22:31:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-10-01 22:31:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-10-01 22:31:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-10-01 22:31:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-10-01 22:31:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-10-01 22:31:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404300000 ms to writer queue
[INFO ] 2018-10-01 22:31:40,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404300000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000'
[INFO ] 2018-10-01 22:31:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538403955000
[INFO ] 2018-10-01 22:31:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404300000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000', took 4547 bytes and 8 ms
[INFO ] 2018-10-01 22:31:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404300000 ms
[INFO ] 2018-10-01 22:31:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:40,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404295000: 
[INFO ] 2018-10-01 22:31:40,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404290000 ms
[INFO ] 2018-10-01 22:31:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404305000 ms.0 from job set of time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404305000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000'
[INFO ] 2018-10-01 22:31:45,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404305000 ms to writer queue
[INFO ] 2018-10-01 22:31:45,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:45,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2025.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:45,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:36351 (size: 2025.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-10-01 22:31:45,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:45,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-10-01 22:31:45,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:45,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:45,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:45,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:45,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (collect at StoreMovieEssay.scala:64) finished in 0.004 s
[INFO ] 2018-10-01 22:31:45,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: collect at StoreMovieEssay.scala:64, took 0.016370 s
[INFO ] 2018-10-01 22:31:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404305000 ms.0 from job set of time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.033 s for time 1538404305000 ms (execution: 0.023 s)
[INFO ] 2018-10-01 22:31:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-10-01 22:31:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-10-01 22:31:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-10-01 22:31:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-10-01 22:31:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-10-01 22:31:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-10-01 22:31:45,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404305000 ms to writer queue
[INFO ] 2018-10-01 22:31:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000.bk
[INFO ] 2018-10-01 22:31:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404305000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000', took 4585 bytes and 22 ms
[INFO ] 2018-10-01 22:31:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404305000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000'
[INFO ] 2018-10-01 22:31:45,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404280000
[INFO ] 2018-10-01 22:31:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404305000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000', took 4547 bytes and 13 ms
[INFO ] 2018-10-01 22:31:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404305000 ms
[INFO ] 2018-10-01 22:31:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:45,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404300000: 
[INFO ] 2018-10-01 22:31:45,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404295000 ms
[INFO ] 2018-10-01 22:31:50,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404310000 ms.0 from job set of time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404310000 ms to writer queue
[INFO ] 2018-10-01 22:31:50,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404310000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000'
[INFO ] 2018-10-01 22:31:50,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:50,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000.bk
[INFO ] 2018-10-01 22:31:50,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404310000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000', took 4583 bytes and 15 ms
[INFO ] 2018-10-01 22:31:50,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-10-01 22:31:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-10-01 22:31:50,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:50,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:50,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:50,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:50,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (collect at StoreMovieEssay.scala:64) finished in 0.006 s
[INFO ] 2018-10-01 22:31:50,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: collect at StoreMovieEssay.scala:64, took 0.024502 s
[INFO ] 2018-10-01 22:31:50,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404310000 ms.0 from job set of time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.041 s for time 1538404310000 ms (execution: 0.033 s)
[INFO ] 2018-10-01 22:31:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17 from persistence list
[INFO ] 2018-10-01 22:31:50,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17
[INFO ] 2018-10-01 22:31:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16 from persistence list
[INFO ] 2018-10-01 22:31:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16
[INFO ] 2018-10-01 22:31:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15 from persistence list
[INFO ] 2018-10-01 22:31:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15
[INFO ] 2018-10-01 22:31:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404310000 ms to writer queue
[INFO ] 2018-10-01 22:31:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404310000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000'
[INFO ] 2018-10-01 22:31:50,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404285000
[INFO ] 2018-10-01 22:31:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404310000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000', took 4547 bytes and 13 ms
[INFO ] 2018-10-01 22:31:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404310000 ms
[INFO ] 2018-10-01 22:31:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:50,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404305000: 
[INFO ] 2018-10-01 22:31:50,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404300000 ms
[INFO ] 2018-10-01 22:31:55,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404315000 ms.0 from job set of time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404315000 ms to writer queue
[INFO ] 2018-10-01 22:31:55,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404315000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000'
[INFO ] 2018-10-01 22:31:55,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:31:55,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 7 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:31:55,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 7 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:31:55,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:31:55,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:31:55,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:31:55,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:55,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000.bk
[INFO ] 2018-10-01 22:31:55,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:31:55,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404315000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000', took 4583 bytes and 10 ms
[INFO ] 2018-10-01 22:31:55,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_7_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:31:55,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:31:55,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:31:55,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 7.0 with 1 tasks
[INFO ] 2018-10-01 22:31:55,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:31:55,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 7.0 (TID 7)
[INFO ] 2018-10-01 22:31:55,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:31:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:31:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:31:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:31:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 7 (collect at StoreMovieEssay.scala:64) finished in 0.009 s
[INFO ] 2018-10-01 22:31:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 7 finished: collect at StoreMovieEssay.scala:64, took 0.019084 s
[INFO ] 2018-10-01 22:31:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404315000 ms.0 from job set of time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20 from persistence list
[INFO ] 2018-10-01 22:31:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.032 s for time 1538404315000 ms (execution: 0.025 s)
[INFO ] 2018-10-01 22:31:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20
[INFO ] 2018-10-01 22:31:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19 from persistence list
[INFO ] 2018-10-01 22:31:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19
[INFO ] 2018-10-01 22:31:55,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18 from persistence list
[INFO ] 2018-10-01 22:31:55,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18
[INFO ] 2018-10-01 22:31:55,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404315000 ms to writer queue
[INFO ] 2018-10-01 22:31:55,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404315000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000'
[INFO ] 2018-10-01 22:31:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404290000
[INFO ] 2018-10-01 22:31:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404315000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000', took 4547 bytes and 7 ms
[INFO ] 2018-10-01 22:31:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404315000 ms
[INFO ] 2018-10-01 22:31:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:31:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404310000: 
[INFO ] 2018-10-01 22:31:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404305000 ms
[INFO ] 2018-10-01 22:32:00,005 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404320000 ms.0 from job set of time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404320000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000'
[INFO ] 2018-10-01 22:32:00,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404320000 ms to writer queue
[INFO ] 2018-10-01 22:32:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 8 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 8 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 8 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:00,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:32:00,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:32:00,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_8_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:00,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 8.0 with 1 tasks
[INFO ] 2018-10-01 22:32:00,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:00,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 8.0 (TID 8)
[INFO ] 2018-10-01 22:32:00,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:00,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000.bk
[INFO ] 2018-10-01 22:32:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404320000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000', took 4586 bytes and 21 ms
[INFO ] 2018-10-01 22:32:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 8 (collect at StoreMovieEssay.scala:64) finished in 0.010 s
[INFO ] 2018-10-01 22:32:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 8 finished: collect at StoreMovieEssay.scala:64, took 0.019014 s
[INFO ] 2018-10-01 22:32:00,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404320000 ms.0 from job set of time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.029 s for time 1538404320000 ms (execution: 0.024 s)
[INFO ] 2018-10-01 22:32:00,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23 from persistence list
[INFO ] 2018-10-01 22:32:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23
[INFO ] 2018-10-01 22:32:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22 from persistence list
[INFO ] 2018-10-01 22:32:00,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22
[INFO ] 2018-10-01 22:32:00,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21 from persistence list
[INFO ] 2018-10-01 22:32:00,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21
[INFO ] 2018-10-01 22:32:00,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404320000 ms to writer queue
[INFO ] 2018-10-01 22:32:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404320000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000'
[INFO ] 2018-10-01 22:32:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404295000
[INFO ] 2018-10-01 22:32:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404320000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000', took 4548 bytes and 7 ms
[INFO ] 2018-10-01 22:32:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404320000 ms
[INFO ] 2018-10-01 22:32:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404315000: 
[INFO ] 2018-10-01 22:32:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404310000 ms
[INFO ] 2018-10-01 22:32:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404325000 ms.0 from job set of time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404325000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000'
[INFO ] 2018-10-01 22:32:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404325000 ms to writer queue
[INFO ] 2018-10-01 22:32:05,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:05,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 9 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 9 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 9 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:05,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:32:05,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:32:05,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_9_piece0 in memory on 192.168.0.100:36351 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:05,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:05,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:05,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 9.0 with 1 tasks
[INFO ] 2018-10-01 22:32:05,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:05,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 9.0 (TID 9)
[INFO ] 2018-10-01 22:32:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000.bk
[INFO ] 2018-10-01 22:32:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404325000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000', took 4587 bytes and 15 ms
[INFO ] 2018-10-01 22:32:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 7 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:05,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:05,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9) in 3 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 9 (collect at StoreMovieEssay.scala:64) finished in 0.004 s
[INFO ] 2018-10-01 22:32:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 9 finished: collect at StoreMovieEssay.scala:64, took 0.014945 s
[INFO ] 2018-10-01 22:32:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404325000 ms.0 from job set of time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26 from persistence list
[INFO ] 2018-10-01 22:32:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.029 s for time 1538404325000 ms (execution: 0.019 s)
[INFO ] 2018-10-01 22:32:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25 from persistence list
[INFO ] 2018-10-01 22:32:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24 from persistence list
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404325000 ms to writer queue
[INFO ] 2018-10-01 22:32:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404325000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000'
[INFO ] 2018-10-01 22:32:05,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404300000
[INFO ] 2018-10-01 22:32:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404325000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000', took 4548 bytes and 7 ms
[INFO ] 2018-10-01 22:32:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404325000 ms
[INFO ] 2018-10-01 22:32:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:05,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404320000: 
[INFO ] 2018-10-01 22:32:05,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404315000 ms
[INFO ] 2018-10-01 22:32:10,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404330000 ms.0 from job set of time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 10 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 10 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 10 (MapPartitionsRDD[32] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:10,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_10 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:32:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_10_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_10_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 10.0 with 1 tasks
[INFO ] 2018-10-01 22:32:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 10.0 (TID 10)
[INFO ] 2018-10-01 22:32:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 7 -> 14
[INFO ] 2018-10-01 22:32:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-01 22:32:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-01 22:32:10,030 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:32:10,031 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:32:10,033 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:32:10,033 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:32:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 7
[INFO ] 2018-10-01 22:32:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404330000 ms to writer queue
[INFO ] 2018-10-01 22:32:10,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404330000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000'
[INFO ] 2018-10-01 22:32:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000.bk
[INFO ] 2018-10-01 22:32:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404330000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000', took 4592 bytes and 14 ms
[INFO ] 2018-10-01 22:32:10,137 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:32:10,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/12915563 
[INFO ] 2018-10-01 22:32:10,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/12915563 
[INFO ] 2018-10-01 22:32:10,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/12915563 
[INFO ] 2018-10-01 22:32:10,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/12915563 ajkhdkfj
[INFO ] 2018-10-01 22:32:10,197 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 10.0 (TID 10). 23560 bytes result sent to driver
[INFO ] 2018-10-01 22:32:10,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 10.0 (TID 10) in 176 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:10,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:10,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 10 (collect at StoreMovieEssay.scala:64) finished in 0.178 s
[INFO ] 2018-10-01 22:32:10,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 10 finished: collect at StoreMovieEssay.scala:64, took 0.185892 s
[INFO ] 2018-10-01 22:32:10,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915563,1627814,/caesarphoenix1515))
[INFO ] 2018-10-01 22:32:10,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915563,1118154,,.   michelle            michellejulian    michellejulian  michellealex   ??,          alex michelle michelle   . michelle        ))
[INFO ] 2018-10-01 22:32:10,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915563,5834885,11989911607Samaritaine2AlexMichleMarionMichleJulienJulienAlexMarion(Michle et Julien, ou LAmour de la Fille et du Gar&ccedil;on)1984Boy Meets GirlLOeil secBoy Meets Girl1984; Mauvais sang, 1986; Les Amants du Pont-Neuf, 1991HansAlexMarionJulienHansFlorence3AlexAlexAlexMichleMichleMichleAlexMichleMichleLeos Carax1991Le Pen AlexMichle4MichleAlexMichleAlexAlexMichleJulienMichleMichleAlexAlcyonHansAlexMichle5AlexMichleHansMichleAlexHansHansAlex MichleAlexAlexPalais de JusticeGottfried KellerMichleHans6MichleMichleAlexMichleAlex MichleAlexAlexAlexMichleMichle7AlexMichleMichleAlexHansAlexMichleMichleAlexAlexMichleAlexJean-Paul SartreLes Mains sales8/here-and-nowMichleAlexMichleAlexMichle9lgretMichle&#8226; Michle////////Jacques Prvert, Chanson de la Seine10 (END)))
[INFO ] 2018-10-01 22:32:10,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404330000 ms.0 from job set of time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.201 s for time 1538404330000 ms (execution: 0.191 s)
[INFO ] 2018-10-01 22:32:10,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 29 from persistence list
[INFO ] 2018-10-01 22:32:10,204 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 29
[INFO ] 2018-10-01 22:32:10,204 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 28 from persistence list
[INFO ] 2018-10-01 22:32:10,205 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 28
[INFO ] 2018-10-01 22:32:10,205 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 27 from persistence list
[INFO ] 2018-10-01 22:32:10,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 27
[INFO ] 2018-10-01 22:32:10,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404330000 ms to writer queue
[INFO ] 2018-10-01 22:32:10,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404330000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000'
[INFO ] 2018-10-01 22:32:10,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404305000
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404330000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000', took 4560 bytes and 14 ms
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404330000 ms
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404325000: 
[INFO ] 2018-10-01 22:32:10,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404320000 ms
[INFO ] 2018-10-01 22:32:15,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404335000 ms.0 from job set of time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404335000 ms to writer queue
[INFO ] 2018-10-01 22:32:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404335000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000'
[INFO ] 2018-10-01 22:32:15,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 11 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 11 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 11 (MapPartitionsRDD[35] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:15,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:15,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_11_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_11_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 11.0 with 1 tasks
[INFO ] 2018-10-01 22:32:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000.bk
[INFO ] 2018-10-01 22:32:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404335000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000', took 4599 bytes and 18 ms
[INFO ] 2018-10-01 22:32:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:15,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 11.0 (TID 11)
[INFO ] 2018-10-01 22:32:15,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 14 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:15,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 11.0 (TID 11). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:15,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 11.0 (TID 11) in 4 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:15,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:15,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 11 (collect at StoreMovieEssay.scala:64) finished in 0.005 s
[INFO ] 2018-10-01 22:32:15,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 11 finished: collect at StoreMovieEssay.scala:64, took 0.019649 s
[INFO ] 2018-10-01 22:32:15,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404335000 ms.0 from job set of time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.041 s for time 1538404335000 ms (execution: 0.025 s)
[INFO ] 2018-10-01 22:32:15,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 32 from persistence list
[INFO ] 2018-10-01 22:32:15,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 32
[INFO ] 2018-10-01 22:32:15,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 31 from persistence list
[INFO ] 2018-10-01 22:32:15,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 31
[INFO ] 2018-10-01 22:32:15,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 30 from persistence list
[INFO ] 2018-10-01 22:32:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 30
[INFO ] 2018-10-01 22:32:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404335000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000'
[INFO ] 2018-10-01 22:32:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404335000 ms to writer queue
[INFO ] 2018-10-01 22:32:15,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404310000
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404335000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000', took 4556 bytes and 13 ms
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404335000 ms
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404330000: 
[INFO ] 2018-10-01 22:32:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404325000 ms
[INFO ] 2018-10-01 22:32:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404340000 ms.0 from job set of time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 12 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 12 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 12 (MapPartitionsRDD[38] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_12_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_12_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[38] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 12.0 with 1 tasks
[INFO ] 2018-10-01 22:32:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 12.0 (TID 12)
[INFO ] 2018-10-01 22:32:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404340000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000'
[INFO ] 2018-10-01 22:32:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404340000 ms to writer queue
[INFO ] 2018-10-01 22:32:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 14 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 12.0 (TID 12). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 12 (collect at StoreMovieEssay.scala:64) finished in 0.005 s
[INFO ] 2018-10-01 22:32:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 12 finished: collect at StoreMovieEssay.scala:64, took 0.016003 s
[INFO ] 2018-10-01 22:32:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404340000 ms.0 from job set of time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 35 from persistence list
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.042 s for time 1538404340000 ms (execution: 0.024 s)
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 35
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 34 from persistence list
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 34
[INFO ] 2018-10-01 22:32:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 33 from persistence list
[INFO ] 2018-10-01 22:32:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 33
[INFO ] 2018-10-01 22:32:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404340000 ms to writer queue
[INFO ] 2018-10-01 22:32:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000.bk
[INFO ] 2018-10-01 22:32:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404340000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000', took 4593 bytes and 21 ms
[INFO ] 2018-10-01 22:32:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404340000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000'
[INFO ] 2018-10-01 22:32:20,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404315000
[INFO ] 2018-10-01 22:32:20,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404340000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000', took 4556 bytes and 17 ms
[INFO ] 2018-10-01 22:32:20,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404340000 ms
[INFO ] 2018-10-01 22:32:20,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:20,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404335000: 
[INFO ] 2018-10-01 22:32:20,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404330000 ms
[INFO ] 2018-10-01 22:32:25,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404345000 ms to writer queue
[INFO ] 2018-10-01 22:32:25,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404345000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000'
[INFO ] 2018-10-01 22:32:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404345000 ms.0 from job set of time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 13 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 13 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:25,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000.bk
[INFO ] 2018-10-01 22:32:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404345000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000', took 4593 bytes and 7 ms
[INFO ] 2018-10-01 22:32:25,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:25,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 13 (MapPartitionsRDD[41] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:25,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:25,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_13_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:25,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_13_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:25,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:25,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:25,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 13.0 with 1 tasks
[INFO ] 2018-10-01 22:32:25,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:25,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 13.0 (TID 13)
[INFO ] 2018-10-01 22:32:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 14 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 13.0 (TID 13). 680 bytes result sent to driver
[INFO ] 2018-10-01 22:32:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:25,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:25,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 13 (collect at StoreMovieEssay.scala:64) finished in 0.004 s
[INFO ] 2018-10-01 22:32:25,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 13 finished: collect at StoreMovieEssay.scala:64, took 0.015795 s
[INFO ] 2018-10-01 22:32:25,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404345000 ms.0 from job set of time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 38 from persistence list
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.036 s for time 1538404345000 ms (execution: 0.017 s)
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 38
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 37 from persistence list
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 37
[INFO ] 2018-10-01 22:32:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 36 from persistence list
[INFO ] 2018-10-01 22:32:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 36
[INFO ] 2018-10-01 22:32:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404345000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000'
[INFO ] 2018-10-01 22:32:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404345000 ms to writer queue
[INFO ] 2018-10-01 22:32:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404320000
[INFO ] 2018-10-01 22:32:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404345000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000', took 4556 bytes and 5 ms
[INFO ] 2018-10-01 22:32:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404345000 ms
[INFO ] 2018-10-01 22:32:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:32:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538404340000: 
[INFO ] 2018-10-01 22:32:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404335000 ms
[INFO ] 2018-10-01 22:32:27,949 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:32:27,951 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538404347951.
[WARN ] 2018-10-01 22:32:27,952 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:32:27,953 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:32:27,954 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:32:27,955 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:32:27,956 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:32:27,956 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:32:27,957 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:32:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404350000 ms.0 from job set of time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404350000 ms to writer queue
[INFO ] 2018-10-01 22:32:30,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404350000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000'
[INFO ] 2018-10-01 22:32:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000.bk
[INFO ] 2018-10-01 22:32:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404350000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000', took 4595 bytes and 5 ms
[INFO ] 2018-10-01 22:32:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 14 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 14 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 14 (MapPartitionsRDD[44] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_14_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_14_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 14.0 with 1 tasks
[INFO ] 2018-10-01 22:32:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 14.0 (TID 14)
[INFO ] 2018-10-01 22:32:30,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 14 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:30,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 14.0 (TID 14). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 14.0 (TID 14) in 4 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 14 (collect at StoreMovieEssay.scala:64) finished in 0.004 s
[INFO ] 2018-10-01 22:32:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 14 finished: collect at StoreMovieEssay.scala:64, took 0.013626 s
[INFO ] 2018-10-01 22:32:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404350000 ms.0 from job set of time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.033 s for time 1538404350000 ms (execution: 0.024 s)
[INFO ] 2018-10-01 22:32:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 41 from persistence list
[INFO ] 2018-10-01 22:32:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 41
[INFO ] 2018-10-01 22:32:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 40 from persistence list
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 40
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 39 from persistence list
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 39
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404350000 ms to writer queue
[INFO ] 2018-10-01 22:32:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404350000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000'
[INFO ] 2018-10-01 22:32:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404325000
[INFO ] 2018-10-01 22:32:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404350000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000', took 4556 bytes and 6 ms
[INFO ] 2018-10-01 22:32:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404350000 ms
[INFO ] 2018-10-01 22:32:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 22:32:30,043 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538404350041 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 22:32:30,045 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 22:32:30,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404340000 ms
[INFO ] 2018-10-01 22:32:35,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538404355000
[INFO ] 2018-10-01 22:32:35,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538404355000 ms.0 from job set of time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-10-01 22:32:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 15 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-10-01 22:32:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 15 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-10-01 22:32:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:32:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:32:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 15 (MapPartitionsRDD[47] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-10-01 22:32:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404355000 ms to writer queue
[INFO ] 2018-10-01 22:32:35,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_15_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 22:32:35,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 22:32:35,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404355000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000'
[INFO ] 2018-10-01 22:32:35,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_15_piece0 in memory on 192.168.0.100:36351 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:32:35,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:32:35,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:32:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:32:35,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 15.0 with 1 tasks
[INFO ] 2018-10-01 22:32:35,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:32:35,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 15.0 (TID 15)
[INFO ] 2018-10-01 22:32:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 14 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:32:35,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 15.0 (TID 15). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:32:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 15.0 (TID 15) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:32:35,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:32:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 15 (collect at StoreMovieEssay.scala:64) finished in 0.007 s
[INFO ] 2018-10-01 22:32:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 15 finished: collect at StoreMovieEssay.scala:64, took 0.016552 s
[INFO ] 2018-10-01 22:32:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538404355000 ms.0 from job set of time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.038 s for time 1538404355000 ms (execution: 0.023 s)
[INFO ] 2018-10-01 22:32:35,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000.bk
[INFO ] 2018-10-01 22:32:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404355000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000', took 4593 bytes and 15 ms
[INFO ] 2018-10-01 22:32:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 44 from persistence list
[INFO ] 2018-10-01 22:32:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 44
[INFO ] 2018-10-01 22:32:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 43 from persistence list
[INFO ] 2018-10-01 22:32:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 43
[INFO ] 2018-10-01 22:32:35,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 42 from persistence list
[INFO ] 2018-10-01 22:32:35,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 42
[INFO ] 2018-10-01 22:32:35,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538404355000 ms to writer queue
[INFO ] 2018-10-01 22:32:35,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538404355000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000'
[INFO ] 2018-10-01 22:32:35,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404330000
[INFO ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538404355000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000', took 4556 bytes and 22 ms
[INFO ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538404355000 ms
[INFO ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538404355064 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 22:32:35,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538404345000 ms
[INFO ] 2018-10-01 22:32:35,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:32:35,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 22:32:35,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 22:32:35,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 22:32:35,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 22:32:35,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 22:32:35,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:32:35,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 22:32:35,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 22:32:35,170 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 22:32:35,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 22:32:35,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 22:32:35,176 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 22:32:35,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 22:32:35,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-e7543f44-4ff0-40dc-980b-22f8a17e8ab4
[INFO ] 2018-10-01 22:53:53,535 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:53:53,936 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:53:54,037 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:53:54,038 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:53:54,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:53:54,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:53:54,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:53:54,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:53:54,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:53:54,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:53:54,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 44137.
[INFO ] 2018-10-01 22:53:54,352 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:53:54,365 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:53:54,367 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:53:54,368 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:53:54,374 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-68fac09d-ce6b-43d3-84a2-5bd52164e439
[INFO ] 2018-10-01 22:53:54,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:53:54,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:53:54,615 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:53:54,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:53:54,794 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:53:54,835 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42403.
[INFO ] 2018-10-01 22:53:54,835 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:42403
[INFO ] 2018-10-01 22:53:54,837 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:53:54,838 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 42403, None)
[INFO ] 2018-10-01 22:53:54,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:42403 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 42403, None)
[INFO ] 2018-10-01 22:53:54,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 42403, None)
[INFO ] 2018-10-01 22:53:54,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 42403, None)
[INFO ] 2018-10-01 22:53:55,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:53:55,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:53:55,566 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:53:55,576 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:53:55,793 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:53:55,793 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:53:55,794 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:53:55,794 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:53:55,799 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:53:55,964 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 2 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:53:55,974 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:53:55,975 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:53:55,975 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:53:55,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:53:55,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@12d5c30e
[INFO ] 2018-10-01 22:53:55,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:53:55,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:53:55,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:53:55,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:53:55,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@80bfa9d
[INFO ] 2018-10-01 22:53:55,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:53:55,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@4012d5bc
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:53:55,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7e0bc8a3
[INFO ] 2018-10-01 22:53:56,023 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:53:56,093 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:53:56,114 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:53:56,114 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:53:56,249 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:53:56,249 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:53:56,249 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:53:56,263 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 9
[INFO ] 2018-10-01 22:53:56,264 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:53:56,275 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538405640000
[INFO ] 2018-10-01 22:53:56,276 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538405640000 ms
[INFO ] 2018-10-01 22:53:56,276 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:53:56,280 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:54:00,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405640000 ms
[INFO ] 2018-10-01 22:54:00,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405640000 ms.0 from job set of time 1538405640000 ms
[INFO ] 2018-10-01 22:54:00,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405640000 ms
[INFO ] 2018-10-01 22:54:00,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405640000 ms
[INFO ] 2018-10-01 22:54:00,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405640000 ms
[INFO ] 2018-10-01 22:54:00,111 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405640000 ms to writer queue
[INFO ] 2018-10-01 22:54:00,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405640000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405640000'
[INFO ] 2018-10-01 22:54:00,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-01 22:54:00,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000.bk
[INFO ] 2018-10-01 22:54:00,148 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405640000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405640000', took 4671 bytes and 38 ms
[INFO ] 2018-10-01 22:54:00,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-01 22:54:00,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-01 22:54:00,154 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:00,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:00,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-01 22:54:00,195 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:42403 (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:00,225 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:00,225 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:54:00,253 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:54:00,263 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:54:00,314 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 14 -> 28
[INFO ] 2018-10-01 22:54:00,316 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-01 22:54:00,318 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-01 22:54:00,320 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:54:00,322 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:54:00,324 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:54:00,324 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:54:00,326 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 14
[INFO ] 2018-10-01 22:54:00,432 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:54:00,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 5489 bytes result sent to driver
[INFO ] 2018-10-01 22:54:00,471 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 225 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:54:00,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:00,477 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:80) finished in 0.240 s
[INFO ] 2018-10-01 22:54:00,481 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:80, took 0.338932 s
[INFO ] 2018-10-01 22:54:00,497 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:54:00,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:54:00,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:54:00,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:00,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:00,500 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:81), which has no missing parents
[INFO ] 2018-10-01 22:54:00,503 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,505 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,518 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:42403 (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,521 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:00,522 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:81) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:00,523 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:54:00,524 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:54:00,525 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:54:00,531 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 14 -> 28
[INFO ] 2018-10-01 22:54:00,531 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 14
[INFO ] 2018-10-01 22:54:00,544 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:42403 in memory (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileb/media/feng//bigdata/test/12915563 () 
[INFO ] 2018-10-01 22:54:00,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileb/media/feng//bigdata/test/12915563 () 
[INFO ] 2018-10-01 22:54:00,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileb/media/feng//bigdata/test/12915563 () 
[INFO ] 2018-10-01 22:54:00,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileb/media/feng//bigdata/test/12915563 () ajkhdkfj
[INFO ] 2018-10-01 22:54:00,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileR/media/feng//bigdata/test/849845654 
[INFO ] 2018-10-01 22:54:00,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileR/media/feng//bigdata/test/849845654 
[INFO ] 2018-10-01 22:54:00,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileR/media/feng//bigdata/test/849845654 
[INFO ] 2018-10-01 22:54:00,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileR/media/feng//bigdata/test/849845654 ajkhdkfj
[INFO ] 2018-10-01 22:54:00,973 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 46195 bytes result sent to driver
[INFO ] 2018-10-01 22:54:00,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 453 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:54:00,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:81) finished in 0.452 s
[INFO ] 2018-10-01 22:54:00,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:81, took 0.479575 s
[INFO ] 2018-10-01 22:54:00,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:00,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
++++++++++++++ not empty
[INFO ] 2018-10-01 22:54:00,993 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:84
[INFO ] 2018-10-01 22:54:00,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:84) with 1 output partitions
[INFO ] 2018-10-01 22:54:00,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:84)
[INFO ] 2018-10-01 22:54:00,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:00,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:00,995 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84), which has no missing parents
[INFO ] 2018-10-01 22:54:00,997 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 1952.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:00,999 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 1270.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:01,000 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:42403 (size: 1270.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:01,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:01,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:01,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:54:01,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 49867 bytes)
[INFO ] 2018-10-01 22:54:01,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[ERROR] 2018-10-01 22:54:01,259 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Exception in task 0.0 in stage 2.0 (TID 2)
java.io.NotSerializableException: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":1,"row":"36551921-1627814","families":{"cf":[{"qualifier":"cn","vlen":4452,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Lscala.Tuple2;, size 6)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:383)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-01 22:54:01,288 org.apache.spark.internal.Logging$class.logError(Logging.scala:70)
Task 0.0 in stage 2.0 (TID 2) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":1,"row":"36551921-1627814","families":{"cf":[{"qualifier":"cn","vlen":4452,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Lscala.Tuple2;, size 6); not retrying
[INFO ] 2018-10-01 22:54:01,289 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:01,292 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cancelling stage 2
[INFO ] 2018-10-01 22:54:01,294 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:84) failed in 0.291 s due to Job aborted due to stage failure: Task 0.0 in stage 2.0 (TID 2) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":1,"row":"36551921-1627814","families":{"cf":[{"qualifier":"cn","vlen":4452,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Lscala.Tuple2;, size 6)
[INFO ] 2018-10-01 22:54:01,295 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 failed: collect at StoreMovieEssay.scala:84, took 0.301611 s
[INFO ] 2018-10-01 22:54:01,298 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405640000 ms.0 from job set of time 1538405640000 ms
[ERROR] 2018-10-01 22:54:01,300 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538405640000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 2.0 (TID 2) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":1,"row":"36551921-1627814","families":{"cf":[{"qualifier":"cn","vlen":4452,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Lscala.Tuple2;, size 6)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:84)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:79)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 22:54:10,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405650000 ms.0 from job set of time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405650000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000'
[INFO ] 2018-10-01 22:54:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405650000 ms to writer queue
[INFO ] 2018-10-01 22:54:10,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-01 22:54:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-01 22:54:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-01 22:54:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-01 22:54:10,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:42403 (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:54:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:54:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:54:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:54:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:54:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:54:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:80) finished in 0.011 s
[INFO ] 2018-10-01 22:54:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:80, took 0.027603 s
[INFO ] 2018-10-01 22:54:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405650000 ms.0 from job set of time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-01 22:54:10,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.052 s for time 1538405650000 ms (execution: 0.035 s)
[INFO ] 2018-10-01 22:54:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-01 22:54:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404335000
[INFO ] 2018-10-01 22:54:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-01 22:54:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405650000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000', took 4715 bytes and 38 ms
[INFO ] 2018-10-01 22:54:10,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-01 22:54:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-01 22:54:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-01 22:54:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405650000 ms to writer queue
[INFO ] 2018-10-01 22:54:10,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405650000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000'
[INFO ] 2018-10-01 22:54:10,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000.bk
[INFO ] 2018-10-01 22:54:10,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405650000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000', took 4677 bytes and 17 ms
[INFO ] 2018-10-01 22:54:10,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405650000 ms
[INFO ] 2018-10-01 22:54:10,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:54:10,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 2 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405640000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538404280975-1538404340975
file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538404345045-1538404405045
[INFO ] 2018-10-01 22:54:10,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 22:54:10,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405640000
[INFO ] 2018-10-01 22:54:10,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405640000
[INFO ] 2018-10-01 22:54:14,787 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:54:14,788 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538405654788.
[WARN ] 2018-10-01 22:54:14,789 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:54:14,789 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:54:14,790 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:54:14,791 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:54:14,791 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:54:14,792 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:54:14,792 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:54:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405660000 ms.0 from job set of time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405660000 ms to writer queue
[INFO ] 2018-10-01 22:54:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405660000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000'
[INFO ] 2018-10-01 22:54:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-01 22:54:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-01 22:54:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-01 22:54:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-01 22:54:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:42403 (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404340000
[INFO ] 2018-10-01 22:54:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 22:54:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405660000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000', took 4718 bytes and 14 ms
[INFO ] 2018-10-01 22:54:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:54:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 22:54:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:54:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:54:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:54:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:80) finished in 0.007 s
[INFO ] 2018-10-01 22:54:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:80, took 0.016886 s
[INFO ] 2018-10-01 22:54:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405660000 ms.0 from job set of time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-01 22:54:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.039 s for time 1538405660000 ms (execution: 0.024 s)
[INFO ] 2018-10-01 22:54:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-01 22:54:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-01 22:54:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-01 22:54:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-01 22:54:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-01 22:54:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405660000 ms to writer queue
[INFO ] 2018-10-01 22:54:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405660000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000'
[INFO ] 2018-10-01 22:54:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000.bk
[INFO ] 2018-10-01 22:54:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405660000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000', took 4677 bytes and 14 ms
[INFO ] 2018-10-01 22:54:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405660000 ms
[INFO ] 2018-10-01 22:54:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 22:54:20,057 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538405660057 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 22:54:20,058 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 22:54:20,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405640000 ms
[INFO ] 2018-10-01 22:54:30,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538405670000
[INFO ] 2018-10-01 22:54:30,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405670000 ms.0 from job set of time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-01 22:54:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-01 22:54:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:54:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:54:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-01 22:54:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405670000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000'
[INFO ] 2018-10-01 22:54:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405670000 ms to writer queue
[INFO ] 2018-10-01 22:54:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:54:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 22:54:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:42403 (size: 2.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 22:54:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:54:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:54:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:54:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-10-01 22:54:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:54:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-10-01 22:54:30,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:54:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-10-01 22:54:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:54:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:54:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (isEmpty at StoreMovieEssay.scala:80) finished in 0.005 s
[INFO ] 2018-10-01 22:54:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: isEmpty at StoreMovieEssay.scala:80, took 0.019629 s
[INFO ] 2018-10-01 22:54:30,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405670000 ms.0 from job set of time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.038 s for time 1538405670000 ms (execution: 0.026 s)
[INFO ] 2018-10-01 22:54:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-10-01 22:54:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-10-01 22:54:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-10-01 22:54:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-10-01 22:54:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-10-01 22:54:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-10-01 22:54:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405670000 ms to writer queue
[INFO ] 2018-10-01 22:54:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404345000
[INFO ] 2018-10-01 22:54:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405670000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000', took 4712 bytes and 33 ms
[INFO ] 2018-10-01 22:54:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405670000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000'
[INFO ] 2018-10-01 22:54:30,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000.bk
[INFO ] 2018-10-01 22:54:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405670000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000', took 4677 bytes and 10 ms
[INFO ] 2018-10-01 22:54:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405670000 ms
[INFO ] 2018-10-01 22:54:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 22:54:30,063 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538405670062 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 22:54:30,063 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 22:54:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405650000 ms
[INFO ] 2018-10-01 22:54:30,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 22:54:30,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 22:54:30,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 22:54:30,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 22:54:30,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 22:54:30,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 22:54:30,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:54:30,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 22:54:30,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 22:54:30,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 22:54:30,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 22:54:30,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 22:54:30,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 22:54:30,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 22:54:30,172 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-d133f291-8be2-4d28-98ea-e3e5301c4d84
[INFO ] 2018-10-01 22:58:15,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 22:58:16,062 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 22:58:16,163 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 22:58:16,163 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 22:58:16,218 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 22:58:16,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 22:58:16,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 22:58:16,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 22:58:16,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 22:58:16,235 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 22:58:16,509 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 34087.
[INFO ] 2018-10-01 22:58:16,528 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 22:58:16,542 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 22:58:16,545 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 22:58:16,545 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 22:58:16,558 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-39ad08c5-d172-48d2-b7d2-a18c2033c2e0
[INFO ] 2018-10-01 22:58:16,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 22:58:16,658 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 22:58:16,821 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 22:58:16,880 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 22:58:16,985 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 22:58:17,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45441.
[INFO ] 2018-10-01 22:58:17,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:45441
[INFO ] 2018-10-01 22:58:17,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 22:58:17,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 45441, None)
[INFO ] 2018-10-01 22:58:17,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:45441 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 45441, None)
[INFO ] 2018-10-01 22:58:17,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 45441, None)
[INFO ] 2018-10-01 22:58:17,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 45441, None)
[INFO ] 2018-10-01 22:58:17,231 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 22:58:17,232 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 22:58:17,753 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 22:58:17,764 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 22:58:17,969 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 22:58:17,970 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 22:58:17,970 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 22:58:17,970 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 22:58:17,974 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 22:58:18,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 22:58:18,160 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:58:18,161 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:58:18,162 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:58:18,162 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:58:18,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@ccf91df
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@f88bfbe
[INFO ] 2018-10-01 22:58:18,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@435cc7f9
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 22:58:18,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6d11ceef
[INFO ] 2018-10-01 22:58:18,214 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:58:18,291 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 22:58:18,311 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:58:18,311 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:58:18,448 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:58:18,448 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:58:18,449 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:58:18,462 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 11
[INFO ] 2018-10-01 22:58:18,463 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 22:58:18,472 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538405900000
[INFO ] 2018-10-01 22:58:18,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538405900000 ms
[INFO ] 2018-10-01 22:58:18,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 22:58:18,478 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 22:58:20,572 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:20,573 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:20,573 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:20,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405900000 ms.0 from job set of time 1538405900000 ms
[INFO ] 2018-10-01 22:58:20,578 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:20,590 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405900000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000'
[INFO ] 2018-10-01 22:58:20,595 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405900000 ms to writer queue
[INFO ] 2018-10-01 22:58:20,616 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:58:20,628 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:58:20,628 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:58:20,630 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:58:20,632 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:58:20,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:58:20,650 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404350000
[INFO ] 2018-10-01 22:58:20,651 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405900000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000', took 4700 bytes and 61 ms
[INFO ] 2018-10-01 22:58:20,674 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:20,851 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2034.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:20,853 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:45441 (size: 2034.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:58:20,856 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:58:20,869 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:58:20,870 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 22:58:20,897 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:58:20,903 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 22:58:20,947 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:58:20,964 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:58:20,991 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 91 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:58:20,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:58:20,996 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:81) finished in 0.116 s
[INFO ] 2018-10-01 22:58:21,000 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:81, took 0.383979 s
[INFO ] 2018-10-01 22:58:21,003 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405900000 ms.0 from job set of time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 1.003 s for time 1538405900000 ms (execution: 0.430 s)
[INFO ] 2018-10-01 22:58:21,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405900000 ms to writer queue
[INFO ] 2018-10-01 22:58:21,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405900000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000'
[INFO ] 2018-10-01 22:58:21,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000.bk
[INFO ] 2018-10-01 22:58:21,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405900000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000', took 4697 bytes and 25 ms
[INFO ] 2018-10-01 22:58:21,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405900000 ms
[INFO ] 2018-10-01 22:58:21,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:58:21,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405890000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538405650087-1538405710087
[INFO ] 2018-10-01 22:58:21,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 22:58:21,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405890000
[INFO ] 2018-10-01 22:58:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405910000 ms.0 from job set of time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405910000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000'
[INFO ] 2018-10-01 22:58:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405910000 ms to writer queue
[INFO ] 2018-10-01 22:58:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:58:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:58:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:58:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:58:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:58:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:58:30,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538404355000
[INFO ] 2018-10-01 22:58:30,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405910000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000', took 4734 bytes and 22 ms
[INFO ] 2018-10-01 22:58:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:30,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:30,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:45441 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:58:30,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:58:30,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:58:30,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 22:58:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:58:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 22:58:30,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:58:30,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:58:30,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:58:30,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:58:30,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (isEmpty at StoreMovieEssay.scala:81) finished in 0.017 s
[INFO ] 2018-10-01 22:58:30,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: isEmpty at StoreMovieEssay.scala:81, took 0.048175 s
[INFO ] 2018-10-01 22:58:30,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405910000 ms.0 from job set of time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.083 s for time 1538405910000 ms (execution: 0.061 s)
[INFO ] 2018-10-01 22:58:30,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-01 22:58:30,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-01 22:58:30,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-01 22:58:30,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-01 22:58:30,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-01 22:58:30,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405910000 ms to writer queue
[INFO ] 2018-10-01 22:58:30,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405910000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000'
[INFO ] 2018-10-01 22:58:30,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-01 22:58:30,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405640000
[INFO ] 2018-10-01 22:58:30,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405910000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000', took 4697 bytes and 28 ms
[INFO ] 2018-10-01 22:58:30,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405910000 ms
[INFO ] 2018-10-01 22:58:30,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:58:30,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405900000: 
[INFO ] 2018-10-01 22:58:30,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 22:58:40,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405920000 ms.0 from job set of time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:58:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:58:40,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:58:40,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405920000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000'
[INFO ] 2018-10-01 22:58:40,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405920000 ms to writer queue
[INFO ] 2018-10-01 22:58:40,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:40,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2033.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:45441 (size: 2033.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:58:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:58:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:58:40,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 22:58:40,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:58:40,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 22:58:40,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:58:40,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:58:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000.bk
[INFO ] 2018-10-01 22:58:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405920000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000', took 4736 bytes and 31 ms
[INFO ] 2018-10-01 22:58:40,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:58:40,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (isEmpty at StoreMovieEssay.scala:81) finished in 0.014 s
[INFO ] 2018-10-01 22:58:40,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: isEmpty at StoreMovieEssay.scala:81, took 0.039303 s
[INFO ] 2018-10-01 22:58:40,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:58:40,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405920000 ms.0 from job set of time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.059 s for time 1538405920000 ms (execution: 0.050 s)
[INFO ] 2018-10-01 22:58:40,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-10-01 22:58:40,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-10-01 22:58:40,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-10-01 22:58:40,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-10-01 22:58:40,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-10-01 22:58:40,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-10-01 22:58:40,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405920000 ms to writer queue
[INFO ] 2018-10-01 22:58:40,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405920000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000'
[INFO ] 2018-10-01 22:58:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405650000
[INFO ] 2018-10-01 22:58:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405920000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000', took 4697 bytes and 13 ms
[INFO ] 2018-10-01 22:58:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405920000 ms
[INFO ] 2018-10-01 22:58:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:58:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405910000: 
[INFO ] 2018-10-01 22:58:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405900000 ms
[INFO ] 2018-10-01 22:58:50,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405930000 ms.0 from job set of time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405930000 ms to writer queue
[INFO ] 2018-10-01 22:58:50,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405930000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000'
[INFO ] 2018-10-01 22:58:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:58:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:58:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:58:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:58:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:58:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:58:50,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:50,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:58:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:45441 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:58:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:58:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:58:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 22:58:50,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:58:50,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 22:58:50,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000.bk
[INFO ] 2018-10-01 22:58:50,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405930000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000', took 4734 bytes and 20 ms
[INFO ] 2018-10-01 22:58:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:58:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:58:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 16 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:58:50,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:58:50,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:81) finished in 0.017 s
[INFO ] 2018-10-01 22:58:50,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:81, took 0.031724 s
[INFO ] 2018-10-01 22:58:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405930000 ms.0 from job set of time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.049 s for time 1538405930000 ms (execution: 0.039 s)
[INFO ] 2018-10-01 22:58:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-01 22:58:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-01 22:58:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-01 22:58:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-01 22:58:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-01 22:58:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-01 22:58:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405930000 ms to writer queue
[INFO ] 2018-10-01 22:58:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405930000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000'
[INFO ] 2018-10-01 22:58:50,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405660000
[INFO ] 2018-10-01 22:58:50,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405930000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000', took 4697 bytes and 10 ms
[INFO ] 2018-10-01 22:58:50,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405930000 ms
[INFO ] 2018-10-01 22:58:50,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:58:50,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405920000: 
[INFO ] 2018-10-01 22:58:50,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405910000 ms
[INFO ] 2018-10-01 22:59:00,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405940000 ms.0 from job set of time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405940000 ms to writer queue
[INFO ] 2018-10-01 22:59:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405940000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000'
[INFO ] 2018-10-01 22:59:00,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:59:00,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:59:00,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:59:00,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:59:00,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:59:00,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:59:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:00,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:45441 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:59:00,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:59:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:59:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 22:59:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:59:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 22:59:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000.bk
[INFO ] 2018-10-01 22:59:00,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405940000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000', took 4736 bytes and 19 ms
[INFO ] 2018-10-01 22:59:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 28 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:59:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:59:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:59:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:59:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:81) finished in 0.007 s
[INFO ] 2018-10-01 22:59:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:81, took 0.025101 s
[INFO ] 2018-10-01 22:59:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405940000 ms.0 from job set of time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.040 s for time 1538405940000 ms (execution: 0.031 s)
[INFO ] 2018-10-01 22:59:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-10-01 22:59:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405940000 ms to writer queue
[INFO ] 2018-10-01 22:59:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405940000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000'
[INFO ] 2018-10-01 22:59:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405670000
[INFO ] 2018-10-01 22:59:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405940000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000', took 4697 bytes and 12 ms
[INFO ] 2018-10-01 22:59:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405940000 ms
[INFO ] 2018-10-01 22:59:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:59:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405930000: 
[INFO ] 2018-10-01 22:59:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405920000 ms
[INFO ] 2018-10-01 22:59:10,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:10,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:10,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:10,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:10,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405950000 ms.0 from job set of time 1538405950000 ms
[INFO ] 2018-10-01 22:59:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405950000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000'
[INFO ] 2018-10-01 22:59:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405950000 ms to writer queue
[INFO ] 2018-10-01 22:59:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:59:10,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:59:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:59:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:59:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:59:10,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:59:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2034.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:45441 (size: 2034.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:59:10,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:59:10,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-10-01 22:59:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:59:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-10-01 22:59:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000.bk
[INFO ] 2018-10-01 22:59:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405950000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000', took 4741 bytes and 34 ms
[INFO ] 2018-10-01 22:59:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 28 -> 35
[INFO ] 2018-10-01 22:59:10,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-01 22:59:10,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-01 22:59:10,056 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:59:10,057 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 22:59:10,059 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 22:59:10,059 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 22:59:10,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 28
[INFO ] 2018-10-01 22:59:10,165 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-01 22:59:10,193 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 5260 bytes result sent to driver
[INFO ] 2018-10-01 22:59:10,204 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 160 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:59:10,206 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (isEmpty at StoreMovieEssay.scala:81) finished in 0.156 s
[INFO ] 2018-10-01 22:59:10,206 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: isEmpty at StoreMovieEssay.scala:81, took 0.183496 s
[INFO ] 2018-10-01 22:59:10,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:59:10,220 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:82
[INFO ] 2018-10-01 22:59:10,220 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (reduce at StoreMovieEssay.scala:82) with 1 output partitions
[INFO ] 2018-10-01 22:59:10,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (reduce at StoreMovieEssay.scala:82)
[INFO ] 2018-10-01 22:59:10,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:59:10,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:59:10,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[18] at filter at StoreMovieEssay.scala:82), which has no missing parents
[INFO ] 2018-10-01 22:59:10,224 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 2024.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,230 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:45441 (size: 2024.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,231 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:59:10,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at filter at StoreMovieEssay.scala:82) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:59:10,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-10-01 22:59:10,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:59:10,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-10-01 22:59:10,239 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 28 -> 35
[INFO ] 2018-10-01 22:59:10,239 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 28
[INFO ] 2018-10-01 22:59:10,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileJ/media/feng//bigdata/test/54561 
[INFO ] 2018-10-01 22:59:10,700 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileJ/media/feng//bigdata/test/54561 
[INFO ] 2018-10-01 22:59:10,700 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileJ/media/feng//bigdata/test/54561 
[INFO ] 2018-10-01 22:59:10,704 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileJ/media/feng//bigdata/test/54561 ajkhdkfj
[INFO ] 2018-10-01 22:59:10,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 23289 bytes result sent to driver
[INFO ] 2018-10-01 22:59:10,715 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 481 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:59:10,715 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:59:10,717 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (reduce at StoreMovieEssay.scala:82) finished in 0.482 s
[INFO ] 2018-10-01 22:59:10,717 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: reduce at StoreMovieEssay.scala:82, took 0.497517 s
[INFO ] 2018-10-01 22:59:10,718 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
++++++++++++++ not empty
[INFO ] 2018-10-01 22:59:10,756 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:85
[INFO ] 2018-10-01 22:59:10,766 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 7 (collect at StoreMovieEssay.scala:85) with 1 output partitions
[INFO ] 2018-10-01 22:59:10,767 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 7 (collect at StoreMovieEssay.scala:85)
[INFO ] 2018-10-01 22:59:10,767 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:59:10,767 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:59:10,767 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 7 (MapPartitionsRDD[20] at map at StoreMovieEssay.scala:85), which has no missing parents
[INFO ] 2018-10-01 22:59:10,770 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7 stored as values in memory (estimated size 1952.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,774 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7_piece0 stored as bytes in memory (estimated size 1245.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,775 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_7_piece0 in memory on 192.168.0.100:45441 (size: 1245.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:59:10,779 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:59:10,780 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at map at StoreMovieEssay.scala:85) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:59:10,780 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 7.0 with 1 tasks
[INFO ] 2018-10-01 22:59:10,787 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 27322 bytes)
[INFO ] 2018-10-01 22:59:10,788 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 7.0 (TID 7)
[INFO ] 2018-10-01 22:59:10,885 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7). 23722 bytes result sent to driver
[INFO ] 2018-10-01 22:59:10,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7) in 121 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:59:10,901 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:59:10,902 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 7 (collect at StoreMovieEssay.scala:85) finished in 0.121 s
[INFO ] 2018-10-01 22:59:10,902 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 7 finished: collect at StoreMovieEssay.scala:85, took 0.145983 s
[INFO ] 2018-10-01 22:59:11,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
--------------------(,{"totalColumns":1,"row":"16545-1627814","families":{"cf":[{"qualifier":"cn","vlen":4452,"tag":[],"timestamp":9223372036854775807}]}})
[INFO ] 2018-10-01 22:59:11,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
--------------------(,{"totalColumns":1,"row":"16545-1118154","families":{"cf":[{"qualifier":"cn","vlen":2317,"tag":[],"timestamp":9223372036854775807}]}})
[INFO ] 2018-10-01 22:59:11,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
--------------------(,{"totalColumns":1,"row":"16545-5834885","families":{"cf":[{"qualifier":"cn","vlen":15623,"tag":[],"timestamp":9223372036854775807}]}})
[INFO ] 2018-10-01 22:59:11,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405950000 ms.0 from job set of time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-10-01 22:59:11,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-10-01 22:59:11,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 1.087 s for time 1538405950000 ms (execution: 1.077 s)
[INFO ] 2018-10-01 22:59:11,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-10-01 22:59:11,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-10-01 22:59:11,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-10-01 22:59:11,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-10-01 22:59:11,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405950000 ms to writer queue
[INFO ] 2018-10-01 22:59:11,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405950000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000'
[INFO ] 2018-10-01 22:59:11,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405900000
[INFO ] 2018-10-01 22:59:11,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405950000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000', took 4709 bytes and 50 ms
[INFO ] 2018-10-01 22:59:11,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405950000 ms
[INFO ] 2018-10-01 22:59:11,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 22:59:11,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538405940000: 
[INFO ] 2018-10-01 22:59:11,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405930000 ms
[INFO ] 2018-10-01 22:59:18,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 22:59:18,928 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538405958928.
[WARN ] 2018-10-01 22:59:18,929 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 22:59:18,929 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 22:59:18,930 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 22:59:18,931 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 22:59:18,931 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 22:59:18,932 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:59:18,932 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 22:59:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538405960000 ms.0 from job set of time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405960000 ms to writer queue
[INFO ] 2018-10-01 22:59:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405960000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000'
[INFO ] 2018-10-01 22:59:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000.bk
[INFO ] 2018-10-01 22:59:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405960000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000', took 4749 bytes and 14 ms
[INFO ] 2018-10-01 22:59:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 22:59:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 8 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 22:59:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 8 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 22:59:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 22:59:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 22:59:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 8 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 22:59:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 22:59:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_8_piece0 in memory on 192.168.0.100:45441 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 22:59:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 22:59:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 22:59:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 8.0 with 1 tasks
[INFO ] 2018-10-01 22:59:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 22:59:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 8.0 (TID 8)
[INFO ] 2018-10-01 22:59:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 35 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 22:59:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8). 702 bytes result sent to driver
[INFO ] 2018-10-01 22:59:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 22:59:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 22:59:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 8 (isEmpty at StoreMovieEssay.scala:81) finished in 0.007 s
[INFO ] 2018-10-01 22:59:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 8 finished: isEmpty at StoreMovieEssay.scala:81, took 0.023376 s
[INFO ] 2018-10-01 22:59:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538405960000 ms.0 from job set of time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17 from persistence list
[INFO ] 2018-10-01 22:59:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.053 s for time 1538405960000 ms (execution: 0.042 s)
[INFO ] 2018-10-01 22:59:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17
[INFO ] 2018-10-01 22:59:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16 from persistence list
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15 from persistence list
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538405960000 ms to writer queue
[INFO ] 2018-10-01 22:59:20,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538405960000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000'
[INFO ] 2018-10-01 22:59:20,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405910000
[INFO ] 2018-10-01 22:59:20,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538405960000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000', took 4704 bytes and 6 ms
[INFO ] 2018-10-01 22:59:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538405960000 ms
[INFO ] 2018-10-01 22:59:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 22:59:20,063 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538405960062 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 22:59:20,065 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 22:59:20,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538405940000 ms
[INFO ] 2018-10-01 23:16:54,716 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 23:16:55,170 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 23:16:55,347 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 23:16:55,348 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 23:16:55,408 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 23:16:55,421 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 23:16:55,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 23:16:55,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 23:16:55,423 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 23:16:55,423 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 23:16:55,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 45333.
[INFO ] 2018-10-01 23:16:55,720 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 23:16:55,733 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 23:16:55,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 23:16:55,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 23:16:55,750 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-319354ea-32aa-4538-b151-fcedef3ffaa5
[INFO ] 2018-10-01 23:16:55,793 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 23:16:55,835 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 23:16:55,983 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 23:16:56,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 23:16:56,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 23:16:56,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40923.
[INFO ] 2018-10-01 23:16:56,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:40923
[INFO ] 2018-10-01 23:16:56,154 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 23:16:56,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 40923, None)
[INFO ] 2018-10-01 23:16:56,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:40923 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 40923, None)
[INFO ] 2018-10-01 23:16:56,161 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 40923, None)
[INFO ] 2018-10-01 23:16:56,161 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 40923, None)
[INFO ] 2018-10-01 23:16:56,365 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 23:16:56,366 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 23:16:56,867 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 23:16:56,875 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 23:16:57,074 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 23:16:57,075 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 23:16:57,075 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 23:16:57,076 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 23:16:57,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 23:16:57,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 23:16:57,244 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:16:57,245 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:16:57,245 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:16:57,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:16:57,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-01 23:16:57,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:16:57,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c87fdf2
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-01 23:16:57,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:16:57,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:16:57,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:16:57,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:16:57,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-01 23:16:57,290 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 23:16:57,355 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 23:16:57,376 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 23:16:57,377 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 23:16:57,514 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 23:16:57,514 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:16:57,515 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:16:57,525 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 13
[INFO ] 2018-10-01 23:16:57,526 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:16:57,540 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538407020000
[INFO ] 2018-10-01 23:16:57,541 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538407020000 ms
[INFO ] 2018-10-01 23:16:57,541 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 23:16:57,551 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 23:17:00,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407020000 ms
[INFO ] 2018-10-01 23:17:00,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407020000 ms
[INFO ] 2018-10-01 23:17:00,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407020000 ms
[INFO ] 2018-10-01 23:17:00,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407020000 ms.0 from job set of time 1538407020000 ms
[INFO ] 2018-10-01 23:17:00,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407020000 ms
[INFO ] 2018-10-01 23:17:00,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407020000 ms to writer queue
[INFO ] 2018-10-01 23:17:00,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407020000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407020000'
[INFO ] 2018-10-01 23:17:00,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:17:00,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000.bk
[INFO ] 2018-10-01 23:17:00,131 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407020000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407020000', took 4708 bytes and 43 ms
[INFO ] 2018-10-01 23:17:00,138 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:17:00,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:17:00,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:17:00,140 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:17:00,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:17:00,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,291 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2034.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,292 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:40923 (size: 2034.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,295 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:17:00,311 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:17:00,312 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 23:17:00,347 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:17:00,353 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 23:17:00,413 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 35 -> 42
[INFO ] 2018-10-01 23:17:00,415 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-01 23:17:00,416 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-01 23:17:00,418 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 23:17:00,420 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 23:17:00,421 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 23:17:00,421 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 23:17:00,424 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 35
[INFO ] 2018-10-01 23:17:00,529 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-01 23:17:00,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 5306 bytes result sent to driver
[INFO ] 2018-10-01 23:17:00,594 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 260 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:17:00,596 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:17:00,600 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:81) finished in 0.275 s
[INFO ] 2018-10-01 23:17:00,604 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:81, took 0.477140 s
[INFO ] 2018-10-01 23:17:00,615 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:82
[INFO ] 2018-10-01 23:17:00,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:82) with 1 output partitions
[INFO ] 2018-10-01 23:17:00,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:82)
[INFO ] 2018-10-01 23:17:00,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:17:00,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:17:00,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:82), which has no missing parents
[INFO ] 2018-10-01 23:17:00,621 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2023.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,628 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:40923 (size: 2023.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:17:00,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:17:00,630 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:82) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:17:00,630 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 23:17:00,631 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:17:00,632 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 23:17:00,636 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 35 -> 42
[INFO ] 2018-10-01 23:17:00,636 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 35
[INFO ] 2018-10-01 23:17:01,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/64564655 
[INFO ] 2018-10-01 23:17:01,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/64564655 
[INFO ] 2018-10-01 23:17:01,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/64564655 
[INFO ] 2018-10-01 23:17:01,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/64564655 ajkhdkfj
[INFO ] 2018-10-01 23:17:01,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 23298 bytes result sent to driver
[INFO ] 2018-10-01 23:17:01,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 443 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:17:01,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:17:01,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:82) finished in 0.444 s
[INFO ] 2018-10-01 23:17:01,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:82, took 0.460039 s
[WARN ] 2018-10-01 23:17:01,154 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-10-01 23:17:01,189 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: runJob at SparkHadoopMapReduceWriter.scala:88
[INFO ] 2018-10-01 23:17:01,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (runJob at SparkHadoopMapReduceWriter.scala:88) with 1 output partitions
[INFO ] 2018-10-01 23:17:01,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (runJob at SparkHadoopMapReduceWriter.scala:88)
[INFO ] 2018-10-01 23:17:01,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:17:01,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:17:01,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84), which has no missing parents
[INFO ] 2018-10-01 23:17:01,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:01,214 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.4 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:01,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:40923 (size: 28.4 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 23:17:01,216 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:17:01,217 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:17:01,218 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 23:17:01,226 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 27331 bytes)
[INFO ] 2018-10-01 23:17:01,227 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[ERROR] 2018-10-01 23:17:01,274 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDefaultWorkFile(FileOutputFormat.java:286)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:137)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-10-01 23:17:01,292 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDefaultWorkFile(FileOutputFormat.java:286)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:137)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] 2018-10-01 23:17:01,296 org.apache.spark.internal.Logging$class.logError(Logging.scala:70)
Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] 2018-10-01 23:17:01,298 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:17:01,299 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cancelling stage 2
[INFO ] 2018-10-01 23:17:01,301 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (runJob at SparkHadoopMapReduceWriter.scala:88) failed in 0.082 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDefaultWorkFile(FileOutputFormat.java:286)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:137)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] 2018-10-01 23:17:01,302 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 failed: runJob at SparkHadoopMapReduceWriter.scala:88, took 0.112144 s
[ERROR] 2018-10-01 23:17:01,303 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Aborting job job_20181001231701_0005.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDefaultWorkFile(FileOutputFormat.java:286)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:137)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2075)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:84)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:80)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDefaultWorkFile(FileOutputFormat.java:286)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:137)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	... 3 more
[WARN ] 2018-10-01 23:17:01,308 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.cleanupJob(FileOutputCommitter.java:383)
Output Path is null in cleanupJob()
[INFO ] 2018-10-01 23:17:01,311 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407020000 ms.0 from job set of time 1538407020000 ms
[ERROR] 2018-10-01 23:17:01,312 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538407020000 ms.0
java.lang.IllegalArgumentException: Can not create a Path from a null string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:122)
	at org.apache.hadoop.fs.Path.<init>(Path.java:134)
	at org.apache.hadoop.fs.Path.<init>(Path.java:88)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.absPathStagingDir(HadoopMapReduceCommitProtocol.scala:58)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.abortJob(HadoopMapReduceCommitProtocol.scala:141)
	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:106)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:84)
	at spark.dataProcess.StoreMovieEssay$$anonfun$processData$3.apply(StoreMovieEssay.scala:80)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-01 23:17:05,593 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 23:17:05,595 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538407025595.
[WARN ] 2018-10-01 23:17:05,595 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 23:17:05,596 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 23:17:05,597 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 23:17:05,597 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 23:17:05,598 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 23:17:05,598 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 23:17:05,599 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 23:17:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407030000 ms.0 from job set of time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:17:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:17:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:17:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:17:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:17:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:17:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407030000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000'
[INFO ] 2018-10-01 23:17:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407030000 ms to writer queue
[INFO ] 2018-10-01 23:17:10,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2033.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:10,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:40923 (size: 2033.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:17:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:17:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:17:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 23:17:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:17:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 23:17:10,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:17:10,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:17:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 15 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:17:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405920000
[INFO ] 2018-10-01 23:17:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407030000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000', took 4753 bytes and 29 ms
[INFO ] 2018-10-01 23:17:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:17:10,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:81) finished in 0.016 s
[INFO ] 2018-10-01 23:17:10,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:81, took 0.035413 s
[INFO ] 2018-10-01 23:17:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407030000 ms.0 from job set of time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.064 s for time 1538407030000 ms (execution: 0.045 s)
[INFO ] 2018-10-01 23:17:10,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-01 23:17:10,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-01 23:17:10,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-01 23:17:10,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-01 23:17:10,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-01 23:17:10,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-01 23:17:10,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407030000 ms to writer queue
[INFO ] 2018-10-01 23:17:10,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407030000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000'
[INFO ] 2018-10-01 23:17:10,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000.bk
[INFO ] 2018-10-01 23:17:10,096 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407030000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000', took 4713 bytes and 14 ms
[INFO ] 2018-10-01 23:17:10,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,099 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407030000 ms
[INFO ] 2018-10-01 23:17:10,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 23:17:10,103 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538407030102 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 23:17:10,103 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 23:17:10,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 23:17:20,000 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538407040000
[INFO ] 2018-10-01 23:17:20,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407040000 ms.0 from job set of time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:17:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 23:17:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:17:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 23:17:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:17:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:17:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:17:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:17:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407040000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000'
[INFO ] 2018-10-01 23:17:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407040000 ms to writer queue
[INFO ] 2018-10-01 23:17:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 23:17:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:40923 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:17:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:17:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:17:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 23:17:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:17:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 23:17:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:17:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405930000
[INFO ] 2018-10-01 23:17:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407040000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000', took 4749 bytes and 27 ms
[INFO ] 2018-10-01 23:17:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:17:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:17:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:17:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:81) finished in 0.018 s
[INFO ] 2018-10-01 23:17:20,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:81, took 0.036846 s
[INFO ] 2018-10-01 23:17:20,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407040000 ms.0 from job set of time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-01 23:17:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.051 s for time 1538407040000 ms (execution: 0.043 s)
[INFO ] 2018-10-01 23:17:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-01 23:17:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-01 23:17:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-01 23:17:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-01 23:17:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-01 23:17:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407040000 ms to writer queue
[INFO ] 2018-10-01 23:17:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407040000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000'
[INFO ] 2018-10-01 23:17:20,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000.bk
[INFO ] 2018-10-01 23:17:20,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407040000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000', took 4713 bytes and 10 ms
[INFO ] 2018-10-01 23:17:20,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407040000 ms
[INFO ] 2018-10-01 23:17:20,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 23:17:20,065 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538407040065 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 23:17:20,065 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 23:17:20,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407020000 ms
[INFO ] 2018-10-01 23:17:20,115 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 23:17:20,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 23:17:20,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 23:17:20,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 23:17:20,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 23:17:20,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 23:17:20,134 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 23:17:20,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 23:17:20,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 23:17:20,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 23:17:20,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 23:17:20,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 23:17:20,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 23:17:20,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 23:17:20,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-185b9616-a591-4624-8062-dbd74d511a75
[INFO ] 2018-10-01 23:23:54,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-01 23:23:55,350 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-01 23:23:55,455 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-01 23:23:55,456 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-01 23:23:55,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-01 23:23:55,508 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-01 23:23:55,508 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-01 23:23:55,509 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-01 23:23:55,509 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-01 23:23:55,510 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-01 23:23:55,740 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 43993.
[INFO ] 2018-10-01 23:23:55,760 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-01 23:23:55,774 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-01 23:23:55,776 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-01 23:23:55,776 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-01 23:23:55,789 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-3159e3ec-49ac-4332-8ad5-5a5bbd0d8de0
[INFO ] 2018-10-01 23:23:55,831 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-01 23:23:55,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-01 23:23:56,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-01 23:23:56,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-01 23:23:56,186 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-01 23:23:56,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32781.
[INFO ] 2018-10-01 23:23:56,214 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:32781
[INFO ] 2018-10-01 23:23:56,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-01 23:23:56,216 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 32781, None)
[INFO ] 2018-10-01 23:23:56,219 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:32781 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 32781, None)
[INFO ] 2018-10-01 23:23:56,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 32781, None)
[INFO ] 2018-10-01 23:23:56,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 32781, None)
[INFO ] 2018-10-01 23:23:56,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-01 23:23:56,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-01 23:23:56,949 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-10-01 23:23:56,958 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-10-01 23:23:57,145 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-01 23:23:57,146 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-01 23:23:57,146 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-01 23:23:57,147 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-01 23:23:57,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-01 23:23:57,316 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-01 23:23:57,330 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:23:57,331 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:23:57,331 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:23:57,332 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:23:57,332 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@58fef7f7
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@797c3c3b
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:23:57,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@6535117e
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-01 23:23:57,334 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1caa9eb6
[INFO ] 2018-10-01 23:23:57,378 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 23:23:57,446 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-01 23:23:57,467 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 23:23:57,467 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 23:23:57,602 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-01 23:23:57,603 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:23:57,603 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:23:57,615 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 15
[INFO ] 2018-10-01 23:23:57,616 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-01 23:23:57,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538407440000
[INFO ] 2018-10-01 23:23:57,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538407440000 ms
[INFO ] 2018-10-01 23:23:57,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-01 23:23:57,632 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-01 23:24:00,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,576 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,576 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,577 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407440000 ms.0 from job set of time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,581 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,587 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407440000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000'
[INFO ] 2018-10-01 23:24:00,588 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407440000 ms to writer queue
[INFO ] 2018-10-01 23:24:00,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405940000
[INFO ] 2018-10-01 23:24:00,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407440000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000', took 4702 bytes and 39 ms
[INFO ] 2018-10-01 23:24:00,639 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:00,651 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:00,651 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:00,651 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:00,652 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:00,656 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:00,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:00,803 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2034.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:00,805 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:32781 (size: 2034.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:00,807 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:00,822 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:00,823 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-01 23:24:00,854 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:00,864 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-01 23:24:00,931 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:24:00,947 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 745 bytes result sent to driver
[INFO ] 2018-10-01 23:24:00,960 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 115 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:00,962 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:00,965 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:81) finished in 0.129 s
[INFO ] 2018-10-01 23:24:00,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:81, took 0.329783 s
[INFO ] 2018-10-01 23:24:00,972 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407440000 ms.0 from job set of time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,973 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.972 s for time 1538407440000 ms (execution: 0.396 s)
[INFO ] 2018-10-01 23:24:00,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,977 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407440000 ms to writer queue
[INFO ] 2018-10-01 23:24:00,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407440000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000'
[INFO ] 2018-10-01 23:24:00,993 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000.bk
[INFO ] 2018-10-01 23:24:00,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407440000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000', took 4699 bytes and 16 ms
[INFO ] 2018-10-01 23:24:00,995 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,997 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407440000 ms
[INFO ] 2018-10-01 23:24:00,999 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:01,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407430000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538405901041-1538405961041
[INFO ] 2018-10-01 23:24:01,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 23:24:01,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407430000
[INFO ] 2018-10-01 23:24:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407450000 ms.0 from job set of time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407450000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000'
[INFO ] 2018-10-01 23:24:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407450000 ms to writer queue
[INFO ] 2018-10-01 23:24:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:10,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405950000
[INFO ] 2018-10-01 23:24:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407450000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000', took 4735 bytes and 20 ms
[INFO ] 2018-10-01 23:24:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:10,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:32781 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-01 23:24:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-01 23:24:10,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:24:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:24:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 15 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:10,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (isEmpty at StoreMovieEssay.scala:81) finished in 0.016 s
[INFO ] 2018-10-01 23:24:10,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: isEmpty at StoreMovieEssay.scala:81, took 0.032097 s
[INFO ] 2018-10-01 23:24:10,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407450000 ms.0 from job set of time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.066 s for time 1538407450000 ms (execution: 0.044 s)
[INFO ] 2018-10-01 23:24:10,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-01 23:24:10,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-01 23:24:10,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-01 23:24:10,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407450000 ms to writer queue
[INFO ] 2018-10-01 23:24:10,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407450000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000'
[INFO ] 2018-10-01 23:24:10,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-01 23:24:10,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-01 23:24:10,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-01 23:24:10,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000.bk
[INFO ] 2018-10-01 23:24:10,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407450000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000', took 4699 bytes and 21 ms
[INFO ] 2018-10-01 23:24:10,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407450000 ms
[INFO ] 2018-10-01 23:24:10,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:10,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407440000: 
[INFO ] 2018-10-01 23:24:10,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-01 23:24:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407460000 ms.0 from job set of time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407460000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000'
[INFO ] 2018-10-01 23:24:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407460000 ms to writer queue
[INFO ] 2018-10-01 23:24:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2033.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:32781 (size: 2033.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-01 23:24:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-01 23:24:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:24:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538405960000
[INFO ] 2018-10-01 23:24:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407460000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000', took 4737 bytes and 39 ms
[INFO ] 2018-10-01 23:24:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:24:20,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 18 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:20,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (isEmpty at StoreMovieEssay.scala:81) finished in 0.018 s
[INFO ] 2018-10-01 23:24:20,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:20,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: isEmpty at StoreMovieEssay.scala:81, took 0.044012 s
[INFO ] 2018-10-01 23:24:20,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407460000 ms.0 from job set of time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.060 s for time 1538407460000 ms (execution: 0.051 s)
[INFO ] 2018-10-01 23:24:20,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-10-01 23:24:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-10-01 23:24:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-10-01 23:24:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-10-01 23:24:20,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-10-01 23:24:20,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-10-01 23:24:20,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407460000 ms to writer queue
[INFO ] 2018-10-01 23:24:20,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407460000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000'
[INFO ] 2018-10-01 23:24:20,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407020000
[INFO ] 2018-10-01 23:24:20,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407460000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000', took 4699 bytes and 13 ms
[INFO ] 2018-10-01 23:24:20,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407460000 ms
[INFO ] 2018-10-01 23:24:20,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:20,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407450000: 
[INFO ] 2018-10-01 23:24:20,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407440000 ms
[INFO ] 2018-10-01 23:24:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407470000 ms.0 from job set of time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:30,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:30,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:32781 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407470000 ms to writer queue
[INFO ] 2018-10-01 23:24:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407470000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000'
[INFO ] 2018-10-01 23:24:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-01 23:24:30,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:30,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-01 23:24:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 42 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:24:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:24:30,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000.bk
[INFO ] 2018-10-01 23:24:30,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407470000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000', took 4738 bytes and 23 ms
[INFO ] 2018-10-01 23:24:30,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 25 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:30,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:30,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:81) finished in 0.026 s
[INFO ] 2018-10-01 23:24:30,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:81, took 0.041174 s
[INFO ] 2018-10-01 23:24:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407470000 ms.0 from job set of time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.062 s for time 1538407470000 ms (execution: 0.047 s)
[INFO ] 2018-10-01 23:24:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-01 23:24:30,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-01 23:24:30,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-01 23:24:30,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-01 23:24:30,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-01 23:24:30,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-01 23:24:30,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407470000 ms to writer queue
[INFO ] 2018-10-01 23:24:30,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407470000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000'
[INFO ] 2018-10-01 23:24:30,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407030000
[INFO ] 2018-10-01 23:24:30,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407470000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000', took 4700 bytes and 16 ms
[INFO ] 2018-10-01 23:24:30,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407470000 ms
[INFO ] 2018-10-01 23:24:30,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:30,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407460000: 
[INFO ] 2018-10-01 23:24:30,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407450000 ms
[INFO ] 2018-10-01 23:24:40,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407480000 ms.0 from job set of time 1538407480000 ms
[INFO ] 2018-10-01 23:24:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407480000 ms to writer queue
[INFO ] 2018-10-01 23:24:40,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407480000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000'
[INFO ] 2018-10-01 23:24:40,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:32781 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:40,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-01 23:24:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:40,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-01 23:24:40,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 42 -> 49
[INFO ] 2018-10-01 23:24:40,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-01 23:24:40,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-01 23:24:40,041 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 23:24:40,043 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-01 23:24:40,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000.bk
[INFO ] 2018-10-01 23:24:40,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407480000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000', took 4744 bytes and 27 ms
[INFO ] 2018-10-01 23:24:40,045 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-01 23:24:40,045 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-01 23:24:40,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 42
[INFO ] 2018-10-01 23:24:40,150 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-01 23:24:40,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 5263 bytes result sent to driver
[INFO ] 2018-10-01 23:24:40,182 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 157 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:40,182 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:40,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:81) finished in 0.158 s
[INFO ] 2018-10-01 23:24:40,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:81, took 0.168781 s
[INFO ] 2018-10-01 23:24:40,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:82
[INFO ] 2018-10-01 23:24:40,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (reduce at StoreMovieEssay.scala:82) with 1 output partitions
[INFO ] 2018-10-01 23:24:40,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (reduce at StoreMovieEssay.scala:82)
[INFO ] 2018-10-01 23:24:40,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:40,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:40,193 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:82), which has no missing parents
[INFO ] 2018-10-01 23:24:40,195 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2024.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:32781 (size: 2024.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:40,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at filter at StoreMovieEssay.scala:82) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:40,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-10-01 23:24:40,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:40,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-10-01 23:24:40,206 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 42 -> 49
[INFO ] 2018-10-01 23:24:40,206 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 42
[INFO ] 2018-10-01 23:24:40,678 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/54564564 
[INFO ] 2018-10-01 23:24:40,678 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/54564564 
[INFO ] 2018-10-01 23:24:40,679 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/54564564 
[INFO ] 2018-10-01 23:24:40,679 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng//bigdata/test/54564564 ajkhdkfj
[INFO ] 2018-10-01 23:24:40,687 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 23298 bytes result sent to driver
[INFO ] 2018-10-01 23:24:40,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 488 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:40,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:40,691 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (reduce at StoreMovieEssay.scala:82) finished in 0.489 s
[INFO ] 2018-10-01 23:24:40,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: reduce at StoreMovieEssay.scala:82, took 0.500553 s
[WARN ] 2018-10-01 23:24:40,774 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-10-01 23:24:40,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: saveAsHadoopDataset at StoreMovieEssay.scala:84
[INFO ] 2018-10-01 23:24:40,818 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (saveAsHadoopDataset at StoreMovieEssay.scala:84) with 1 output partitions
[INFO ] 2018-10-01 23:24:40,818 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (saveAsHadoopDataset at StoreMovieEssay.scala:84)
[INFO ] 2018-10-01 23:24:40,818 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:40,818 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:40,819 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[17] at map at StoreMovieEssay.scala:84), which has no missing parents
[INFO ] 2018-10-01 23:24:40,834 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:24:40,839 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.5 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:24:40,840 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:32781 (size: 28.5 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:40,840 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:40,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[17] at map at StoreMovieEssay.scala:84) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:40,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-10-01 23:24:40,850 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 27331 bytes)
[INFO ] 2018-10-01 23:24:40,850 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-10-01 23:24:41,082 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:122)
Process identifier=hconnection-0x732d7e73 connecting to ZooKeeper ensemble=localhost:2181
[WARN ] 2018-10-01 23:24:41,169 org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)
Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
[INFO ] 2018-10-01 23:24:41,186 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:376)
Scheduled snapshot period at 10 second(s).
[INFO ] 2018-10-01 23:24:41,186 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)
HBase metrics system started
[INFO ] 2018-10-01 23:24:41,205 org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:65)
Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
[INFO ] 2018-10-01 23:24:41,218 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[INFO ] 2018-10-01 23:24:41,219 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:host.name=feng
[INFO ] 2018-10-01 23:24:41,219 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.version=1.8.0_181
[INFO ] 2018-10-01 23:24:41,219 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.vendor=Oracle Corporation
[INFO ] 2018-10-01 23:24:41,219 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
[INFO ] 2018-10-01 23:24:41,219 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.class.path=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/home/feng/software/scala-2.11.11/lib/akka-actor_2.11-2.3.16.jar:/home/feng/software/scala-2.11.11/lib/config-1.2.1.jar:/home/feng/software/scala-2.11.11/lib/jline-2.14.3.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-2.11.0.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-migration_2.11-1.1.0.jar:/home/feng/software/scala-2.11.11/lib/scala-compiler.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-library_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-plugin_2.11.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-library.jar:/home/feng/software/scala-2.11.11/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/scala-2.11.11/lib/scala-reflect.jar:/home/feng/software/scala-2.11.11/lib/scala-swing_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-xml_2.11-1.0.5.jar:/home/feng/software/scala-2.11.11/lib/scalap-2.11.11.jar:/home/feng/software/code/bigdata/target/classes:/home/feng/software/code/jars/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/feng/software/code/jars/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/feng/software/code/jars/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/feng/software/code/jars/org/tukaani/xz/1.0/xz-1.0.jar:/home/feng/software/code/jars/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/feng/software/code/jars/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/home/feng/software/code/jars/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/feng/software/code/jars/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/feng/software/code/jars/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/feng/software/code/jars/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/home/feng/software/code/jars/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/feng/software/code/jars/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/feng/software/code/jars/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/feng/software/code/jars/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/feng/software/code/jars/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/feng/software/code/jars/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/home/feng/software/code/jars/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/home/feng/software/code/jars/net/java/dev/jets3t/jets3t/0.9.3/jets3t-0.9.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/home/feng/software/code/jars/commons-codec/commons-codec/1.8/commons-codec-1.8.jar:/home/feng/software/code/jars/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/feng/software/code/jars/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar:/home/feng/software/code/jars/javax/mail/mail/1.4.7/mail-1.4.7.jar:/home/feng/software/code/jars/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar:/home/feng/software/code/jars/com/jamesmurty/utils/java-xmlbuilder/1.0/java-xmlbuilder-1.0.jar:/home/feng/software/code/jars/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/feng/software/code/jars/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/feng/software/code/jars/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/feng/software/code/jars/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/feng/software/code/jars/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/feng/software/code/jars/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/feng/software/code/jars/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/feng/software/code/jars/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/feng/software/code/jars/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/feng/software/code/jars/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/feng/software/code/jars/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/feng/software/code/jars/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/feng/software/code/jars/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/feng/software/code/jars/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/feng/software/code/jars/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/feng/software/code/jars/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/feng/software/code/jars/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/feng/software/code/jars/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/feng/software/code/jars/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-xml_2.11/1.0.1/scala-xml_2.11-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/feng/software/code/jars/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/feng/software/code/jars/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/feng/software/code/jars/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/feng/software/code/jars/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/feng/software/code/jars/io/netty/netty-all/4.0.43.Final/netty-all-4.0.43.Final.jar:/home/feng/software/code/jars/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/feng/software/code/jars/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/home/feng/software/code/jars/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/home/feng/software/code/jars/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/feng/software/code/jars/oro/oro/2.0.8/oro-2.0.8.jar:/home/feng/software/code/jars/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/feng/software/code/jars/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/home/feng/software/code/jars/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/feng/software/code/jars/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/home/feng/software/code/jars/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/home/feng/software/code/jars/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/home/feng/software/code/jars/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/home/feng/software/code/jars/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/home/feng/software/code/jars/com/101tec/zkclient/0.8/zkclient-0.8.jar:/home/feng/software/code/jars/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/code/jars/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-spark/4.14.0-HBase-1.4/phoenix-spark-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-core/4.14.0-HBase-1.4/phoenix-core-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-api/0.14.0-incubating/tephra-api-0.14.0-incubating.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-core/0.14.0-incubating/tephra-core-0.14.0-incubating.jar:/home/feng/software/code/jars/com/google/inject/guice/3.0/guice-3.0.jar:/home/feng/software/code/jars/javax/inject/javax.inject/1/javax.inject-1.jar:/home/feng/software/code/jars/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/feng/software/code/jars/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/feng/software/code/jars/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/feng/software/code/jars/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/feng/software/code/jars/org/apache/twill/twill-common/0.8.0/twill-common-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-core/0.8.0/twill-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-api/0.8.0/twill-api-0.8.0.jar:/home/feng/software/code/jars/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-api/0.8.0/twill-discovery-api-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-core/0.8.0/twill-discovery-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-zookeeper/0.8.0/twill-zookeeper-0.8.0.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-hbase-compat-1.4/0.14.0-incubating/tephra-hbase-compat-1.4-0.14.0-incubating.jar:/home/feng/software/code/jars/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/feng/software/code/jars/jline/jline/2.11/jline-2.11.jar:/home/feng/software/code/jars/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/feng/software/code/jars/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/feng/software/code/jars/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/feng/software/code/jars/junit/junit/4.12/junit-4.12.jar:/home/feng/software/code/jars/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/feng/software/code/jars/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/feng/software/code/jars/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/feng/software/code/jars/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/feng/software/code/jars/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/feng/software/code/jars/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/feng/software/code/jars/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-annotations/1.4.0/hbase-annotations-1.4.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-protocol/1.4.0/hbase-protocol-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-server/1.4.0/hbase-server-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-procedure/1.4.0/hbase-procedure-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0-tests.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-prefix-tree/1.4.0/hbase-prefix-tree-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics-api/1.4.0/hbase-metrics-api-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics/1.4.0/hbase-metrics-1.4.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/feng/software/code/jars/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/feng/software/code/jars/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/feng/software/code/jars/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/feng/software/code/jars/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop-compat/1.4.0/hbase-hadoop-compat-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop2-compat/1.4.0/hbase-hadoop2-compat-1.4.0.jar:/home/feng/software/code/jars/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/feng/software/code/jars/com/salesforce/i18n/i18n-util/1.0.4/i18n-util-1.0.4.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j/60.2/icu4j-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-localespi/60.2/icu4j-localespi-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-charset/60.2/icu4j-charset-60.2.jar:/home/feng/software/code/jars/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-common/2.7.5/hadoop-common-2.7.5.jar:/home/feng/software/code/jars/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/feng/software/code/jars/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/feng/software/code/jars/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/feng/software/code/jars/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/feng/software/code/jars/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/feng/software/code/jars/asm/asm/3.1/asm-3.1.jar:/home/feng/software/code/jars/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/feng/software/code/jars/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/feng/software/code/jars/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/feng/software/code/jars/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/feng/software/code/jars/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-auth/2.7.5/hadoop-auth-2.7.5.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/feng/software/code/jars/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/feng/software/code/jars/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/feng/software/code/jars/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-client/1.4.0/hbase-client-1.4.0.jar:/home/feng/software/code/jars/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/feng/software/code/jars/io/debezium/debezium-core/0.8.1.Final/debezium-core-0.8.1.Final.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar:/home/feng/software/code/jars/com/alibaba/fastjson/1.2.47/fastjson-1.2.47.jar:/home/feng/software/code/jars/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/feng/software/idea/idea-IC-181.5281.24/lib/idea_rt.jar
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.io.tmpdir=/tmp
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.compiler=<NA>
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.name=Linux
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.arch=amd64
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.version=4.13.0-36-generic
[INFO ] 2018-10-01 23:24:41,220 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.name=feng
[INFO ] 2018-10-01 23:24:41,221 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.home=/home/feng
[INFO ] 2018-10-01 23:24:41,221 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.dir=/home/feng/software/code/bigdata
[INFO ] 2018-10-01 23:24:41,221 org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@14c3162a
[INFO ] 2018-10-01 23:24:41,235 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO ] 2018-10-01 23:24:41,236 org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:852)
Socket connection established to localhost/127.0.0.1:2181, initiating session
[INFO ] 2018-10-01 23:24:41,274 org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1235)
Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1662fedb0b6002a, negotiated timeout = 40000
[INFO ] 2018-10-01 23:24:41,883 org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.closeZooKeeperWatcher(ConnectionManager.java:1757)
Closing zookeeper sessionid=0x1662fedb0b6002a
[INFO ] 2018-10-01 23:24:41,904 org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684)
Session: 0x1662fedb0b6002a closed
[INFO ] 2018-10-01 23:24:41,905 org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
EventThread shut down
[INFO ] 2018-10-01 23:24:41,909 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
No need to commit output of task because needsTaskCommit=false: attempt_20181001232440_0006_m_000000_6
[INFO ] 2018-10-01 23:24:41,915 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 709 bytes result sent to driver
[INFO ] 2018-10-01 23:24:41,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 1074 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:41,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:41,917 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (saveAsHadoopDataset at StoreMovieEssay.scala:84) finished in 1.075 s
[INFO ] 2018-10-01 23:24:41,917 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: saveAsHadoopDataset at StoreMovieEssay.scala:84, took 1.099912 s
[WARN ] 2018-10-01 23:24:41,918 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:322)
Output Path is null in commitJob()
[INFO ] 2018-10-01 23:24:41,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407480000 ms.0 from job set of time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 1.919 s for time 1538407480000 ms (execution: 1.908 s)
[INFO ] 2018-10-01 23:24:41,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-10-01 23:24:41,920 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-10-01 23:24:41,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-10-01 23:24:41,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-10-01 23:24:41,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-10-01 23:24:41,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-10-01 23:24:41,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407480000 ms to writer queue
[INFO ] 2018-10-01 23:24:41,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407480000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000'
[INFO ] 2018-10-01 23:24:41,933 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407040000
[INFO ] 2018-10-01 23:24:41,934 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407480000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000', took 4712 bytes and 11 ms
[INFO ] 2018-10-01 23:24:41,934 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,934 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407480000 ms
[INFO ] 2018-10-01 23:24:41,934 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:41,935 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407470000: 
[INFO ] 2018-10-01 23:24:41,935 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407460000 ms
[INFO ] 2018-10-01 23:24:50,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407490000 ms.0 from job set of time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 7 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 7 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:24:50,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:24:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:24:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 7 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:24:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000'
[INFO ] 2018-10-01 23:24:50,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407490000 ms to writer queue
[INFO ] 2018-10-01 23:24:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:24:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 23:24:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_7_piece0 in memory on 192.168.0.100:32781 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:24:50,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:24:50,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:24:50,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 7.0 with 1 tasks
[INFO ] 2018-10-01 23:24:50,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:24:50,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 7.0 (TID 7)
[INFO ] 2018-10-01 23:24:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000.bk
[INFO ] 2018-10-01 23:24:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 49 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:24:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000', took 4753 bytes and 24 ms
[INFO ] 2018-10-01 23:24:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:24:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:24:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:24:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 7 (isEmpty at StoreMovieEssay.scala:81) finished in 0.008 s
[INFO ] 2018-10-01 23:24:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 7 finished: isEmpty at StoreMovieEssay.scala:81, took 0.035467 s
[INFO ] 2018-10-01 23:24:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407490000 ms.0 from job set of time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.055 s for time 1538407490000 ms (execution: 0.041 s)
[INFO ] 2018-10-01 23:24:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-10-01 23:24:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000'
[INFO ] 2018-10-01 23:24:50,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407490000 ms to writer queue
[INFO ] 2018-10-01 23:24:50,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407440000
[INFO ] 2018-10-01 23:24:50,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000', took 4708 bytes and 24 ms
[INFO ] 2018-10-01 23:24:50,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407490000 ms
[INFO ] 2018-10-01 23:24:50,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:24:50,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407480000: 
[INFO ] 2018-10-01 23:24:50,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407470000 ms
[INFO ] 2018-10-01 23:25:00,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407500000 ms to writer queue
[INFO ] 2018-10-01 23:25:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407500000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000'
[INFO ] 2018-10-01 23:25:00,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407500000 ms.0 from job set of time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:25:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 8 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:25:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 8 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:25:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:25:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:25:00,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 8 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:25:00,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000.bk
[INFO ] 2018-10-01 23:25:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407500000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000', took 4747 bytes and 10 ms
[INFO ] 2018-10-01 23:25:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 1406.3 MB)
[INFO ] 2018-10-01 23:25:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.3 MB)
[INFO ] 2018-10-01 23:25:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_8_piece0 in memory on 192.168.0.100:32781 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:25:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:25:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 8.0 with 1 tasks
[INFO ] 2018-10-01 23:25:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:25:00,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 8.0 (TID 8)
[INFO ] 2018-10-01 23:25:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 49 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:25:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:25:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:25:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:25:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 8 (isEmpty at StoreMovieEssay.scala:81) finished in 0.008 s
[INFO ] 2018-10-01 23:25:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 8 finished: isEmpty at StoreMovieEssay.scala:81, took 0.025158 s
[INFO ] 2018-10-01 23:25:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407500000 ms.0 from job set of time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20 from persistence list
[INFO ] 2018-10-01 23:25:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.043 s for time 1538407500000 ms (execution: 0.032 s)
[INFO ] 2018-10-01 23:25:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20
[INFO ] 2018-10-01 23:25:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19 from persistence list
[INFO ] 2018-10-01 23:25:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19
[INFO ] 2018-10-01 23:25:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18 from persistence list
[INFO ] 2018-10-01 23:25:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18
[INFO ] 2018-10-01 23:25:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407500000 ms to writer queue
[INFO ] 2018-10-01 23:25:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407500000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000'
[INFO ] 2018-10-01 23:25:00,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_6_piece0 on 192.168.0.100:32781 in memory (size: 28.5 KB, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407450000
[INFO ] 2018-10-01 23:25:00,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407500000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000', took 4708 bytes and 31 ms
[INFO ] 2018-10-01 23:25:00,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407500000 ms
[INFO ] 2018-10-01 23:25:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:25:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407490000: 
[INFO ] 2018-10-01 23:25:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407480000 ms
[INFO ] 2018-10-01 23:25:00,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_5_piece0 on 192.168.0.100:32781 in memory (size: 2024.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_4_piece0 on 192.168.0.100:32781 in memory (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:32781 in memory (size: 2034.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_3_piece0 on 192.168.0.100:32781 in memory (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_7_piece0 on 192.168.0.100:32781 in memory (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_1_piece0 on 192.168.0.100:32781 in memory (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_8_piece0 on 192.168.0.100:32781 in memory (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:00,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_2_piece0 on 192.168.0.100:32781 in memory (size: 2033.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407510000 ms.0 from job set of time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407510000 ms to writer queue
[INFO ] 2018-10-01 23:25:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407510000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000'
[INFO ] 2018-10-01 23:25:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:25:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 9 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:25:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 9 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:25:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:25:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:25:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 9 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:25:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:10,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_9_piece0 in memory on 192.168.0.100:32781 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:25:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:25:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 9.0 with 1 tasks
[INFO ] 2018-10-01 23:25:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:25:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 9.0 (TID 9)
[INFO ] 2018-10-01 23:25:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 49 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:25:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:25:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:25:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:25:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 9 (isEmpty at StoreMovieEssay.scala:81) finished in 0.000 s
[INFO ] 2018-10-01 23:25:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 9 finished: isEmpty at StoreMovieEssay.scala:81, took 0.025365 s
[INFO ] 2018-10-01 23:25:10,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407510000 ms.0 from job set of time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.041 s for time 1538407510000 ms (execution: 0.030 s)
[INFO ] 2018-10-01 23:25:10,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23 from persistence list
[INFO ] 2018-10-01 23:25:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23
[INFO ] 2018-10-01 23:25:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22 from persistence list
[INFO ] 2018-10-01 23:25:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22
[INFO ] 2018-10-01 23:25:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21 from persistence list
[INFO ] 2018-10-01 23:25:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21
[INFO ] 2018-10-01 23:25:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407510000 ms to writer queue
[INFO ] 2018-10-01 23:25:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000.bk
[INFO ] 2018-10-01 23:25:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407510000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000', took 4746 bytes and 39 ms
[INFO ] 2018-10-01 23:25:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407510000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000'
[INFO ] 2018-10-01 23:25:10,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407460000
[INFO ] 2018-10-01 23:25:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407510000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000', took 4709 bytes and 9 ms
[INFO ] 2018-10-01 23:25:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407510000 ms
[INFO ] 2018-10-01 23:25:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-01 23:25:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538407500000: 
[INFO ] 2018-10-01 23:25:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407490000 ms
[INFO ] 2018-10-01 23:25:13,963 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-01 23:25:13,964 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538407513964.
[WARN ] 2018-10-01 23:25:13,965 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-01 23:25:13,965 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-01 23:25:13,966 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-01 23:25:13,966 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-01 23:25:13,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-01 23:25:13,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 23:25:13,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-01 23:25:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407520000 ms.0 from job set of time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407520000 ms to writer queue
[INFO ] 2018-10-01 23:25:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407520000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000'
[INFO ] 2018-10-01 23:25:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:25:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 10 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:25:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 10 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:25:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:25:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:25:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 10 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:25:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_10_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_10_piece0 in memory on 192.168.0.100:32781 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:25:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:25:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 10.0 with 1 tasks
[INFO ] 2018-10-01 23:25:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:25:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 10.0 (TID 10)
[INFO ] 2018-10-01 23:25:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000.bk
[INFO ] 2018-10-01 23:25:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 49 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:25:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407520000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000', took 4747 bytes and 16 ms
[INFO ] 2018-10-01 23:25:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 10.0 (TID 10). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:25:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 10.0 (TID 10) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:25:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:25:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 10 (isEmpty at StoreMovieEssay.scala:81) finished in 0.007 s
[INFO ] 2018-10-01 23:25:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 10 finished: isEmpty at StoreMovieEssay.scala:81, took 0.018302 s
[INFO ] 2018-10-01 23:25:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407520000 ms.0 from job set of time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.038 s for time 1538407520000 ms (execution: 0.024 s)
[INFO ] 2018-10-01 23:25:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26 from persistence list
[INFO ] 2018-10-01 23:25:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26
[INFO ] 2018-10-01 23:25:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25 from persistence list
[INFO ] 2018-10-01 23:25:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25
[INFO ] 2018-10-01 23:25:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24 from persistence list
[INFO ] 2018-10-01 23:25:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24
[INFO ] 2018-10-01 23:25:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407520000 ms to writer queue
[INFO ] 2018-10-01 23:25:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407520000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000'
[INFO ] 2018-10-01 23:25:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407470000
[INFO ] 2018-10-01 23:25:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407520000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000', took 4708 bytes and 8 ms
[INFO ] 2018-10-01 23:25:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407520000 ms
[INFO ] 2018-10-01 23:25:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 23:25:20,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538407520048 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 23:25:20,053 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 23:25:20,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407500000 ms
[INFO ] 2018-10-01 23:25:30,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538407530000
[INFO ] 2018-10-01 23:25:30,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538407530000 ms.0 from job set of time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:81
[INFO ] 2018-10-01 23:25:30,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 11 (isEmpty at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-01 23:25:30,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 11 (isEmpty at StoreMovieEssay.scala:81)
[INFO ] 2018-10-01 23:25:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-01 23:25:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-01 23:25:30,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 11 (MapPartitionsRDD[32] at filter at StoreMovieEssay.scala:79), which has no missing parents
[INFO ] 2018-10-01 23:25:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-01 23:25:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:30,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 23:25:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407530000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000'
[INFO ] 2018-10-01 23:25:30,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407530000 ms to writer queue
[INFO ] 2018-10-01 23:25:30,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_11_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-10-01 23:25:30,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_11_piece0 in memory on 192.168.0.100:32781 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-10-01 23:25:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-01 23:25:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[32] at filter at StoreMovieEssay.scala:79) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-01 23:25:30,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 11.0 with 1 tasks
[INFO ] 2018-10-01 23:25:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-01 23:25:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 11.0 (TID 11)
[INFO ] 2018-10-01 23:25:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 49 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-01 23:25:30,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 11.0 (TID 11). 702 bytes result sent to driver
[INFO ] 2018-10-01 23:25:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 11.0 (TID 11) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-01 23:25:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-01 23:25:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 11 (isEmpty at StoreMovieEssay.scala:81) finished in 0.007 s
[INFO ] 2018-10-01 23:25:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 11 finished: isEmpty at StoreMovieEssay.scala:81, took 0.020282 s
[INFO ] 2018-10-01 23:25:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538407530000 ms.0 from job set of time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 29 from persistence list
[INFO ] 2018-10-01 23:25:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.041 s for time 1538407530000 ms (execution: 0.028 s)
[INFO ] 2018-10-01 23:25:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 29
[INFO ] 2018-10-01 23:25:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 28 from persistence list
[INFO ] 2018-10-01 23:25:30,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000.bk
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407530000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000', took 4745 bytes and 16 ms
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 28
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 27 from persistence list
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 27
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538407530000 ms to writer queue
[INFO ] 2018-10-01 23:25:30,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538407530000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000'
[INFO ] 2018-10-01 23:25:30,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407480000
[INFO ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538407530000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000', took 4708 bytes and 7 ms
[INFO ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538407530000 ms
[INFO ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538407530051 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-01 23:25:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538407510000 ms
[INFO ] 2018-10-01 23:25:30,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-01 23:25:30,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-01 23:25:30,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-01 23:25:30,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-01 23:25:30,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-01 23:25:30,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-01 23:25:30,148 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-01 23:25:30,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-01 23:25:30,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-01 23:25:30,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-01 23:25:30,168 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-01 23:25:30,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-01 23:25:30,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-01 23:25:30,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-01 23:25:30,179 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-03ee3c3e-539d-4f1e-be30-08daf36c7874
[INFO ] 2018-10-01 23:36:27,801 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291559,16456,)
[INFO ] 2018-10-01 23:36:27,803 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291560,1118154,)
[INFO ] 2018-10-01 23:36:27,803 sample.CommonTest$.reverse(CommonTest.scala:31)
21565
[INFO ] 2018-10-01 23:36:27,806 sample.CommonTest$.reverse(CommonTest.scala:32)
56512
[INFO ] 2018-10-01 23:36:49,902 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291559,16456,)
[INFO ] 2018-10-01 23:36:49,911 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291560,1118154,)
[INFO ] 2018-10-01 23:36:49,911 sample.CommonTest$.reverse(CommonTest.scala:32)
21565
[INFO ] 2018-10-01 23:36:49,916 sample.CommonTest$.reverse(CommonTest.scala:33)
56512
[INFO ] 2018-10-01 23:36:49,917 sample.CommonTest$.testConfig(CommonTest.scala:26)
192.168.0.100:9092
[INFO ] 2018-10-01 23:36:49,917 sample.CommonTest$.testConfig(CommonTest.scala:28)
192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092
