[INFO ] 2018-09-30 00:31:55,500 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:31:56,017 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:31:56,154 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:31:56,155 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:31:56,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:31:56,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:31:56,209 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:31:56,209 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:31:56,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:31:56,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:31:56,521 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 46605.
[INFO ] 2018-09-30 00:31:56,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:31:56,613 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:31:56,615 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:31:56,616 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:31:56,630 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-73721ef0-020f-4957-9d7f-b60842e3b333
[INFO ] 2018-09-30 00:31:56,672 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:31:56,729 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:31:57,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:31:57,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:31:57,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:31:57,206 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41029.
[INFO ] 2018-09-30 00:31:57,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:41029
[INFO ] 2018-09-30 00:31:57,209 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:31:57,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 41029, None)
[INFO ] 2018-09-30 00:31:57,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:41029 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 41029, None)
[INFO ] 2018-09-30 00:31:57,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 41029, None)
[INFO ] 2018-09-30 00:31:57,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 41029, None)
[INFO ] 2018-09-30 00:31:57,679 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:31:57,687 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:31:58,511 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:31:58,549 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:31:58,832 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:31:58,832 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:31:58,833 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:31:58,833 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:31:58,838 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:31:59,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:31:59,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:31:59,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:31:59,208 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:31:59,209 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3a66e67e
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@12e5da86
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:31:59,210 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:31:59,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:31:59,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@4c2af006
[INFO ] 2018-09-30 00:31:59,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:31:59,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:31:59,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:31:59,214 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:31:59,214 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@33425811
[INFO ] 2018-09-30 00:31:59,276 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:31:59,353 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:31:59,380 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:31:59,381 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[WARN ] 2018-09-30 00:31:59,676 org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleResponse(NetworkClient.java:600)
Error while fetching metadata with correlation id 1 : {movie-essay-topic=LEADER_NOT_AVAILABLE}
[INFO ] 2018-09-30 00:31:59,677 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:31:59,677 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:31:59,677 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:31:59,800 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 1
[INFO ] 2018-09-30 00:31:59,801 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:31:59,804 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538238720000
[INFO ] 2018-09-30 00:31:59,805 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538238720000 ms
[INFO ] 2018-09-30 00:31:59,806 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:31:59,818 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:32:00,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238720000 ms
[INFO ] 2018-09-30 00:32:00,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238720000 ms
[INFO ] 2018-09-30 00:32:00,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238720000 ms
[INFO ] 2018-09-30 00:32:00,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238720000 ms.0 from job set of time 1538238720000 ms
[INFO ] 2018-09-30 00:32:00,099 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238720000 ms
[INFO ] 2018-09-30 00:32:00,105 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238720000 ms to writer queue
[INFO ] 2018-09-30 00:32:00,105 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238720000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238720000'
[INFO ] 2018-09-30 00:32:00,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890455000.bk
[INFO ] 2018-09-30 00:32:00,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238720000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238720000', took 4404 bytes and 70 ms
[WARN ] 2018-09-30 00:32:00,228 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-09-30 00:32:00,285 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: saveAsHadoopDataset at CommonUtil.scala:30
[INFO ] 2018-09-30 00:32:00,297 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (saveAsHadoopDataset at CommonUtil.scala:30) with 1 output partitions
[INFO ] 2018-09-30 00:32:00,297 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (saveAsHadoopDataset at CommonUtil.scala:30)
[INFO ] 2018-09-30 00:32:00,297 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:32:00,300 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:32:00,305 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[4] at map at CommonUtil.scala:29), which has no missing parents
[INFO ] 2018-09-30 00:32:00,422 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-09-30 00:32:00,454 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.4 KB, free 1406.3 MB)
[INFO ] 2018-09-30 00:32:00,457 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:41029 (size: 28.4 KB, free: 1406.4 MB)
[INFO ] 2018-09-30 00:32:00,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:32:00,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at CommonUtil.scala:29) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:32:00,492 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:32:00,543 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4918 bytes)
[INFO ] 2018-09-30 00:32:00,554 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:32:01,074 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:122)
Process identifier=hconnection-0x1f6226dd connecting to ZooKeeper ensemble=spark1:2181,spark2:2181,spark3:2181
[WARN ] 2018-09-30 00:32:01,184 org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)
Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
[INFO ] 2018-09-30 00:32:01,207 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:376)
Scheduled snapshot period at 10 second(s).
[INFO ] 2018-09-30 00:32:01,207 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)
HBase metrics system started
[INFO ] 2018-09-30 00:32:01,232 org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:65)
Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:host.name=feng
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.version=1.8.0_181
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.vendor=Oracle Corporation
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
[INFO ] 2018-09-30 00:32:01,246 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.class.path=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/home/feng/software/scala-2.11.11/lib/akka-actor_2.11-2.3.16.jar:/home/feng/software/scala-2.11.11/lib/config-1.2.1.jar:/home/feng/software/scala-2.11.11/lib/jline-2.14.3.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-2.11.0.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-migration_2.11-1.1.0.jar:/home/feng/software/scala-2.11.11/lib/scala-compiler.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-library_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-plugin_2.11.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-library.jar:/home/feng/software/scala-2.11.11/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/scala-2.11.11/lib/scala-reflect.jar:/home/feng/software/scala-2.11.11/lib/scala-swing_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-xml_2.11-1.0.5.jar:/home/feng/software/scala-2.11.11/lib/scalap-2.11.11.jar:/home/feng/software/code/bigdata/target/classes:/home/feng/software/code/jars/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/feng/software/code/jars/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/feng/software/code/jars/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/feng/software/code/jars/org/tukaani/xz/1.0/xz-1.0.jar:/home/feng/software/code/jars/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/feng/software/code/jars/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/home/feng/software/code/jars/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/feng/software/code/jars/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/feng/software/code/jars/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/feng/software/code/jars/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/home/feng/software/code/jars/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/feng/software/code/jars/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/feng/software/code/jars/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/feng/software/code/jars/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/feng/software/code/jars/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/feng/software/code/jars/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/home/feng/software/code/jars/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/home/feng/software/code/jars/net/java/dev/jets3t/jets3t/0.9.3/jets3t-0.9.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/home/feng/software/code/jars/commons-codec/commons-codec/1.8/commons-codec-1.8.jar:/home/feng/software/code/jars/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/feng/software/code/jars/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar:/home/feng/software/code/jars/javax/mail/mail/1.4.7/mail-1.4.7.jar:/home/feng/software/code/jars/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar:/home/feng/software/code/jars/com/jamesmurty/utils/java-xmlbuilder/1.0/java-xmlbuilder-1.0.jar:/home/feng/software/code/jars/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/feng/software/code/jars/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/feng/software/code/jars/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/feng/software/code/jars/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/feng/software/code/jars/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/feng/software/code/jars/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/feng/software/code/jars/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/feng/software/code/jars/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/feng/software/code/jars/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/feng/software/code/jars/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/feng/software/code/jars/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/feng/software/code/jars/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/feng/software/code/jars/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/feng/software/code/jars/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/feng/software/code/jars/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/feng/software/code/jars/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/feng/software/code/jars/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/feng/software/code/jars/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/feng/software/code/jars/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-xml_2.11/1.0.1/scala-xml_2.11-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/feng/software/code/jars/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/feng/software/code/jars/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/feng/software/code/jars/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/feng/software/code/jars/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/feng/software/code/jars/io/netty/netty-all/4.0.43.Final/netty-all-4.0.43.Final.jar:/home/feng/software/code/jars/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/feng/software/code/jars/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/home/feng/software/code/jars/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/home/feng/software/code/jars/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/feng/software/code/jars/oro/oro/2.0.8/oro-2.0.8.jar:/home/feng/software/code/jars/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/feng/software/code/jars/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/home/feng/software/code/jars/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/feng/software/code/jars/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/home/feng/software/code/jars/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/home/feng/software/code/jars/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/home/feng/software/code/jars/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/home/feng/software/code/jars/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/home/feng/software/code/jars/com/101tec/zkclient/0.8/zkclient-0.8.jar:/home/feng/software/code/jars/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/code/jars/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-spark/4.14.0-HBase-1.4/phoenix-spark-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-core/4.14.0-HBase-1.4/phoenix-core-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-api/0.14.0-incubating/tephra-api-0.14.0-incubating.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-core/0.14.0-incubating/tephra-core-0.14.0-incubating.jar:/home/feng/software/code/jars/com/google/inject/guice/3.0/guice-3.0.jar:/home/feng/software/code/jars/javax/inject/javax.inject/1/javax.inject-1.jar:/home/feng/software/code/jars/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/feng/software/code/jars/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/feng/software/code/jars/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/feng/software/code/jars/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/feng/software/code/jars/org/apache/twill/twill-common/0.8.0/twill-common-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-core/0.8.0/twill-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-api/0.8.0/twill-api-0.8.0.jar:/home/feng/software/code/jars/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-api/0.8.0/twill-discovery-api-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-core/0.8.0/twill-discovery-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-zookeeper/0.8.0/twill-zookeeper-0.8.0.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-hbase-compat-1.4/0.14.0-incubating/tephra-hbase-compat-1.4-0.14.0-incubating.jar:/home/feng/software/code/jars/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/feng/software/code/jars/jline/jline/2.11/jline-2.11.jar:/home/feng/software/code/jars/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/feng/software/code/jars/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/feng/software/code/jars/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/feng/software/code/jars/junit/junit/4.12/junit-4.12.jar:/home/feng/software/code/jars/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/feng/software/code/jars/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/feng/software/code/jars/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/feng/software/code/jars/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/feng/software/code/jars/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/feng/software/code/jars/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/feng/software/code/jars/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-annotations/1.4.0/hbase-annotations-1.4.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-protocol/1.4.0/hbase-protocol-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-server/1.4.0/hbase-server-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-procedure/1.4.0/hbase-procedure-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0-tests.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-prefix-tree/1.4.0/hbase-prefix-tree-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics-api/1.4.0/hbase-metrics-api-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics/1.4.0/hbase-metrics-1.4.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/feng/software/code/jars/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/feng/software/code/jars/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/feng/software/code/jars/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/feng/software/code/jars/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop-compat/1.4.0/hbase-hadoop-compat-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop2-compat/1.4.0/hbase-hadoop2-compat-1.4.0.jar:/home/feng/software/code/jars/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/feng/software/code/jars/com/salesforce/i18n/i18n-util/1.0.4/i18n-util-1.0.4.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j/60.2/icu4j-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-localespi/60.2/icu4j-localespi-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-charset/60.2/icu4j-charset-60.2.jar:/home/feng/software/code/jars/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-common/2.7.5/hadoop-common-2.7.5.jar:/home/feng/software/code/jars/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/feng/software/code/jars/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/feng/software/code/jars/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/feng/software/code/jars/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/feng/software/code/jars/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/feng/software/code/jars/asm/asm/3.1/asm-3.1.jar:/home/feng/software/code/jars/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/feng/software/code/jars/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/feng/software/code/jars/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/feng/software/code/jars/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/feng/software/code/jars/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-auth/2.7.5/hadoop-auth-2.7.5.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/feng/software/code/jars/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/feng/software/code/jars/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/feng/software/code/jars/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-client/1.4.0/hbase-client-1.4.0.jar:/home/feng/software/code/jars/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/feng/software/code/jars/io/debezium/debezium-core/0.8.1.Final/debezium-core-0.8.1.Final.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar:/home/feng/software/code/jars/com/alibaba/fastjson/1.2.47/fastjson-1.2.47.jar:/home/feng/software/code/jars/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/feng/software/idea/idea-IC-181.5281.24/lib/idea_rt.jar
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.io.tmpdir=/tmp
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.compiler=<NA>
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.name=Linux
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.arch=amd64
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.version=4.13.0-36-generic
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.name=feng
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.home=/home/feng
[INFO ] 2018-09-30 00:32:01,248 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.dir=/home/feng/software/code/bigdata
[INFO ] 2018-09-30 00:32:01,249 org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
Initiating client connection, connectString=spark1:2181,spark2:2181,spark3:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@20ac2712
[INFO ] 2018-09-30 00:32:01,273 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:01,275 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:01,380 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:01,381 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:01,481 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:01,482 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:02,583 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:02,583 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:02,684 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:02,685 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:02,786 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:02,786 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:03,887 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:03,888 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:03,989 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:03,990 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:04,091 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:04,091 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:05,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238725000 ms
[INFO ] 2018-09-30 00:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238725000 ms
[INFO ] 2018-09-30 00:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238725000 ms
[INFO ] 2018-09-30 00:32:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238725000 ms
[INFO ] 2018-09-30 00:32:05,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238725000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238725000'
[INFO ] 2018-09-30 00:32:05,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238725000 ms to writer queue
[INFO ] 2018-09-30 00:32:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890455000
[INFO ] 2018-09-30 00:32:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238725000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238725000', took 4425 bytes and 14 ms
[INFO ] 2018-09-30 00:32:05,192 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:05,193 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:05,293 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:05,294 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:05,394 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:05,395 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:06,495 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:06,496 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:06,597 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:06,598 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:06,699 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:06,699 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:07,800 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:07,801 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:07,901 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:07,902 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:08,003 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:08,004 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:09,104 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:09,105 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:09,205 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:09,206 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:09,307 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:09,308 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:10,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238730000 ms
[INFO ] 2018-09-30 00:32:10,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238730000 ms
[INFO ] 2018-09-30 00:32:10,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238730000 ms
[INFO ] 2018-09-30 00:32:10,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238730000 ms
[INFO ] 2018-09-30 00:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238730000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238730000'
[INFO ] 2018-09-30 00:32:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238730000 ms to writer queue
[INFO ] 2018-09-30 00:32:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890460000.bk
[INFO ] 2018-09-30 00:32:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238730000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238730000', took 4442 bytes and 20 ms
[INFO ] 2018-09-30 00:32:10,409 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:10,409 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:10,510 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:10,511 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:10,611 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:10,612 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:11,713 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:11,714 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:11,814 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:11,815 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:11,916 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:11,917 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:13,017 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:13,018 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:13,119 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:13,119 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:13,220 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:13,220 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:14,140 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:32:14,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538238734142.
[WARN ] 2018-09-30 00:32:14,143 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:32:14,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:32:14,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:32:14,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:32:14,145 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:32:14,145 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:32:14,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:32:14,321 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:14,321 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:14,422 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:14,423 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:14,523 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:14,524 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238735000 ms
[INFO ] 2018-09-30 00:32:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238735000 ms
[INFO ] 2018-09-30 00:32:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238735000 ms
[INFO ] 2018-09-30 00:32:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238735000 ms
[INFO ] 2018-09-30 00:32:15,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238735000 ms to writer queue
[INFO ] 2018-09-30 00:32:15,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238735000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238735000'
[INFO ] 2018-09-30 00:32:15,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890460000
[INFO ] 2018-09-30 00:32:15,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238735000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238735000', took 4452 bytes and 17 ms
[INFO ] 2018-09-30 00:32:15,624 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:15,625 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:15,725 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:15,726 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:15,826 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:15,827 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:16,927 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:16,928 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:17,029 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:17,030 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:17,130 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:17,131 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:18,232 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:18,232 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:18,333 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[ERROR] 2018-09-30 00:32:18,333 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper exists failed after 4 attempts
[WARN ] 2018-09-30 00:32:18,334 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[WARN ] 2018-09-30 00:32:18,334 org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:428)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to set watcher on znode (/hbase/hbaseid)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:231)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:425)
	at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:945)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:721)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:68)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:94)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-09-30 00:32:18,335 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:231)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:425)
	at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:945)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:721)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:68)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:94)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:32:18,336 org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:110)
Can't retrieve clusterId from Zookeeper
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:231)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:425)
	at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:945)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:721)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:68)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:94)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-09-30 00:32:18,435 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:18,435 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:19,536 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:19,537 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:19,637 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:19,638 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:19,739 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:19,739 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:20,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538238740000
[INFO ] 2018-09-30 00:32:20,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238740000 ms
[INFO ] 2018-09-30 00:32:20,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238740000 ms
[INFO ] 2018-09-30 00:32:20,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238740000 ms
[INFO ] 2018-09-30 00:32:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 00:32:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:32:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238740000 ms
[INFO ] 2018-09-30 00:32:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238740000 ms to writer queue
[INFO ] 2018-09-30 00:32:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238740000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238740000'
[INFO ] 2018-09-30 00:32:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890530000.bk
[INFO ] 2018-09-30 00:32:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238740000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238740000', took 4463 bytes and 29 ms
[INFO ] 2018-09-30 00:32:20,840 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:20,841 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:20,941 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:20,942 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:21,042 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:21,043 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:22,144 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:22,144 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:22,244 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:22,245 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:22,345 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:22,346 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:23,447 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:23,447 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:23,548 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:23,548 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:23,649 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:23,650 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:24,750 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:24,751 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:24,852 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:24,853 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:24,953 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:24,954 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:26,055 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:26,055 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:26,156 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:26,157 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:26,257 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:26,259 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:27,359 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:27,360 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:27,461 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:27,462 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:27,563 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:27,564 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:28,664 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:28,665 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:28,765 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:28,766 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:28,866 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:28,867 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:29,968 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:29,969 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:30,069 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:30,070 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:30,171 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:30,172 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:31,272 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:31,273 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:31,373 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:31,374 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:31,475 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:31,476 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:32,576 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:32,577 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:32,678 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:32,678 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:32,779 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:32,780 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:33,880 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:33,881 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:33,981 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:33,982 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:34,082 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:34,083 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:35,183 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:35,185 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[ERROR] 2018-09-30 00:32:35,285 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper getData failed after 4 attempts
[INFO ] 2018-09-30 00:32:35,285 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:35,285 org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:637)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to get data of znode /hbase/meta-region-server
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-09-30 00:32:35,286 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:32:35,286 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:35,387 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:35,388 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:36,489 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:36,489 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:36,590 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:36,591 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:36,691 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:36,692 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:37,792 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:37,793 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:37,893 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:37,894 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:37,995 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:37,996 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:39,097 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:39,098 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:39,198 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:39,199 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:39,299 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:39,300 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:40,401 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:40,402 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:40,502 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:40,503 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:40,604 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:40,604 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:41,705 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:41,706 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:41,806 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:41,807 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:41,908 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:41,909 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:43,009 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:43,010 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:43,111 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:43,111 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:43,212 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:43,213 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:44,313 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:44,314 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:44,415 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:44,416 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:44,516 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:44,517 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:45,617 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:45,619 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:45,719 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:45,720 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:45,821 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:45,821 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:46,922 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:46,923 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:47,023 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:47,024 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:47,125 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:47,125 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:48,226 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:48,226 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:48,327 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:48,327 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:48,428 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:48,429 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:49,529 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:49,530 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:49,630 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:49,631 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:49,732 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:49,733 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:50,833 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:50,834 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:50,934 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:50,935 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:51,036 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:51,036 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:52,137 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:52,137 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:52,238 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[ERROR] 2018-09-30 00:32:52,238 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper getData failed after 4 attempts
[WARN ] 2018-09-30 00:32:52,238 org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:637)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to get data of znode /hbase/meta-region-server
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:32:52,239 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[ERROR] 2018-09-30 00:32:52,239 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-09-30 00:32:52,340 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:52,341 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:53,441 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:53,442 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:53,543 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:53,544 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:53,644 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:53,645 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:54,746 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:54,747 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:54,847 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:54,848 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:54,949 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:54,950 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:56,050 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:56,051 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:56,152 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:56,152 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:56,253 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:56,254 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:57,354 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:57,355 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:57,455 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:57,456 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:57,556 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:57,557 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:58,657 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:58,658 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:58,758 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:58,759 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:58,860 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:58,861 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:32:59,961 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:32:59,962 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:00,062 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:00,064 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:00,164 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:00,165 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:01,266 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:01,266 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:01,367 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:01,368 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:01,468 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:01,469 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:02,570 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:02,570 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:02,671 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:02,671 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:02,772 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:02,773 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:03,873 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:03,875 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:03,975 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:03,976 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:04,076 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:04,077 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[WARN ] 2018-09-30 00:33:04,150 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Timed out while stopping the job generator (timeout = 50000)
[INFO ] 2018-09-30 00:33:04,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:33:04,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-09-30 00:33:04,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 00:33:05,177 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:05,178 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:05,278 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:05,279 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:05,380 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:05,381 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:06,481 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:06,482 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:06,582 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:06,583 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:06,683 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:06,684 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:07,784 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:07,785 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:07,886 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:07,887 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:07,987 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:07,988 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:09,089 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:09,090 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:09,190 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[ERROR] 2018-09-30 00:33:09,190 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper getData failed after 4 attempts
[WARN ] 2018-09-30 00:33:09,191 org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:637)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to get data of znode /hbase/meta-region-server
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-09-30 00:33:09,191 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:33:09,191 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:09,292 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:09,293 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:10,394 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:10,395 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:10,495 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:10,496 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:10,597 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:10,597 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:11,698 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:11,699 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:11,799 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:11,800 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:11,900 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:11,901 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:13,002 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:13,002 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:13,103 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:13,103 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:13,204 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:13,205 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:14,305 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:14,306 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:14,407 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:14,407 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:14,508 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:14,509 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:15,609 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:15,610 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:15,710 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:15,711 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:15,811 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:15,813 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:16,914 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:16,915 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:17,015 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:17,016 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:17,117 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:17,117 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:18,218 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:18,219 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:18,319 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:18,320 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:18,421 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:18,422 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:19,522 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:19,524 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:19,624 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:19,625 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:19,726 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:19,727 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:20,827 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:20,828 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:20,929 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:20,929 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:21,030 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:21,030 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:22,131 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:22,132 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:22,233 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:22,233 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:22,334 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:22,335 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:23,435 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:23,436 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:23,537 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:23,537 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:23,638 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:23,639 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:24,739 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:24,740 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:24,841 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:24,842 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:24,942 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:24,943 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:26,043 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:26,044 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:26,144 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[ERROR] 2018-09-30 00:33:26,144 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper getData failed after 4 attempts
[WARN ] 2018-09-30 00:33:26,145 org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:637)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to get data of znode /hbase/meta-region-server
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:33:26,145 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[ERROR] 2018-09-30 00:33:26,145 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:356)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:277)
	at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:438)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:312)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1324)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-09-30 00:33:26,246 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:26,247 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:27,347 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:27,348 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:27,449 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:27,449 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:27,550 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:27,550 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:28,651 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:28,651 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:28,752 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:28,753 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:28,853 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:28,854 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:29,954 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:29,955 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:30,056 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:30,057 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:30,157 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:30,158 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:31,258 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:31,260 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:31,360 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:31,361 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:31,461 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:31,462 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:32,563 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:32,564 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:32,664 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:32,665 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:32,765 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:32,766 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:33,867 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:33,868 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:33,968 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:33,969 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:34,069 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:34,070 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:35,171 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:35,171 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:35,272 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:35,273 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:35,373 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:35,374 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:36,475 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:36,476 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:36,576 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:36,577 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:36,677 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:36,678 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:37,779 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:37,780 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:37,880 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:37,881 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:37,982 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:37,982 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:39,083 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:39,084 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:39,184 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:39,185 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:39,285 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:39,286 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:40,387 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:40,388 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:40,488 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:40,489 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:40,589 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:40,590 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:41,690 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:41,692 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:41,792 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:41,793 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:41,893 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:41,894 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:42,995 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:42,996 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:43,096 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[ERROR] 2018-09-30 00:33:43,096 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.retryOrThrow(RecoverableZooKeeper.java:300)
ZooKeeper getData failed after 4 attempts
[WARN ] 2018-09-30 00:33:43,096 org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:637)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Unable to get data of znode /hbase/meta-region-server
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.relocateRegion(ConnectionManager.java:1192)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1402)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-09-30 00:33:43,097 org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:734)
hconnection-0x1f6226dd0x0, quorum=spark1:2181,spark2:2181,spark3:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:397)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:629)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState(MetaTableLocator.java:487)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation(MetaTableLocator.java:168)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:607)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:588)
	at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:561)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:61)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1251)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1218)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.relocateRegion(ConnectionManager.java:1192)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1402)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-09-30 00:33:43,097 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:43,197 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:43,198 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:44,299 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:44,300 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:44,400 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:44,401 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:44,501 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:44,502 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:45,603 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:45,603 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:45,704 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:45,705 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:45,805 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:45,806 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:46,907 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark3/192.168.0.107:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:46,908 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:47,008 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark2/192.168.0.108:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:47,009 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:33:47,110 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server spark1/192.168.0.101:2181. Will not attempt to authenticate using SASL (unknown error)
[WARN ] 2018-09-30 00:33:47,110 org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1102)
Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
[INFO ] 2018-09-30 00:34:59,747 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:35:00,187 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:35:00,322 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:35:00,322 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:35:00,349 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:35:00,361 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:35:00,362 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:35:00,362 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:35:00,362 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:35:00,363 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:35:00,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 35429.
[INFO ] 2018-09-30 00:35:00,607 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:35:00,620 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:35:00,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:35:00,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:35:00,634 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-87c8c5c6-811f-4b04-9fe1-d5dcd496008b
[INFO ] 2018-09-30 00:35:00,677 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:35:00,724 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:35:00,871 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:35:00,936 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:35:01,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:35:01,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38363.
[INFO ] 2018-09-30 00:35:01,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:38363
[INFO ] 2018-09-30 00:35:01,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:35:01,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 38363, None)
[INFO ] 2018-09-30 00:35:01,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:38363 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 38363, None)
[INFO ] 2018-09-30 00:35:01,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 38363, None)
[INFO ] 2018-09-30 00:35:01,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 38363, None)
[INFO ] 2018-09-30 00:35:01,301 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:35:01,301 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:35:01,872 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:35:01,883 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:35:02,108 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:35:02,109 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:35:02,110 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:35:02,110 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:35:02,115 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:35:02,261 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:35:02,270 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:35:02,270 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:35:02,271 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:35:02,271 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1b956cfa
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2373ad99
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:35:02,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@1c8f6a90
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:35:02,273 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2bc426f0
[INFO ] 2018-09-30 00:35:02,313 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:35:02,377 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:35:02,399 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:35:02,399 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:35:02,534 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:35:02,535 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:35:02,536 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:35:02,551 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 1
[INFO ] 2018-09-30 00:35:02,552 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [movie-essay-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:35:02,592 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538238905000
[INFO ] 2018-09-30 00:35:02,592 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538238905000 ms
[INFO ] 2018-09-30 00:35:02,593 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:35:02,599 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:35:05,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238905000 ms.0 from job set of time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238905000 ms to writer queue
[INFO ] 2018-09-30 00:35:05,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238905000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000'
[INFO ] 2018-09-30 00:35:05,148 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890530000
[INFO ] 2018-09-30 00:35:05,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238905000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000', took 4497 bytes and 56 ms
[INFO ] 2018-09-30 00:35:05,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:72
[INFO ] 2018-09-30 00:35:05,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:72) with 1 output partitions
[INFO ] 2018-09-30 00:35:05,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:72)
[INFO ] 2018-09-30 00:35:05,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:35:05,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:35:05,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:35:05,205 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:05,216 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2019.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:05,218 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:38363 (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:05,220 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:35:05,236 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:35:05,237 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:35:05,272 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:35:05,278 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:35:05,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:35:05,368 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:35:05,376 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 114 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:35:05,378 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:35:05,382 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:72) finished in 0.132 s
[INFO ] 2018-09-30 00:35:05,398 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:72, took 0.246446 s
[INFO ] 2018-09-30 00:35:05,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538238905000 ms.0 from job set of time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,427 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.426 s for time 1538238905000 ms (execution: 0.342 s)
[INFO ] 2018-09-30 00:35:05,468 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,468 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,468 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,470 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238905000 ms to writer queue
[INFO ] 2018-09-30 00:35:05,470 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238905000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000'
[INFO ] 2018-09-30 00:35:05,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890535000.bk
[INFO ] 2018-09-30 00:35:05,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238905000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000', took 4494 bytes and 20 ms
[INFO ] 2018-09-30 00:35:05,494 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,496 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538238905000 ms
[INFO ] 2018-09-30 00:35:05,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:35:05,524 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538238900000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1537890530789-1537890590789
[INFO ] 2018-09-30 00:35:05,527 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:35:05,529 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538238900000
[INFO ] 2018-09-30 00:35:05,537 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:38363 in memory (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238910000 ms.0 from job set of time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238910000 ms to writer queue
[INFO ] 2018-09-30 00:35:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238910000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000'
[INFO ] 2018-09-30 00:35:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:72
[INFO ] 2018-09-30 00:35:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:72) with 1 output partitions
[INFO ] 2018-09-30 00:35:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:72)
[INFO ] 2018-09-30 00:35:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:35:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:35:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:35:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:10,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:38363 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:35:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890535000
[INFO ] 2018-09-30 00:35:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:35:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 00:35:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238910000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000', took 4536 bytes and 20 ms
[INFO ] 2018-09-30 00:35:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:35:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 00:35:10,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:35:10,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:35:10,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:35:10,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:35:10,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:72) finished in 0.008 s
[INFO ] 2018-09-30 00:35:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:72, took 0.021119 s
[INFO ] 2018-09-30 00:35:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538238910000 ms.0 from job set of time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.058 s for time 1538238910000 ms (execution: 0.031 s)
[INFO ] 2018-09-30 00:35:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 00:35:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 00:35:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 00:35:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 00:35:10,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 00:35:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 00:35:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238910000 ms to writer queue
[INFO ] 2018-09-30 00:35:10,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238910000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000'
[INFO ] 2018-09-30 00:35:10,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890540000.bk
[INFO ] 2018-09-30 00:35:10,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238910000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000', took 4495 bytes and 22 ms
[INFO ] 2018-09-30 00:35:10,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538238910000 ms
[INFO ] 2018-09-30 00:35:10,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:35:10,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538238905000: 
[INFO ] 2018-09-30 00:35:10,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:35:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238915000 ms.0 from job set of time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:72
[INFO ] 2018-09-30 00:35:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238915000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000'
[INFO ] 2018-09-30 00:35:15,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238915000 ms to writer queue
[INFO ] 2018-09-30 00:35:15,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:72) with 1 output partitions
[INFO ] 2018-09-30 00:35:15,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:72)
[INFO ] 2018-09-30 00:35:15,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:35:15,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:35:15,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:35:15,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:15,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2018.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:38363 (size: 2018.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:35:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:35:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 00:35:15,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:35:15,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 00:35:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1537890540000
[INFO ] 2018-09-30 00:35:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238915000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000', took 4535 bytes and 29 ms
[INFO ] 2018-09-30 00:35:15,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:35:15,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:35:15,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:35:15,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:35:15,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:72) finished in 0.019 s
[INFO ] 2018-09-30 00:35:15,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:72, took 0.039455 s
[INFO ] 2018-09-30 00:35:15,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538238915000 ms.0 from job set of time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.056 s for time 1538238915000 ms (execution: 0.048 s)
[INFO ] 2018-09-30 00:35:15,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 00:35:15,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 00:35:15,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 00:35:15,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 00:35:15,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 00:35:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 00:35:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238915000 ms to writer queue
[INFO ] 2018-09-30 00:35:15,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238915000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000'
[INFO ] 2018-09-30 00:35:15,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238720000
[INFO ] 2018-09-30 00:35:15,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238915000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000', took 4495 bytes and 11 ms
[INFO ] 2018-09-30 00:35:15,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538238915000 ms
[INFO ] 2018-09-30 00:35:15,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:35:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538238910000: 
[INFO ] 2018-09-30 00:35:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538238905000 ms
[INFO ] 2018-09-30 00:35:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238920000 ms.0 from job set of time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:72
[INFO ] 2018-09-30 00:35:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:72) with 1 output partitions
[INFO ] 2018-09-30 00:35:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:72)
[INFO ] 2018-09-30 00:35:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:35:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:35:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:35:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:38363 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:35:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:35:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-09-30 00:35:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238920000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000'
[INFO ] 2018-09-30 00:35:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238920000 ms to writer queue
[INFO ] 2018-09-30 00:35:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:35:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-09-30 00:35:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:35:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:35:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:35:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:35:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:72) finished in 0.008 s
[INFO ] 2018-09-30 00:35:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:72, took 0.019984 s
[INFO ] 2018-09-30 00:35:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538238920000 ms.0 from job set of time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.039 s for time 1538238920000 ms (execution: 0.027 s)
[INFO ] 2018-09-30 00:35:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-09-30 00:35:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-09-30 00:35:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-09-30 00:35:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-09-30 00:35:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-09-30 00:35:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-09-30 00:35:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238920000 ms to writer queue
[INFO ] 2018-09-30 00:35:20,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238725000
[INFO ] 2018-09-30 00:35:20,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238920000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000', took 4537 bytes and 17 ms
[INFO ] 2018-09-30 00:35:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238920000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000'
[INFO ] 2018-09-30 00:35:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238730000
[INFO ] 2018-09-30 00:35:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238920000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000', took 4495 bytes and 9 ms
[INFO ] 2018-09-30 00:35:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538238920000 ms
[INFO ] 2018-09-30 00:35:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:35:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538238915000: 
[INFO ] 2018-09-30 00:35:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538238910000 ms
[INFO ] 2018-09-30 00:35:23,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:35:23,286 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538238923286.
[WARN ] 2018-09-30 00:35:23,286 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:35:23,287 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:35:23,288 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:35:23,288 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:35:23,289 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:35:23,289 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:35:23,290 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:35:25,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538238925000 ms.0 from job set of time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238925000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000'
[INFO ] 2018-09-30 00:35:25,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238925000 ms to writer queue
[INFO ] 2018-09-30 00:35:25,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:72
[INFO ] 2018-09-30 00:35:25,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:72) with 1 output partitions
[INFO ] 2018-09-30 00:35:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:72)
[INFO ] 2018-09-30 00:35:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:35:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:35:25,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:35:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238735000
[INFO ] 2018-09-30 00:35:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238925000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000', took 4535 bytes and 23 ms
[INFO ] 2018-09-30 00:35:25,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:35:25,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:38363 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:35:25,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:35:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:35:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-09-30 00:35:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:35:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-09-30 00:35:25,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:35:25,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:35:25,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:35:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:35:25,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:72) finished in 0.006 s
[INFO ] 2018-09-30 00:35:25,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:72, took 0.021158 s
[INFO ] 2018-09-30 00:35:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538238925000 ms.0 from job set of time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.046 s for time 1538238925000 ms (execution: 0.035 s)
[INFO ] 2018-09-30 00:35:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-09-30 00:35:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-09-30 00:35:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-09-30 00:35:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-09-30 00:35:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-09-30 00:35:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-09-30 00:35:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538238925000 ms to writer queue
[INFO ] 2018-09-30 00:35:25,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538238925000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000'
[INFO ] 2018-09-30 00:35:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238740000
[INFO ] 2018-09-30 00:35:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538238925000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000', took 4495 bytes and 7 ms
[INFO ] 2018-09-30 00:35:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538238925000 ms
[INFO ] 2018-09-30 00:35:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:35:25,058 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538238925057 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:35:25,060 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:35:25,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538238915000 ms
[INFO ] 2018-09-30 00:38:29,148 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:38:29,572 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:38:29,676 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:38:29,677 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:38:29,722 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:38:29,738 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:38:29,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:38:29,740 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:38:29,741 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:38:29,741 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:38:29,991 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 40721.
[INFO ] 2018-09-30 00:38:30,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:38:30,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:38:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:38:30,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:38:30,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-59b4ed1d-2eed-4233-92ab-83daedcd88e9
[INFO ] 2018-09-30 00:38:30,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:38:30,135 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:38:30,302 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:38:30,362 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:38:30,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:38:30,493 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44313.
[INFO ] 2018-09-30 00:38:30,493 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:44313
[INFO ] 2018-09-30 00:38:30,494 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:38:30,496 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 44313, None)
[INFO ] 2018-09-30 00:38:30,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:44313 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 44313, None)
[INFO ] 2018-09-30 00:38:30,502 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 44313, None)
[INFO ] 2018-09-30 00:38:30,503 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 44313, None)
[INFO ] 2018-09-30 00:38:30,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:38:30,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:38:31,306 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:38:31,317 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:38:31,515 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:38:31,515 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:38:31,516 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:38:31,524 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:38:31,529 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:38:31,676 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:38:31,691 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:38:31,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:38:31,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:38:31,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@ff23ae7
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@617389a
[INFO ] 2018-09-30 00:38:31,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@c2584d3
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:38:31,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7d3c09ec
[INFO ] 2018-09-30 00:38:31,749 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:38:31,849 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:38:31,892 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:38:31,892 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:38:32,032 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:38:32,034 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:38:32,035 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:38:32,054 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 3
[INFO ] 2018-09-30 00:38:32,056 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [movie-essay-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:38:32,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538239115000
[INFO ] 2018-09-30 00:38:32,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538239115000 ms
[INFO ] 2018-09-30 00:38:32,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:38:32,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:38:35,567 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,571 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239115000 ms.0 from job set of time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,571 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:38:35,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,588 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239115000 ms to writer queue
[INFO ] 2018-09-30 00:38:35,589 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239115000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000'
[INFO ] 2018-09-30 00:38:35,619 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:38:35,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:38:35,633 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:38:35,633 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:38:35,634 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:38:35,639 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000.bk
[INFO ] 2018-09-30 00:38:35,639 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:38:35,640 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239115000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000', took 4497 bytes and 51 ms
[INFO ] 2018-09-30 00:38:35,678 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:35,689 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2019.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:35,691 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:44313 (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:35,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:38:35,710 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:38:35,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:38:35,746 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:38:35,751 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:38:35,820 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:38:35,839 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:38:35,846 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 112 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:38:35,848 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:38:35,852 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:73) finished in 0.128 s
[INFO ] 2018-09-30 00:38:35,868 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:73, took 0.247997 s
[INFO ] 2018-09-30 00:38:35,872 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239115000 ms.0 from job set of time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,873 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.872 s for time 1538239115000 ms (execution: 0.302 s)
[INFO ] 2018-09-30 00:38:35,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239115000 ms to writer queue
[INFO ] 2018-09-30 00:38:35,883 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239115000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000'
[INFO ] 2018-09-30 00:38:35,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238905000
[INFO ] 2018-09-30 00:38:35,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239115000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000', took 4494 bytes and 41 ms
[INFO ] 2018-09-30 00:38:35,925 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239115000 ms
[INFO ] 2018-09-30 00:38:35,930 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:38:35,942 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239110000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538238905511-1538238965511
[INFO ] 2018-09-30 00:38:35,944 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:38:35,945 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239110000
[INFO ] 2018-09-30 00:38:40,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239120000 ms.0 from job set of time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:38:40,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239120000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000'
[INFO ] 2018-09-30 00:38:40,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239120000 ms to writer queue
[INFO ] 2018-09-30 00:38:40,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:38:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:38:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:38:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:38:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:38:40,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:38:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:40,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:40,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:38:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:38:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 00:38:40,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:38:40,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 00:38:40,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:38:40,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:38:40,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:44313 in memory (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000.bk
[INFO ] 2018-09-30 00:38:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239120000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000', took 4535 bytes and 49 ms
[INFO ] 2018-09-30 00:38:40,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 23 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:38:40,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:38:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:73) finished in 0.026 s
[INFO ] 2018-09-30 00:38:40,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:73, took 0.043470 s
[INFO ] 2018-09-30 00:38:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239120000 ms.0 from job set of time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.077 s for time 1538239120000 ms (execution: 0.062 s)
[INFO ] 2018-09-30 00:38:40,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 00:38:40,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 00:38:40,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 00:38:40,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 00:38:40,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 00:38:40,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 00:38:40,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239120000 ms to writer queue
[INFO ] 2018-09-30 00:38:40,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239120000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000'
[INFO ] 2018-09-30 00:38:40,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238910000
[INFO ] 2018-09-30 00:38:40,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239120000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000', took 4494 bytes and 11 ms
[INFO ] 2018-09-30 00:38:40,097 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239120000 ms
[INFO ] 2018-09-30 00:38:40,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:38:40,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239115000: 
[INFO ] 2018-09-30 00:38:40,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:38:45,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:38:45,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239125000 ms to writer queue
[INFO ] 2018-09-30 00:38:45,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239125000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000'
[INFO ] 2018-09-30 00:38:45,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239125000 ms.0 from job set of time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:38:45,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:38:45,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:38:45,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:38:45,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:38:45,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:38:45,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:45,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2018.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:45,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:44313 (size: 2018.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:45,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:38:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:38:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 00:38:45,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:38:45,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 00:38:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:38:45,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000.bk
[INFO ] 2018-09-30 00:38:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239125000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000', took 4533 bytes and 27 ms
[INFO ] 2018-09-30 00:38:45,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:38:45,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:38:45,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:38:45,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:73) finished in 0.009 s
[INFO ] 2018-09-30 00:38:45,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:73, took 0.025259 s
[INFO ] 2018-09-30 00:38:45,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239125000 ms.0 from job set of time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.037 s for time 1538239125000 ms (execution: 0.029 s)
[INFO ] 2018-09-30 00:38:45,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 00:38:45,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 00:38:45,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 00:38:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 00:38:45,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 00:38:45,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 00:38:45,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239125000 ms to writer queue
[INFO ] 2018-09-30 00:38:45,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239125000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000'
[INFO ] 2018-09-30 00:38:45,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238915000
[INFO ] 2018-09-30 00:38:45,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239125000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000', took 4494 bytes and 10 ms
[INFO ] 2018-09-30 00:38:45,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239125000 ms
[INFO ] 2018-09-30 00:38:45,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:38:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239120000: 
[INFO ] 2018-09-30 00:38:45,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239115000 ms
[INFO ] 2018-09-30 00:38:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:38:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239130000 ms.0 from job set of time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:38:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:38:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:38:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:38:50,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:38:50,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239130000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000'
[INFO ] 2018-09-30 00:38:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239130000 ms to writer queue
[INFO ] 2018-09-30 00:38:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:38:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:50,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:50,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:38:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:38:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-09-30 00:38:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:38:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-09-30 00:38:50,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:38:50,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:38:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 17 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:38:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:73) finished in 0.018 s
[INFO ] 2018-09-30 00:38:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:73, took 0.028376 s
[INFO ] 2018-09-30 00:38:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:38:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239130000 ms.0 from job set of time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.052 s for time 1538239130000 ms (execution: 0.035 s)
[INFO ] 2018-09-30 00:38:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-09-30 00:38:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-09-30 00:38:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-09-30 00:38:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-09-30 00:38:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-09-30 00:38:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-09-30 00:38:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000.bk
[INFO ] 2018-09-30 00:38:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239130000 ms to writer queue
[INFO ] 2018-09-30 00:38:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239130000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000', took 4535 bytes and 32 ms
[INFO ] 2018-09-30 00:38:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239130000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000'
[INFO ] 2018-09-30 00:38:50,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238920000
[INFO ] 2018-09-30 00:38:50,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239130000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000', took 4494 bytes and 9 ms
[INFO ] 2018-09-30 00:38:50,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239130000 ms
[INFO ] 2018-09-30 00:38:50,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:38:50,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239125000: 
[INFO ] 2018-09-30 00:38:50,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239120000 ms
[INFO ] 2018-09-30 00:38:55,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239135000 ms.0 from job set of time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:38:55,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239135000 ms to writer queue
[INFO ] 2018-09-30 00:38:55,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239135000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000'
[INFO ] 2018-09-30 00:38:55,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:38:55,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:38:55,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:38:55,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:38:55,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:38:55,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:38:55,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:55,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:38:55,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:38:55,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:38:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:38:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-09-30 00:38:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:38:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-09-30 00:38:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:38:55,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 766 bytes result sent to driver
[INFO ] 2018-09-30 00:38:55,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 10 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:38:55,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:38:55,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:73) finished in 0.012 s
[INFO ] 2018-09-30 00:38:55,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:73, took 0.020970 s
[INFO ] 2018-09-30 00:38:55,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239135000 ms.0 from job set of time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.042 s for time 1538239135000 ms (execution: 0.027 s)
[INFO ] 2018-09-30 00:38:55,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-09-30 00:38:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-09-30 00:38:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-09-30 00:38:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-09-30 00:38:55,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-09-30 00:38:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-09-30 00:38:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000.bk
[INFO ] 2018-09-30 00:38:55,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239135000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000', took 4533 bytes and 31 ms
[INFO ] 2018-09-30 00:38:55,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239135000 ms to writer queue
[INFO ] 2018-09-30 00:38:55,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239135000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000'
[INFO ] 2018-09-30 00:38:55,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538238925000
[INFO ] 2018-09-30 00:38:55,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239135000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000', took 4494 bytes and 9 ms
[INFO ] 2018-09-30 00:38:55,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239135000 ms
[INFO ] 2018-09-30 00:38:55,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:38:55,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239130000: 
[INFO ] 2018-09-30 00:38:55,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239125000 ms
[INFO ] 2018-09-30 00:39:00,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:00,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239140000 ms to writer queue
[INFO ] 2018-09-30 00:39:00,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239140000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000'
[INFO ] 2018-09-30 00:39:00,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:39:00,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239140000 ms.0 from job set of time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:39:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:39:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:00,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:39:00,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:00,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2019.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:00,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:44313 (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:00,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-09-30 00:39:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:39:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000.bk
[INFO ] 2018-09-30 00:39:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239140000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000', took 4533 bytes and 15 ms
[INFO ] 2018-09-30 00:39:00,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-09-30 00:39:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:39:00,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:39:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (collect at StoreMovieEssay.scala:73) finished in 0.009 s
[INFO ] 2018-09-30 00:39:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: collect at StoreMovieEssay.scala:73, took 0.018495 s
[INFO ] 2018-09-30 00:39:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239140000 ms.0 from job set of time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.038 s for time 1538239140000 ms (execution: 0.019 s)
[INFO ] 2018-09-30 00:39:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-09-30 00:39:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-09-30 00:39:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-09-30 00:39:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-09-30 00:39:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-09-30 00:39:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-09-30 00:39:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239140000 ms to writer queue
[INFO ] 2018-09-30 00:39:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239140000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000'
[INFO ] 2018-09-30 00:39:00,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239115000
[INFO ] 2018-09-30 00:39:00,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239140000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000', took 4494 bytes and 6 ms
[INFO ] 2018-09-30 00:39:00,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239140000 ms
[INFO ] 2018-09-30 00:39:00,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:39:00,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239135000: 
[INFO ] 2018-09-30 00:39:00,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239130000 ms
[INFO ] 2018-09-30 00:39:05,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:05,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239145000 ms.0 from job set of time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239145000 ms to writer queue
[INFO ] 2018-09-30 00:39:05,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239145000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000'
[INFO ] 2018-09-30 00:39:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:39:05,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:39:05,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:39:05,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:05,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:05,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:39:05,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:05,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:05,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:05,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:05,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:05,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-09-30 00:39:05,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:39:05,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-09-30 00:39:05,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:39:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 680 bytes result sent to driver
[INFO ] 2018-09-30 00:39:05,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000.bk
[INFO ] 2018-09-30 00:39:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239145000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000', took 4535 bytes and 20 ms
[INFO ] 2018-09-30 00:39:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (collect at StoreMovieEssay.scala:73) finished in 0.009 s
[INFO ] 2018-09-30 00:39:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: collect at StoreMovieEssay.scala:73, took 0.018623 s
[INFO ] 2018-09-30 00:39:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239145000 ms.0 from job set of time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.030 s for time 1538239145000 ms (execution: 0.024 s)
[INFO ] 2018-09-30 00:39:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17 from persistence list
[INFO ] 2018-09-30 00:39:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16 from persistence list
[INFO ] 2018-09-30 00:39:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17
[INFO ] 2018-09-30 00:39:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16
[INFO ] 2018-09-30 00:39:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15 from persistence list
[INFO ] 2018-09-30 00:39:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15
[INFO ] 2018-09-30 00:39:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239145000 ms to writer queue
[INFO ] 2018-09-30 00:39:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239145000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000'
[INFO ] 2018-09-30 00:39:05,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239120000
[INFO ] 2018-09-30 00:39:05,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239145000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000', took 4494 bytes and 19 ms
[INFO ] 2018-09-30 00:39:05,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239145000 ms
[INFO ] 2018-09-30 00:39:05,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:39:05,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239140000: 
[INFO ] 2018-09-30 00:39:05,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239135000 ms
[INFO ] 2018-09-30 00:39:10,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:10,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239150000 ms.0 from job set of time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239150000 ms to writer queue
[INFO ] 2018-09-30 00:39:10,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239150000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000'
[INFO ] 2018-09-30 00:39:10,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:39:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 7 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:39:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 7 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:39:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:10,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:39:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000.bk
[INFO ] 2018-09-30 00:39:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239150000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000', took 4533 bytes and 7 ms
[INFO ] 2018-09-30 00:39:10,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_7_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 7.0 with 1 tasks
[INFO ] 2018-09-30 00:39:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:39:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 7.0 (TID 7)
[INFO ] 2018-09-30 00:39:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:39:10,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7). 766 bytes result sent to driver
[INFO ] 2018-09-30 00:39:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 7 (collect at StoreMovieEssay.scala:73) finished in 0.006 s
[INFO ] 2018-09-30 00:39:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 7 finished: collect at StoreMovieEssay.scala:73, took 0.015973 s
[INFO ] 2018-09-30 00:39:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239150000 ms.0 from job set of time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20 from persistence list
[INFO ] 2018-09-30 00:39:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.029 s for time 1538239150000 ms (execution: 0.023 s)
[INFO ] 2018-09-30 00:39:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20
[INFO ] 2018-09-30 00:39:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19 from persistence list
[INFO ] 2018-09-30 00:39:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19
[INFO ] 2018-09-30 00:39:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18 from persistence list
[INFO ] 2018-09-30 00:39:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18
[INFO ] 2018-09-30 00:39:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239150000 ms to writer queue
[INFO ] 2018-09-30 00:39:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239150000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000'
[INFO ] 2018-09-30 00:39:10,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239125000
[INFO ] 2018-09-30 00:39:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239150000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000', took 4494 bytes and 12 ms
[INFO ] 2018-09-30 00:39:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239150000 ms
[INFO ] 2018-09-30 00:39:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:39:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239145000: 
[INFO ] 2018-09-30 00:39:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239140000 ms
[INFO ] 2018-09-30 00:39:14,853 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:39:14,854 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538239154854.
[WARN ] 2018-09-30 00:39:14,855 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:39:14,855 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:39:14,856 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:39:14,856 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:39:14,857 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:39:14,857 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:39:14,858 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:39:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239155000 ms.0 from job set of time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:15,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239155000 ms to writer queue
[INFO ] 2018-09-30 00:39:15,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239155000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000'
[INFO ] 2018-09-30 00:39:15,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:39:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 8 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:39:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 8 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:39:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 8 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:39:15,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_8_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:15,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_8_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:15,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:15,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:15,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 8.0 with 1 tasks
[INFO ] 2018-09-30 00:39:15,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:39:15,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 8.0 (TID 8)
[INFO ] 2018-09-30 00:39:15,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:39:15,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:39:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 8.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 8 (collect at StoreMovieEssay.scala:73) finished in 0.005 s
[INFO ] 2018-09-30 00:39:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 8 finished: collect at StoreMovieEssay.scala:73, took 0.017888 s
[INFO ] 2018-09-30 00:39:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239155000 ms.0 from job set of time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23 from persistence list
[INFO ] 2018-09-30 00:39:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.034 s for time 1538239155000 ms (execution: 0.026 s)
[INFO ] 2018-09-30 00:39:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 23
[INFO ] 2018-09-30 00:39:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22 from persistence list
[INFO ] 2018-09-30 00:39:15,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 22
[INFO ] 2018-09-30 00:39:15,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21 from persistence list
[INFO ] 2018-09-30 00:39:15,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 21
[INFO ] 2018-09-30 00:39:15,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239155000 ms to writer queue
[INFO ] 2018-09-30 00:39:15,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000.bk
[INFO ] 2018-09-30 00:39:15,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239155000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000', took 4533 bytes and 32 ms
[INFO ] 2018-09-30 00:39:15,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239155000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000'
[INFO ] 2018-09-30 00:39:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239130000
[INFO ] 2018-09-30 00:39:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239155000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000', took 4494 bytes and 7 ms
[INFO ] 2018-09-30 00:39:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239155000 ms
[INFO ] 2018-09-30 00:39:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:39:15,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239155049 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:39:15,053 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:39:15,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239145000 ms
[INFO ] 2018-09-30 00:39:20,000 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538239160000
[INFO ] 2018-09-30 00:39:20,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:20,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239160000 ms.0 from job set of time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:73
[INFO ] 2018-09-30 00:39:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 9 (collect at StoreMovieEssay.scala:73) with 1 output partitions
[INFO ] 2018-09-30 00:39:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 9 (collect at StoreMovieEssay.scala:73)
[INFO ] 2018-09-30 00:39:20,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 9 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:69), which has no missing parents
[INFO ] 2018-09-30 00:39:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 00:39:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_9_piece0 stored as bytes in memory (estimated size 2020.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:39:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239160000 ms to writer queue
[INFO ] 2018-09-30 00:39:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_9_piece0 in memory on 192.168.0.100:44313 (size: 2020.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:20,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239160000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000'
[INFO ] 2018-09-30 00:39:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at filter at StoreMovieEssay.scala:69) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 9.0 with 1 tasks
[INFO ] 2018-09-30 00:39:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4715 bytes)
[INFO ] 2018-09-30 00:39:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 9.0 (TID 9)
[INFO ] 2018-09-30 00:39:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 0 is the same as ending offset skipping movie-essay-topic 0
[INFO ] 2018-09-30 00:39:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:39:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 9.0 (TID 9) in 4 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 9 (collect at StoreMovieEssay.scala:73) finished in 0.004 s
[INFO ] 2018-09-30 00:39:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 9 finished: collect at StoreMovieEssay.scala:73, took 0.014115 s
[INFO ] 2018-09-30 00:39:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239160000 ms.0 from job set of time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.027 s for time 1538239160000 ms (execution: 0.020 s)
[INFO ] 2018-09-30 00:39:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26 from persistence list
[INFO ] 2018-09-30 00:39:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 26
[INFO ] 2018-09-30 00:39:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25 from persistence list
[INFO ] 2018-09-30 00:39:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 25
[INFO ] 2018-09-30 00:39:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24 from persistence list
[INFO ] 2018-09-30 00:39:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239160000 ms to writer queue
[INFO ] 2018-09-30 00:39:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 24
[INFO ] 2018-09-30 00:39:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000.bk
[INFO ] 2018-09-30 00:39:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239160000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000', took 4535 bytes and 21 ms
[INFO ] 2018-09-30 00:39:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239160000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000'
[INFO ] 2018-09-30 00:39:20,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239135000
[INFO ] 2018-09-30 00:39:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239160000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000', took 4494 bytes and 18 ms
[INFO ] 2018-09-30 00:39:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239160000 ms
[INFO ] 2018-09-30 00:39:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:39:20,058 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239160057 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:39:20,058 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:39:20,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239150000 ms
[INFO ] 2018-09-30 00:39:20,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:39:20,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-09-30 00:39:20,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 00:39:20,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-09-30 00:39:20,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-09-30 00:39:20,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 00:39:20,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:39:20,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 00:39:20,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 00:39:20,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 00:39:20,175 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 00:39:20,176 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 00:39:20,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 00:39:20,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 00:39:20,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-569a1018-ebf7-4e66-a7de-60483778e68b
[INFO ] 2018-09-30 00:39:51,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:39:51,458 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:39:51,639 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:39:51,639 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:39:51,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:39:51,726 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:39:51,727 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:39:51,728 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:39:51,728 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:39:51,729 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:39:51,985 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 44867.
[INFO ] 2018-09-30 00:39:52,004 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:39:52,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:39:52,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:39:52,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:39:52,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-57677ba5-d36c-4495-b068-954341264353
[INFO ] 2018-09-30 00:39:52,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:39:52,124 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:39:52,281 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:39:52,336 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:39:52,412 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:39:52,432 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44269.
[INFO ] 2018-09-30 00:39:52,433 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:44269
[INFO ] 2018-09-30 00:39:52,434 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:39:52,435 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 44269, None)
[INFO ] 2018-09-30 00:39:52,437 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:44269 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 44269, None)
[INFO ] 2018-09-30 00:39:52,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 44269, None)
[INFO ] 2018-09-30 00:39:52,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 44269, None)
[INFO ] 2018-09-30 00:39:52,628 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:39:52,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:39:53,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:39:53,218 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:39:53,422 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:39:53,422 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:39:53,423 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:39:53,423 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:39:53,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:39:53,594 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:39:53,605 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:39:53,606 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:39:53,606 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:39:53,607 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@ccf91df
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@6192a5d5
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:39:53,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@4b0f2299
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:39:53,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6d11ceef
[INFO ] 2018-09-30 00:39:53,666 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:39:53,776 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:39:53,804 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:39:53,804 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:39:53,995 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:39:53,998 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:39:54,001 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:39:54,030 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 5
[INFO ] 2018-09-30 00:39:54,031 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:39:54,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538239195000
[INFO ] 2018-09-30 00:39:54,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538239195000 ms
[INFO ] 2018-09-30 00:39:54,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:39:54,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:39:55,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239195000 ms.0 from job set of time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:39:55,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239195000 ms to writer queue
[INFO ] 2018-09-30 00:39:55,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239195000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000'
[INFO ] 2018-09-30 00:39:55,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:39:55,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000.bk
[INFO ] 2018-09-30 00:39:55,140 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239195000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000', took 4486 bytes and 46 ms
[INFO ] 2018-09-30 00:39:55,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:39:55,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:39:55,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:39:55,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:39:55,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:39:55,188 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:55,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:39:55,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:44269 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:39:55,205 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:39:55,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:39:55,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:39:55,254 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:39:55,260 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:39:55,313 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:39:55,324 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:39:55,350 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 101 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:39:55,356 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:74) finished in 0.120 s
[INFO ] 2018-09-30 00:39:55,356 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:39:55,373 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:74, took 0.239904 s
[INFO ] 2018-09-30 00:39:55,378 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239195000 ms.0 from job set of time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,380 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.378 s for time 1538239195000 ms (execution: 0.294 s)
[INFO ] 2018-09-30 00:39:55,384 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,384 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,384 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,386 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239195000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000'
[INFO ] 2018-09-30 00:39:55,387 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239195000 ms to writer queue
[INFO ] 2018-09-30 00:39:55,432 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239140000
[INFO ] 2018-09-30 00:39:55,432 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239195000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000', took 4483 bytes and 46 ms
[INFO ] 2018-09-30 00:39:55,434 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,436 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239195000 ms
[INFO ] 2018-09-30 00:39:55,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:39:55,461 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239190000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538239115933-1538239175933
[INFO ] 2018-09-30 00:39:55,463 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:39:55,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239190000
[INFO ] 2018-09-30 00:39:55,467 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:44269 in memory (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:00,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239200000 ms.0 from job set of time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239200000 ms to writer queue
[INFO ] 2018-09-30 00:40:00,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239200000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000'
[INFO ] 2018-09-30 00:40:00,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:00,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:44269 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:00,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 00:40:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:00,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 00:40:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000.bk
[INFO ] 2018-09-30 00:40:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239200000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000', took 4524 bytes and 18 ms
[INFO ] 2018-09-30 00:40:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:40:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:74) finished in 0.006 s
[INFO ] 2018-09-30 00:40:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:74, took 0.019435 s
[INFO ] 2018-09-30 00:40:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239200000 ms.0 from job set of time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.046 s for time 1538239200000 ms (execution: 0.027 s)
[INFO ] 2018-09-30 00:40:00,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 00:40:00,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 00:40:00,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 00:40:00,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 00:40:00,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 00:40:00,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 00:40:00,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239200000 ms to writer queue
[INFO ] 2018-09-30 00:40:00,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239200000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000'
[INFO ] 2018-09-30 00:40:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239145000
[INFO ] 2018-09-30 00:40:00,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239200000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000', took 4483 bytes and 25 ms
[INFO ] 2018-09-30 00:40:00,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239200000 ms
[INFO ] 2018-09-30 00:40:00,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:40:00,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239195000: 
[INFO ] 2018-09-30 00:40:00,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:40:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:05,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239205000 ms to writer queue
[INFO ] 2018-09-30 00:40:05,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239205000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000'
[INFO ] 2018-09-30 00:40:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239205000 ms.0 from job set of time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:05,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000.bk
[INFO ] 2018-09-30 00:40:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239205000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000', took 4524 bytes and 17 ms
[INFO ] 2018-09-30 00:40:05,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2029.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:44269 (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 00:40:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 00:40:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:05,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:40:05,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 8 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:05,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:05,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:74) finished in 0.009 s
[INFO ] 2018-09-30 00:40:05,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:74, took 0.018822 s
[INFO ] 2018-09-30 00:40:05,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239205000 ms.0 from job set of time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.044 s for time 1538239205000 ms (execution: 0.034 s)
[INFO ] 2018-09-30 00:40:05,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 00:40:05,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 00:40:05,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 00:40:05,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 00:40:05,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 00:40:05,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 00:40:05,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239205000 ms to writer queue
[INFO ] 2018-09-30 00:40:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239205000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000'
[INFO ] 2018-09-30 00:40:05,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239150000
[INFO ] 2018-09-30 00:40:05,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239205000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000', took 4483 bytes and 9 ms
[INFO ] 2018-09-30 00:40:05,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239205000 ms
[INFO ] 2018-09-30 00:40:05,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:40:05,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239200000: 
[INFO ] 2018-09-30 00:40:05,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239195000 ms
[INFO ] 2018-09-30 00:40:10,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239210000 ms.0 from job set of time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:10,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:10,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239210000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000'
[INFO ] 2018-09-30 00:40:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:10,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239210000 ms to writer queue
[INFO ] 2018-09-30 00:40:10,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:44269 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:10,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:10,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:10,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-09-30 00:40:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-09-30 00:40:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:40:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:10,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:10,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:74) finished in 0.013 s
[INFO ] 2018-09-30 00:40:10,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:74, took 0.026681 s
[INFO ] 2018-09-30 00:40:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239210000 ms.0 from job set of time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.048 s for time 1538239210000 ms (execution: 0.036 s)
[INFO ] 2018-09-30 00:40:10,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-09-30 00:40:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-09-30 00:40:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-09-30 00:40:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-09-30 00:40:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-09-30 00:40:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-09-30 00:40:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239210000 ms to writer queue
[INFO ] 2018-09-30 00:40:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000.bk
[INFO ] 2018-09-30 00:40:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239210000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000', took 4522 bytes and 25 ms
[INFO ] 2018-09-30 00:40:10,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239210000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000'
[INFO ] 2018-09-30 00:40:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239155000
[INFO ] 2018-09-30 00:40:10,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239210000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000', took 4483 bytes and 8 ms
[INFO ] 2018-09-30 00:40:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239210000 ms
[INFO ] 2018-09-30 00:40:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:40:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239205000: 
[INFO ] 2018-09-30 00:40:10,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239200000 ms
[INFO ] 2018-09-30 00:40:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:15,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239215000 ms.0 from job set of time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239215000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000'
[INFO ] 2018-09-30 00:40:15,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239215000 ms to writer queue
[INFO ] 2018-09-30 00:40:15,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:15,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:15,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:15,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:44269 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:15,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-09-30 00:40:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:15,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-09-30 00:40:15,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:15,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:40:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 11 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:15,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:74) finished in 0.020 s
[INFO ] 2018-09-30 00:40:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:74, took 0.030810 s
[INFO ] 2018-09-30 00:40:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239215000 ms.0 from job set of time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.044 s for time 1538239215000 ms (execution: 0.037 s)
[INFO ] 2018-09-30 00:40:15,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000.bk
[INFO ] 2018-09-30 00:40:15,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239215000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000', took 4522 bytes and 39 ms
[INFO ] 2018-09-30 00:40:15,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-09-30 00:40:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-09-30 00:40:15,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-09-30 00:40:15,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-09-30 00:40:15,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-09-30 00:40:15,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-09-30 00:40:15,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239215000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000'
[INFO ] 2018-09-30 00:40:15,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239215000 ms to writer queue
[INFO ] 2018-09-30 00:40:15,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239160000
[INFO ] 2018-09-30 00:40:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239215000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000', took 4483 bytes and 17 ms
[INFO ] 2018-09-30 00:40:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239215000 ms
[INFO ] 2018-09-30 00:40:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:40:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239210000: 
[INFO ] 2018-09-30 00:40:15,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239205000 ms
[INFO ] 2018-09-30 00:40:19,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:40:19,431 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538239219430.
[WARN ] 2018-09-30 00:40:19,431 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:40:19,432 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:40:19,437 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:40:19,437 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:40:19,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:40:19,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:40:19,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:40:20,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:20,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239220000 ms.0 from job set of time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239220000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000'
[INFO ] 2018-09-30 00:40:20,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239220000 ms to writer queue
[INFO ] 2018-09-30 00:40:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:20,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:20,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:44269 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-09-30 00:40:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000.bk
[INFO ] 2018-09-30 00:40:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-09-30 00:40:20,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239220000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000', took 4522 bytes and 18 ms
[INFO ] 2018-09-30 00:40:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 680 bytes result sent to driver
[INFO ] 2018-09-30 00:40:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (collect at StoreMovieEssay.scala:74) finished in 0.001 s
[INFO ] 2018-09-30 00:40:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: collect at StoreMovieEssay.scala:74, took 0.026175 s
[INFO ] 2018-09-30 00:40:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239220000 ms.0 from job set of time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.040 s for time 1538239220000 ms (execution: 0.032 s)
[INFO ] 2018-09-30 00:40:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-09-30 00:40:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-09-30 00:40:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239220000 ms to writer queue
[INFO ] 2018-09-30 00:40:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239220000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000'
[INFO ] 2018-09-30 00:40:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239195000
[INFO ] 2018-09-30 00:40:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239220000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000', took 4483 bytes and 6 ms
[INFO ] 2018-09-30 00:40:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239220000 ms
[INFO ] 2018-09-30 00:40:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:40:20,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239220050 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:40:20,053 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:40:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239210000 ms
[INFO ] 2018-09-30 00:40:25,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538239225000
[INFO ] 2018-09-30 00:40:25,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
------------------------------------------------------
[INFO ] 2018-09-30 00:40:25,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239225000 ms.0 from job set of time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:74
[INFO ] 2018-09-30 00:40:25,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (collect at StoreMovieEssay.scala:74) with 1 output partitions
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (collect at StoreMovieEssay.scala:74)
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239225000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000'
[INFO ] 2018-09-30 00:40:25,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:70), which has no missing parents
[INFO ] 2018-09-30 00:40:25,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239225000 ms to writer queue
[INFO ] 2018-09-30 00:40:25,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:40:25,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:25,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000.bk
[INFO ] 2018-09-30 00:40:25,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239225000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000', took 4525 bytes and 11 ms
[INFO ] 2018-09-30 00:40:25,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 2031.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:40:25,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:44269 (size: 2031.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:40:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:40:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:70) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:40:25,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-09-30 00:40:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:40:25,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-09-30 00:40:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:40:25,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:40:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:40:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:40:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (collect at StoreMovieEssay.scala:74) finished in 0.006 s
[INFO ] 2018-09-30 00:40:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: collect at StoreMovieEssay.scala:74, took 0.023441 s
[INFO ] 2018-09-30 00:40:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239225000 ms.0 from job set of time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17 from persistence list
[INFO ] 2018-09-30 00:40:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.046 s for time 1538239225000 ms (execution: 0.033 s)
[INFO ] 2018-09-30 00:40:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17
[INFO ] 2018-09-30 00:40:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16 from persistence list
[INFO ] 2018-09-30 00:40:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16
[INFO ] 2018-09-30 00:40:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15 from persistence list
[INFO ] 2018-09-30 00:40:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15
[INFO ] 2018-09-30 00:40:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239225000 ms to writer queue
[INFO ] 2018-09-30 00:40:25,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239225000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000'
[INFO ] 2018-09-30 00:40:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239200000
[INFO ] 2018-09-30 00:40:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239225000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000', took 4484 bytes and 7 ms
[INFO ] 2018-09-30 00:40:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239225000 ms
[INFO ] 2018-09-30 00:40:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:40:25,057 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239225057 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:40:25,057 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:40:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239215000 ms
[INFO ] 2018-09-30 00:40:25,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:40:25,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-09-30 00:40:25,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 00:40:25,124 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-09-30 00:40:25,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-09-30 00:40:25,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 00:40:25,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:40:25,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 00:40:25,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 00:40:25,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 00:40:25,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 00:40:25,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 00:40:25,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 00:40:25,172 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 00:40:25,172 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-945950d6-17d9-49fb-9c11-180231db3380
[INFO ] 2018-09-30 00:44:07,533 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:44:07,983 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:44:08,124 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:44:08,125 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:44:08,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:44:08,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:44:08,179 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:44:08,179 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:44:08,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:44:08,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:44:08,494 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 33415.
[INFO ] 2018-09-30 00:44:08,526 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:44:08,542 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:44:08,545 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:44:08,545 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:44:08,553 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-172859cf-295d-4c99-8469-69fc6c250583
[INFO ] 2018-09-30 00:44:08,614 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:44:08,681 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:44:08,875 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:44:08,913 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:44:08,989 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:44:09,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36461.
[INFO ] 2018-09-30 00:44:09,008 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:36461
[INFO ] 2018-09-30 00:44:09,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:44:09,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 36461, None)
[INFO ] 2018-09-30 00:44:09,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:36461 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 36461, None)
[INFO ] 2018-09-30 00:44:09,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 36461, None)
[INFO ] 2018-09-30 00:44:09,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 36461, None)
[INFO ] 2018-09-30 00:44:09,225 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:44:09,226 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:44:09,780 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:44:09,790 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:44:10,005 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:44:10,006 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:44:10,007 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:44:10,007 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:44:10,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:44:10,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:44:10,181 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:44:10,181 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:44:10,182 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6cf0a747
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:44:10,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4bc33720
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@2373ad99
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:44:10,184 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:44:10,185 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@37468787
[INFO ] 2018-09-30 00:44:10,227 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:44:10,320 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:44:10,346 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:44:10,346 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:44:10,532 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:44:10,533 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:44:10,535 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:44:10,556 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 7
[INFO ] 2018-09-30 00:44:10,558 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:44:10,573 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538239455000
[INFO ] 2018-09-30 00:44:10,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538239455000 ms
[INFO ] 2018-09-30 00:44:10,575 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:44:10,583 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:44:15,564 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,565 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,566 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239455000 ms.0 from job set of time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,571 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,590 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239455000 ms to writer queue
[INFO ] 2018-09-30 00:44:15,592 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239455000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000'
[INFO ] 2018-09-30 00:44:15,639 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:15,657 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:15,657 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:15,662 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:15,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:15,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:15,668 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000.bk
[INFO ] 2018-09-30 00:44:15,669 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239455000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000', took 4522 bytes and 77 ms
[INFO ] 2018-09-30 00:44:15,706 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:15,724 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2027.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:15,725 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:36461 (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:15,727 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:15,741 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:15,742 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:44:15,770 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:15,776 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:44:15,847 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:15,863 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:15,889 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 126 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:15,899 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:58) finished in 0.145 s
[INFO ] 2018-09-30 00:44:15,899 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:15,910 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:58, took 0.270971 s
[INFO ] 2018-09-30 00:44:15,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239455000 ms.0 from job set of time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.922 s for time 1538239455000 ms (execution: 0.352 s)
[INFO ] 2018-09-30 00:44:15,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,931 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239455000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000'
[INFO ] 2018-09-30 00:44:15,932 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239455000 ms to writer queue
[INFO ] 2018-09-30 00:44:15,954 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239205000
[INFO ] 2018-09-30 00:44:15,954 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239455000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000', took 4519 bytes and 23 ms
[INFO ] 2018-09-30 00:44:15,956 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,959 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239455000 ms
[INFO ] 2018-09-30 00:44:15,961 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:15,971 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239450000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538239195442-1538239255442
[INFO ] 2018-09-30 00:44:15,973 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:44:15,975 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239450000
[INFO ] 2018-09-30 00:44:20,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239460000 ms.0 from job set of time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:20,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239460000 ms to writer queue
[INFO ] 2018-09-30 00:44:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239460000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000'
[INFO ] 2018-09-30 00:44:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:20,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:36461 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:20,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 00:44:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:20,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 00:44:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000.bk
[INFO ] 2018-09-30 00:44:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239460000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000', took 4555 bytes and 19 ms
[INFO ] 2018-09-30 00:44:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:58) finished in 0.010 s
[INFO ] 2018-09-30 00:44:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:58, took 0.029225 s
[INFO ] 2018-09-30 00:44:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239460000 ms.0 from job set of time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.049 s for time 1538239460000 ms (execution: 0.036 s)
[INFO ] 2018-09-30 00:44:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 00:44:20,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 00:44:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 00:44:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 00:44:20,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 00:44:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239460000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000'
[INFO ] 2018-09-30 00:44:20,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239460000 ms to writer queue
[INFO ] 2018-09-30 00:44:20,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 00:44:20,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239210000
[INFO ] 2018-09-30 00:44:20,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239460000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000', took 4519 bytes and 32 ms
[INFO ] 2018-09-30 00:44:20,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239460000 ms
[INFO ] 2018-09-30 00:44:20,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:20,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239455000: 
[INFO ] 2018-09-30 00:44:20,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:44:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239465000 ms.0 from job set of time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239465000 ms to writer queue
[INFO ] 2018-09-30 00:44:25,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239465000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000'
[INFO ] 2018-09-30 00:44:25,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_1_piece0 on 192.168.0.100:36461 in memory (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:25,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:36461 in memory (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:25,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:25,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:25,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:25,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000.bk
[INFO ] 2018-09-30 00:44:25,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239465000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000', took 4557 bytes and 44 ms
[INFO ] 2018-09-30 00:44:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:25,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:36461 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:25,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:25,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:25,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 00:44:25,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:25,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 00:44:25,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:25,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:25,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:25,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:25,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:58) finished in 0.008 s
[INFO ] 2018-09-30 00:44:25,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:58, took 0.028817 s
[INFO ] 2018-09-30 00:44:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239465000 ms.0 from job set of time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 00:44:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.070 s for time 1538239465000 ms (execution: 0.059 s)
[INFO ] 2018-09-30 00:44:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 00:44:25,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239465000 ms to writer queue
[INFO ] 2018-09-30 00:44:25,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239465000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000'
[INFO ] 2018-09-30 00:44:25,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239215000
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239465000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000', took 4519 bytes and 9 ms
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239465000 ms
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239460000: 
[INFO ] 2018-09-30 00:44:25,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239455000 ms
[INFO ] 2018-09-30 00:44:30,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239470000 ms.0 from job set of time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239470000 ms to writer queue
[INFO ] 2018-09-30 00:44:30,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239470000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000'
[INFO ] 2018-09-30 00:44:30,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000.bk
[INFO ] 2018-09-30 00:44:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:30,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239470000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000', took 4557 bytes and 16 ms
[INFO ] 2018-09-30 00:44:30,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:30,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:30,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:36461 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:30,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:30,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-09-30 00:44:30,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:30,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-09-30 00:44:30,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:30,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:30,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:58) finished in 0.008 s
[INFO ] 2018-09-30 00:44:30,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:58, took 0.017303 s
[INFO ] 2018-09-30 00:44:30,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239470000 ms.0 from job set of time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.050 s for time 1538239470000 ms (execution: 0.033 s)
[INFO ] 2018-09-30 00:44:30,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-09-30 00:44:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-09-30 00:44:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-09-30 00:44:30,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-09-30 00:44:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-09-30 00:44:30,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-09-30 00:44:30,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239470000 ms to writer queue
[INFO ] 2018-09-30 00:44:30,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239470000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000'
[INFO ] 2018-09-30 00:44:30,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239220000
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239470000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000', took 4519 bytes and 16 ms
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239470000 ms
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239465000: 
[INFO ] 2018-09-30 00:44:30,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239460000 ms
[INFO ] 2018-09-30 00:44:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239475000 ms.0 from job set of time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239475000 ms to writer queue
[INFO ] 2018-09-30 00:44:35,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239475000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000'
[INFO ] 2018-09-30 00:44:35,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:35,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:35,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:35,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:35,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:36461 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:35,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-09-30 00:44:35,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:35,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-09-30 00:44:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000.bk
[INFO ] 2018-09-30 00:44:35,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239475000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000', took 4555 bytes and 22 ms
[INFO ] 2018-09-30 00:44:35,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:35,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:35,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:35,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:35,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:58) finished in 0.006 s
[INFO ] 2018-09-30 00:44:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:58, took 0.024707 s
[INFO ] 2018-09-30 00:44:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239475000 ms.0 from job set of time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.046 s for time 1538239475000 ms (execution: 0.030 s)
[INFO ] 2018-09-30 00:44:35,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-09-30 00:44:35,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-09-30 00:44:35,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-09-30 00:44:35,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-09-30 00:44:35,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-09-30 00:44:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-09-30 00:44:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239475000 ms to writer queue
[INFO ] 2018-09-30 00:44:35,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239475000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000'
[INFO ] 2018-09-30 00:44:35,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239225000
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239475000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000', took 4519 bytes and 9 ms
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239475000 ms
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239470000: 
[INFO ] 2018-09-30 00:44:35,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239465000 ms
[INFO ] 2018-09-30 00:44:40,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239480000 ms.0 from job set of time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:40,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239480000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000'
[INFO ] 2018-09-30 00:44:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:40,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:40,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:40,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239480000 ms to writer queue
[INFO ] 2018-09-30 00:44:40,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:40,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2027.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:40,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:36461 (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:40,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:40,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:40,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-09-30 00:44:40,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000.bk
[INFO ] 2018-09-30 00:44:40,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239480000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000', took 4555 bytes and 16 ms
[INFO ] 2018-09-30 00:44:40,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:40,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-09-30 00:44:40,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 5 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:40,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:40,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (collect at StoreMovieEssay.scala:58) finished in 0.006 s
[INFO ] 2018-09-30 00:44:40,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: collect at StoreMovieEssay.scala:58, took 0.022737 s
[INFO ] 2018-09-30 00:44:40,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239480000 ms.0 from job set of time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.047 s for time 1538239480000 ms (execution: 0.029 s)
[INFO ] 2018-09-30 00:44:40,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-09-30 00:44:40,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-09-30 00:44:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-09-30 00:44:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-09-30 00:44:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-09-30 00:44:40,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-09-30 00:44:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239480000 ms to writer queue
[INFO ] 2018-09-30 00:44:40,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239480000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000'
[INFO ] 2018-09-30 00:44:40,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239455000
[INFO ] 2018-09-30 00:44:40,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239480000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000', took 4519 bytes and 6 ms
[INFO ] 2018-09-30 00:44:40,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239480000 ms
[INFO ] 2018-09-30 00:44:40,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:44:40,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239475000: 
[INFO ] 2018-09-30 00:44:40,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239470000 ms
[INFO ] 2018-09-30 00:44:40,643 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:44:40,644 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538239480644.
[WARN ] 2018-09-30 00:44:40,645 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:44:40,645 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:44:40,647 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:44:40,647 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:44:40,648 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:44:40,648 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:44:40,649 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:44:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239485000 ms.0 from job set of time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239485000 ms to writer queue
[INFO ] 2018-09-30 00:44:45,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239485000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000'
[INFO ] 2018-09-30 00:44:45,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 6 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 6 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:45,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:45,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:45,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:45,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_6_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:45,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_6_piece0 in memory on 192.168.0.100:36461 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:45,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:45,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 6.0 with 1 tasks
[INFO ] 2018-09-30 00:44:45,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:45,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 6.0 (TID 6)
[INFO ] 2018-09-30 00:44:45,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000.bk
[INFO ] 2018-09-30 00:44:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239485000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000', took 4557 bytes and 16 ms
[INFO ] 2018-09-30 00:44:45,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:45,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:45,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 6.0 (TID 6) in 4 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:45,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:45,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 6 (collect at StoreMovieEssay.scala:58) finished in 0.001 s
[INFO ] 2018-09-30 00:44:45,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 6 finished: collect at StoreMovieEssay.scala:58, took 0.025830 s
[INFO ] 2018-09-30 00:44:45,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239485000 ms.0 from job set of time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17 from persistence list
[INFO ] 2018-09-30 00:44:45,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.067 s for time 1538239485000 ms (execution: 0.057 s)
[INFO ] 2018-09-30 00:44:45,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 17
[INFO ] 2018-09-30 00:44:45,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16 from persistence list
[INFO ] 2018-09-30 00:44:45,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 16
[INFO ] 2018-09-30 00:44:45,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15 from persistence list
[INFO ] 2018-09-30 00:44:45,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 15
[INFO ] 2018-09-30 00:44:45,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239485000 ms to writer queue
[INFO ] 2018-09-30 00:44:45,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239485000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000'
[INFO ] 2018-09-30 00:44:45,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239460000
[INFO ] 2018-09-30 00:44:45,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239485000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000', took 4519 bytes and 8 ms
[INFO ] 2018-09-30 00:44:45,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239485000 ms
[INFO ] 2018-09-30 00:44:45,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:44:45,081 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239485079 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:44:45,083 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:44:45,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239475000 ms
[INFO ] 2018-09-30 00:44:50,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538239490000
[INFO ] 2018-09-30 00:44:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239490000 ms.0 from job set of time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:44:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 7 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:44:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 7 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:44:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:44:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:44:50,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:44:50,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:50,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_7_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:44:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_7_piece0 in memory on 192.168.0.100:36461 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:44:50,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:44:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:44:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 00:44:50,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:44:50,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 7.0 with 1 tasks
[INFO ] 2018-09-30 00:44:50,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:44:50,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 7.0 (TID 7)
[INFO ] 2018-09-30 00:44:50,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000'
[INFO ] 2018-09-30 00:44:50,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239490000 ms to writer queue
[INFO ] 2018-09-30 00:44:50,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:44:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:44:50,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 7.0 (TID 7) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:44:50,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:44:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 7 (collect at StoreMovieEssay.scala:58) finished in 0.008 s
[INFO ] 2018-09-30 00:44:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 7 finished: collect at StoreMovieEssay.scala:58, took 0.016566 s
[INFO ] 2018-09-30 00:44:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000.bk
[INFO ] 2018-09-30 00:44:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239490000 ms.0 from job set of time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000', took 4555 bytes and 7 ms
[INFO ] 2018-09-30 00:44:50,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.033 s for time 1538239490000 ms (execution: 0.019 s)
[INFO ] 2018-09-30 00:44:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20 from persistence list
[INFO ] 2018-09-30 00:44:50,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 20
[INFO ] 2018-09-30 00:44:50,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19 from persistence list
[INFO ] 2018-09-30 00:44:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 19
[INFO ] 2018-09-30 00:44:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18 from persistence list
[INFO ] 2018-09-30 00:44:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 18
[INFO ] 2018-09-30 00:44:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239490000 ms to writer queue
[INFO ] 2018-09-30 00:44:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000'
[INFO ] 2018-09-30 00:44:50,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239465000
[INFO ] 2018-09-30 00:44:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000', took 4519 bytes and 7 ms
[INFO ] 2018-09-30 00:44:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239490000 ms
[INFO ] 2018-09-30 00:44:50,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:44:50,057 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239490057 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:44:50,057 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:44:50,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239480000 ms
[INFO ] 2018-09-30 00:44:50,125 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:44:50,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 1 ms.
[INFO ] 2018-09-30 00:44:50,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 00:44:50,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-09-30 00:44:50,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-09-30 00:44:50,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 00:44:50,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:44:50,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 00:44:50,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 00:44:50,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 00:44:50,175 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 00:44:50,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 00:44:50,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 00:44:50,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 00:44:50,179 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-ba6f88ab-1d4f-413b-b67e-5ea2fc12186e
[INFO ] 2018-09-30 00:47:41,837 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 00:47:42,250 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 00:47:42,395 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 00:47:42,395 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 00:47:42,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 00:47:42,443 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 00:47:42,443 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 00:47:42,444 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 00:47:42,445 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 00:47:42,445 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 00:47:42,675 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 40019.
[INFO ] 2018-09-30 00:47:42,697 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 00:47:42,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 00:47:42,714 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 00:47:42,714 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 00:47:42,727 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-32af8d32-d682-47db-bff4-15ee0a90280f
[INFO ] 2018-09-30 00:47:42,777 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 00:47:42,829 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 00:47:42,980 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 00:47:43,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:47:43,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 00:47:43,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46435.
[INFO ] 2018-09-30 00:47:43,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:46435
[INFO ] 2018-09-30 00:47:43,175 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 00:47:43,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 46435, None)
[INFO ] 2018-09-30 00:47:43,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:46435 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 46435, None)
[INFO ] 2018-09-30 00:47:43,187 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 46435, None)
[INFO ] 2018-09-30 00:47:43,188 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 46435, None)
[INFO ] 2018-09-30 00:47:43,447 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 00:47:43,448 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 00:47:44,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 00:47:44,026 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 00:47:44,227 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 00:47:44,228 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 00:47:44,228 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 00:47:44,229 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 00:47:44,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 00:47:44,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 00:47:44,486 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:47:44,487 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:47:44,487 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:47:44,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:47:44,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6e4f263e
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@70730db
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:47:44,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@2e869098
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 00:47:44,491 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5a3a1bf9
[INFO ] 2018-09-30 00:47:44,553 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:47:44,619 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 00:47:44,640 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:47:44,640 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:47:44,774 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:47:44,774 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:47:44,774 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:47:44,786 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 9
[INFO ] 2018-09-30 00:47:44,787 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 00:47:44,796 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538239665000
[INFO ] 2018-09-30 00:47:44,797 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538239665000 ms
[INFO ] 2018-09-30 00:47:44,797 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 00:47:44,802 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 00:47:45,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239665000 ms.0 from job set of time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,080 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239665000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000'
[INFO ] 2018-09-30 00:47:45,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239665000 ms to writer queue
[INFO ] 2018-09-30 00:47:45,110 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:47:45,134 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000.bk
[INFO ] 2018-09-30 00:47:45,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239665000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000', took 4529 bytes and 38 ms
[INFO ] 2018-09-30 00:47:45,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:47:45,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:47:45,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:47:45,138 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:47:45,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:47:45,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:45,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2027.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:45,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:46435 (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:47:45,193 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:47:45,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:47:45,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 00:47:45,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:47:45,241 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 00:47:45,290 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 56 -> 63
[INFO ] 2018-09-30 00:47:45,293 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-09-30 00:47:45,295 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-09-30 00:47:45,297 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-09-30 00:47:45,298 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-09-30 00:47:45,301 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 00:47:45,302 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 00:47:45,303 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 56
[INFO ] 2018-09-30 00:47:45,408 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-09-30 00:47:45,428 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291556556 
[INFO ] 2018-09-30 00:47:45,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291556556 
[INFO ] 2018-09-30 00:47:45,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291556556 
[INFO ] 2018-09-30 00:47:45,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291556556 ajkhdkfj
[INFO ] 2018-09-30 00:47:45,447 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 23609 bytes result sent to driver
[INFO ] 2018-09-30 00:47:45,455 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 229 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:47:45,457 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:47:45,460 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:58) finished in 0.242 s
[INFO ] 2018-09-30 00:47:45,465 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:58, took 0.354415 s
[INFO ] 2018-09-30 00:47:45,468 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291556556,1627814,从悖反到服从，于毁灭得拯救，再到彼岸。文/caesarphoenix有些片子需要沉下头潜进去才会发现大有可观，《新桥恋人》就是这样。该片声音和画面剪辑都很特别，错位剪辑运用很多，有的是“闪前”式，还有很多极短不稳定的快速剪辑，这些都营造了一种边缘的秩序外的，后城市化时代里的精神焦虑和生活困境。音乐声效的运用及画面色彩光线，乃至人物造型等等都体现了某种难以言喻的东西，也许是某种病症或者说是亚文化，也许是每个人内心隐处的情思。说实话有些镜头组接甚至惊吓到了我，大概是最近看的影片剪辑都比较传统，不适应吧。不过该片如果在电影院看，应该更为惊人吧。现有社会的生存法则（秩序规范）的存续（存在、维持、发展）是一种社会成本最优的需要，但它的存在无疑是对自由心灵的一种束缚，它指归了一种生活方式、树立了一个标杆，逾越它的尺寸将被视为异类。亚历是秩序外的人，米雪对秩序进行反叛的人。这两个人在一个时期融入了彼此的生活，行为非常相似。但这两人相似的行为的根源却是不同的。米雪主要是因为眼睛即将失明，兼有失恋打击，对生活产生绝望，在艺术家（画家）的神经质天性作用下，以流浪的方式没有目的没有方向的行进，也许冀望于某种体验和超脱，抑或只是在行走中麻木自己被绝望占据的心灵。米雪的这种对现有秩序的悖反是后天因素造成的。一般来说人们不愿意特立独行于公共秩序之外，因为和其他人一样会给我带来一种免于孤立的安全感，而这种安全感往往是幸福的基础。所以一旦米雪从广播中得知她的眼疾有可能治好（这一消息使得她有了重回公共秩序的希望），她便不惜在酒里下药，离开亚历，去进行治疗。亚历的独立于秩序之外则不同，他的绝望异常深刻，影片中并没有交代，有两种可能：一是在生命之初他就生活在秩序之外（没有接触，就没有认同），二是遭遇过极大的变故打击（比如汉斯的丧女丧妻之痛）。所以他会想尽一切办法撕毁烧毁寻找米雪的海报，因为在他的心目中一旦米雪康复就必然会离开他这个和现实格格不入的跛子。因为在秩序外，所以影片的前大半部分呈现出神经质的影像风格，我认为还带点超现实主义。比如米雪是否杀了前男友朱利安？拉提琴的朱利安确实被米雪追到（亚历看到米雪赶上了朱利安坐的地铁气愤得把拐杖都扔了）。我们也看到米雪尾随到朱利安住处后对着猫眼开了枪。但在新桥烟火庆祝时，亚历检查手枪说15发都在，其后亚历、米雪各开了七枪，在米雪走后，亚历还开了一枪打伤了自己的手，于是15发子弹确实都在。那杀朱利安的那颗子弹是从哪里来的呢？杀朱利安这个场景是幻想的还是真实的呢？同样寻找米雪的海报也是，一开始只有一幅，但转过头来其他的广告墙面也都被贴满了，每个电线杆、墙面都贴的满满的，一般来说就算再有钱，也不可能这样贴寻人启事，打开广播也立刻听到了相关信息，这样密集在现实生活中也几乎是不可能的。这大概是对亚历预感即将要失去米雪的精神焦虑的放大表现。米雪的眼疾康复了，判了三年刑的亚历在狱中也治好了腿，并学会了焊接技能。生理上的缺陷消失后，米雪表现出了对秩序的服从。然而在那些悖反的日子里，和亚历在一起时那种精神身体上的自由却是难以抹去的记忆。于是他们再相见了。从秩序外到秩序内，两个人的感情并没有消失。但在新桥上亚历发现米雪已经不是以前的米雪，将她扑下新桥，两人一起落入水中。也许是在冰冷的水里，更能够接近灵魂还是别的什么，总之于须臾死生之中得大智慧，顿悟跳脱，对生命有了更本质的认识，情感也进入一个新的层次。去大西洋或是别的什么地方，将腐烂的城市遗留在身后，就不再是一种单纯的反叛，而是一种证悟。秩序内抑或秩序外不再重要，两人在一起的喜悦幸福才是历久弥新永恒不变的。))
[INFO ] 2018-09-30 00:47:45,469 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291556556,1118154,影片开始的长镜头的黑暗,把我的心陷入沉默中.混乱不堪的社会底层流浪生活 似乎无处洁净 落迫肮脏市侩流血曾经写过气质相通的人定会在一起 命运保佑michelle的出现 象一束光突然来到  这个盲眼的脏脏的画手 红色的大衣突然让新桥上明亮起来电影中落泊的美学 让人心醉 这是如此纯洁的作品 法国影片有相当部分以情欲为主 而我更仰慕这样纯洁的影片和落泊的美学暗色的夜幕 鲜艳的衣物 或红或橙或蓝或黄 映照出彩虹般疯狂的爱情总是对这样的镜头印象深刻 michelle穿着黄色的衬衫在自己爱人julian的急促的大提琴声中奔跑风鼓满了皱的衫 当眼睛失去印象 总会有声音去记得第一次看新桥恋人的时候爱好于其纯洁的感情和疯狂的拥抱爱好于绚丽的烟火以及孩子般的天真而再次看的时候却开始关注导演所崇尚的美学 非主流的美学 阴郁的明亮当michelle跟踪julian到家用枪对准他企求般的说着她需要见他的时候 丧心病狂的爱情 暴力的美学当michelle和同在新桥露宿的alex烟花中和着巴黎的音乐颠狂的起舞的时候 我似看见在黑暗里的绚丽这是导演力求的美学 落泊的生活却拥有灿烂纯真的爱情 我们配拥有爱情么?我们这样的流浪的人难道有空间给爱情么?爱情需要房间,而我们只有新桥生活教会人们的总是奢望 却没有告诉我们拥有的平凡的已经足够我们只需要奔跑 需要疯狂 需要爱 其余是生活的背景而已有人爱上你了 如果你也爱他 那么说今天的天是白的 如果我就是他 那么我会回答但云是暗的这样的对白让我突然想微笑 温暖的孩子气 正是alex的性格魅力正是这种孩子气 可以和michelle奔去看海 对michelle说抱紧我 在黑暗中找不到她的时候喝酒打架导演心里的乌托帮正是充满孩子气的疯狂 绝望的明亮 甜蜜的爱恋.爱人怕失去 烧掉所有的寻人启示却不能挽留住自己的michelle 抱得再紧也没有明天亲爱的 你教会了我入睡 却没有教会我忘记 你的子弹带走了我的手指 带走了我的心当你再次出现的时候 我怎么也不会再放开 我们去往最美的天堂 以新桥的名义))
[INFO ] 2018-09-30 00:47:45,469 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291556556,5834885,1、《新桥恋人》把背景设置在了1989－91年间关闭大修的“新桥”——巴黎最古老的一座桥上；它于塞纳河最宽阔处横跨西岱岛，至今仍沿用着亨利四世1607年给它取的名字。故事的大部分场景都设置在桥的北段：一面闪烁着Samaritaine百货大楼不眠的霓虹，远处是卢浮宫巍峨的剪影；一面响荡着圣母院的钟声，凝重的司法建筑群无言矗立。——无疑，如果有什么地点天然为与国家力量对抗的个人提供栖息之所的话，那么就是新桥。它是一种在历史与现实之间漂浮的能指，奇妙地把商业、艺术、宗教和政治剪接在一起；而它身下塞纳河的流水，则是孤独心灵永不枯竭的力量源泉。2、导演显然有意在影片中留下拼贴的痕迹。从某种意义上说，“拼贴”，就是他的签名。Alex寻访Michèle过去的一段，他进入Marion的房间，看见四幅Michèle为她的初恋情人Julien画的肖像。镜头长长凝视着肖像，足以让观众看清楚这是位黑发、相貌敏感的男子。然而我们后来在地铁站见到的那位Julien却是满头金发。事实上，四幅肖像画的脸庞大约画的都是现实中的导演本人。　　此外，Alex从Marion桌上取走的红色封面手稿《米雪与朱利安，或豆蔻之恋》(Michèle et Julien, ou L’Amour de la Fille et du Gar&ccedil;on)显然暗示着导演1984年制作的影片Boy Meets Girl，而旁边还有一本蓝色封皮的书：《干涩的眼睛》（L’Oeil sec），则指向这部影片本身，因而富有“元创作”的色彩。更不用说导演具有自传意味的这三部曲（Boy Meets Girl，1984; Mauvais sang, 1986; Les Amants du Pont-Neuf, 1991）之间存在着的丰富互文：Hans、Alex、Marion、Julien都在前面的两部影片中出现过，而Hans回忆中他死去的妻子Florence，则既是第一部影片的女主角，又是导演少年时代的恋人，她的面容在三部影片中无所不在。　　文本、前文本与文本外的现实，都通过人物的记忆（被发掘或被阐述的记忆）铰接在一起。3、国家机器的修补力量通过镜头语言得到清晰的表达。影片开头，一辆小汽车碾伤了倒在地上的醉汉Alex的脚，后者晕阙过去——汽车象征着消费主义文化的破坏力量，暗示出它与政治的共谋——于是Alex被巡逻车带往收容所，与许多其它带着刺青、伤口，孱弱衰老肮脏的肉体一起被洗涮干净，分格存放，统一包装。第二天，镜头中出现一条膝盖以下打着石膏的腿，医生仔细地用纱布把它打磨光滑，缠上绷带。“修好”的Alex获许返回社会，返回同样正在“修补”之中的新桥。　　然而“修补”并不意味着慈悲。有些“无法修补”的肉体就被放任自流、成为社会边缘漂浮的碎片。Michèle，出身富室，但在因眼疾离家流浪的一年多的时间里，她的家人似乎并没有付出努力来寻找她。然而，当出现一种新的治疗手段可能治愈Michèle眼疾的时候，我们见识了国家机器无孔不入的力量：每一条隧道、每一根电线杆、每一寸墙壁，都贴满了Michèle未经破坏的美丽脸庞，在雨中如同梦魇。即便Alex将这些寻人启事全部付之一炬，Michèle依然在不经意中从电台广播里听见了这个消息。“回来吧！”整个社会都在召唤，“你是可以修补的！”　　修补术正是国家意识形态强大整合力量的体现。影片的高潮部分——庆祝法国革命二百周年的巴士底日那天，我们看见，九架战斗机排成锲形，在长空中拉出红白蓝三色烟幕。Michèle冲出地铁——鸟群惊飞，直升机、坦克、步兵、骑兵，在节奏感强烈的大提琴伴奏下各军种以整齐的阅兵队形前进。导演Leos Carax在1991年接受一次记者采访时承认，这里有一些影射Le Pen及其法西斯主义的意味；但并不仅仅如此，这一场景更是国家机器强大压迫力量的象征。 “个人意志”在这种压抑下窒息，正如Alex的脚被车碾伤晕阙之后，Michèle所画的他的脸：扭曲、变形，半边陷入模糊，仿佛被一种无形的力量所挤压，张着的嘴形成画面中央一个不规则的椭圆形黑洞。4、Michèle穿过军队、坦克的队列奔向新桥，奔向Alex；他们对抗这种外界强大恐怖力量的方式，就是沉入酒精忘却现实，或者用他们自己的方式狂欢。因而影片中长达二十分钟的巴士底日之夜也成了超现实主义拼贴的经典：导演使用了数十种完全不同风格的音乐，将拉丁舞曲、民歌、圆舞曲、小步舞曲、摇滚乐、说唱乐乃至交响乐，等等，不加任何过渡地剪辑在一起，而Michèle与Alex在满天灿烂焰火之中，随着不同的音乐改变舞步和节奏，用张狂的肢体语言尽情抖露他们苍白沉默的身体内部生命力的蓬勃。　　这个场景同时也标志出导演对类型化的彻底拒斥。影片开头似乎具有现实主义风格，甚至“造物现实主义”风格（如上面描述的收容所片断）；之后，Alex所发现的潜文本——Michèle与Julien的故事，则具有罗曼蒂克情节剧的所有因素（初恋、生死不渝、背叛、情杀，等等）。而巴士底日之夜的场景表面上是写实的——Michèle说：“城中弦歌不断”，在市中心的新桥上欣赏国庆之夜的焰火歌声仿佛都是很自然的事情——实际却是一种现实中不可能存在的状况（各种音乐的拼接，塞纳河上美丽的焰火，偷窃游艇的典型喜剧方式），展现出超现实的诗意。在这个场景之后合情合理的叙事逐渐变得不可能，情节越来越多地融合了喜剧与荒诞的色彩：Michèle与Alex用麻醉剂Alcyon窃取钱财，从而在海边沙滩上度过一段伊甸园的日子；老流浪汉Hans从口袋里掏出了几乎全巴黎——甚至包括卢浮宫——的钥匙（很有几分巴尔扎克的味道，不是吗？）；还有前面描述过的，Alex的雨中噩梦，到处贴满Michèle凝视的眼睛；结尾，两人双双从新桥跳入塞纳河后，神秘的运沙船；等等。在场景之间存在着明显的时间、空间以及情节合理性的裂痕。5、荒诞的拼贴同样体现在人物的身份上：Alex，吐火者，杂耍艺人，身手矫健的跛子；Michèle，上校的女儿，街头流浪艺术家，即将失明的画家；Hans，掌握了全巴黎的钥匙却无家可归的新桥国王。并且，如果说Michèle的流浪多少出于被动（由于失恋或失明，二者孰先孰后我们并不知晓），那么Alex与Hans的选择则是完全主动的。正如Hans所说：“这种生活，对于我，对于Alex，都是唯一的生活。”选择没有家庭、固定的工作和遮风蔽雨的住所的生活，所追求的只有一个，就是自由。　　自由，不仅是导演指导创作的主旨，也是影片所要表达的主旨。 导演说：“我只有一个念头，要讲述一种没有化妆、没有电话、没有卧室的爱情，一种纯粹、迅猛的爱的状态。……有的人，一无所有，处于爱的年龄，发现这仿佛是种吞噬身心的陌生病毒。”自由，就是不依赖于任何有形的物质条件和精神凭藉。从这个意义上说，Michèle接受眼疾手术离开Alex是有理由的，因为眼疾使她日益依赖Alex，而后者因为害怕失去她企图使她成为新桥的囚徒。只有摆脱了从属地位，才能有真正的爱。　　然而这种自由，并不同于他们身后西岱岛上，司法宫（Palais de Justice）门楣上刻的、作为国家理念的自由。自由，与其让它承担起宗教的使命，不如只把它作为深藏在个人心中的信仰。（别忘了法国大革命的经典名句：“公民不自由，就强迫他自由。”正如Gottfried Keller针对现代民主社会的预言：“自由的最终胜利将是贫瘠的。”）是以，富有讽刺意味的一幕发生在影片后半部分，Michèle在Hans陪同下进入卢浮宫看她“失明前还想看最后一眼”的画时，她径直走过德拉克洛瓦的名画《自由引导人民》，都懒得顺便看一眼——虽然这幅画论尺径远大于那幅伦勃朗自画像，因而比后者更加适合于她惨淡的视力。6、导演在影片中揭示的意识形态的载体，不仅仅是影像，还有音乐和色彩。除了影片开头巡逻车上穿制服者得意洋洋的咏叹调与老妇人哼的迷茫的儿歌——影片中间Michèle视力日益恶化，在早锻炼的时候她哼唱的也是这同一首儿歌——的对立之外，大提琴低沉辽阔的音色被选中成为“国家力量”的象征，而摇滚乐、低音吉他则是分别属于Michèle和Alex的音乐。红、白、蓝，法国的象征，这里也被指摘出了其中蕴涵的意识形态成分。飞行表演的红白蓝烟雾宣布巴士底日开始；而影片结尾，Michèle和Alex在桥上约会， Michèle穿着白色短大衣走下出租车（与开头相呼应），Alex穿着黑色外套走出地铁；午夜过后，桥上只剩下他们时，Alex只穿着红色毛衣在桥栏上翻跟头，里面露出蓝色套头衫领；而当Alex识破Michèle的谎言向她走去时，风吹开Michèle的衣襟，她里面穿的也是蓝色毛衣——红、白、蓝跌落水底。被运沙船救起后，他们脱下湿衣，换上干燥的衣物，虽然还是以白色为主，间有橙红蔚蓝，但已经不是明显的三色对比搭配了。　　导演所反对的并非自由、平等、博爱本身，而是反对将它们作为国家理想，从而剥夺了原本属于个人的价值判断权，蜕变为意识形态。这里，在色彩的变换中，同样体现了这一立场。7、意识形态无所不在。作为渺小的个人，为了对抗这种无孔不入的力量，唯一的手段，就是给自己留下无法愈合的伤疤。Alex在Michèle走后开枪打断了自己的左手无名指，通过这一举动，他拒绝了被修补——这一枪的时机颇为到位，我们看到，虽然拥有酒、麻醉剂、手枪，Michèle、Alex和Hans对社会的拒斥始终是和平的，没有陷入性、暴力和毒品的纠葛。只有Alex放火焚烧Michèle的寻人启事时，阴差阳错烧死了贴广告的工人（这一段导演的处理不可思议地平静），因此，第二天早上，也就是Michèle离开和Alex自残之后，巴黎警察以惊人的效率找到他，把他从梦中唤醒，逮捕、殴打、审判、监禁。　　在监狱里度过的三年使Alex的跛腿痊愈——狱医显然和收容所的医生一样能干负责——但残缺的手依然固执住爱的记忆。而Michèle虽然治好了自己的眼睛，并且嫁给了她的眼科医生，曾被Alex的火焰灼伤的心灵却保留住他清晰的身影。最终，就像萨特（Jean-Paul Sartre）的剧本《脏手》（Les Mains sales）的最后一句台词：“不可回收！”一样，三年后的新年午夜，他们相拥从修整一新的新桥上纵身跳入塞纳河的举动，仿佛大声宣告：“不可修补！”8、个人通过保护肉体与精神的“裂痕”与国家修补术对抗，某种意义上说是一种自残、自伤的行为。因而“拼贴图”的主角必然是反英雄的形象：他/她的对抗方式是非组织、非暴力的，与“国家力量”的辉煌灿烂、意志坚强相比，他们渺小、不美。只有在对自己立场的坚持里，体现出一种孤独的不妥协，一种真正的英雄气质。　　因此，一个不可回避的困境是，在意识形态的象征因素被充分意识并彻底摒弃后，乌托邦也相应成为一片废墟——意识形态与乌托邦，它们本是一对相伴而生、彼此对立的兄弟。抛弃一切“错误意识”，也同时意味着抛弃乌托邦意识。“此岸－彼岸”的二元对立随之消失，乌托邦被“此时此地”（here-and-now）的幸福哲学所取代。拼贴作品中“此时此地”的力量是令人惊异的。它反映了对暂时性事件的更加平庸、即刻、日常的感觉，艺术家因此编织了一个浮动拆析的世界。临近尾声时Michèle所讲的“幸福人的故事”——“因为就是今晚！”正是这种哲学的体现。然而，随着圣母院凌晨三点的钟声敲响，提醒她外部世界的存在，她仿佛又忘记了自己刚刚所说的故事：“Alex，我必须回去了。……你对我要有耐心。有一天我会告诉你的，但不是今晚。……不，不，今晚不行！”　　“伊甸园”在人间寻不到位置：Michèle与Alex度过赤身裸体、天真无邪的几个月生活的“海边”并非一个具体地点，也不可能是一个真实的地点；而结尾他们乘运沙船离开巴黎，Michèle问船主：“你们去哪里？”回答：“直到尽头。”“哪儿？勒阿弗尔吗？”勒阿弗尔，塞纳河的入海口。这里指涉的显然也并非真实的、同样处在法国政府管理之下的勒阿弗尔市，而是取其位于陆地与大海之间的意义。就像“新桥”一样，它同样是一个漂浮的、不确定的能指。9、或许正是凭借超现实主义“轻盈”（légèreté）的伪装，我们才难以察觉最后一幕的沉重：在现实中的个人是无法战胜国家力量，或摆脱承载其合法性的一整套意识形态的，永远不可能。唯一的出路就是死亡。在冰凉蔚蓝的河水中，经过一番挣扎，两人忽然互相凝视，上方传来螺旋桨有力的声音。这似乎暗示着，他们的生命已经结束了。然而影片并没有结束。两人分别浮出水面，被救上这艘神秘的运沙船，见到两位神秘的船主，Michèle问：“你们是卖沙子的吗？”“不，我们只负责运送。”“这是我们最后一趟航行了。”另一个说。——“沙”是时间的象征，《圣经&#8226;约伯记》：“……必增添我的日子，多如尘沙。” ——Michèle要求：“能带上我们吗？”“可以。”“当然可以。我们安排一下。”于是，他们钻出船舱在沙堆上奔跑，如同跑过平原和山脉，来到船头大声歌唱那首在影片中第三次出现、然而情绪已经大不一样的儿歌，并快乐地喊道：“巴黎，你留在我心里！”但前方是什么？如儿歌所唱的，他们并不知道，只是随着塞纳河的波浪前行，“……然而塞纳河如此平静/永无忧扰/不分日夜/她温柔的流波/向着勒阿弗尔/向着大海/如同一个梦幻/滑过巴黎的神秘/和悲惨。”（节译自Jacques Prévert, Chanson de la Seine）10、影片中所展示出的世纪末情怀、意识形态的死亡留下的真空，只有审美的形式能够填补。有的批评家虽然认为这部影片的叙事充满缺陷，但也承认其视觉效果足以弥补这一缺陷。事实上，“有缺陷的叙事”，正是拼贴的典型特征；而令人愉悦的视觉效果，却是作品本身得以超越日常、易逝的材料碎片确立其艺术价值的保证。正如影片的这最后一幕所暗示的，漂浮的“碎片”不再孤独，它在永恒的时间之流中获得了意义；伤痕依然存在，但影片本身作为一件“拼贴”的艺术作品，获得了永恒的价值。 (END)))
[INFO ] 2018-09-30 00:47:45,474 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239665000 ms.0 from job set of time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,496 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.473 s for time 1538239665000 ms (execution: 0.398 s)
[INFO ] 2018-09-30 00:47:45,502 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,508 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:46435 in memory (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:47:45,511 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,511 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,513 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239665000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000'
[INFO ] 2018-09-30 00:47:45,514 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239665000 ms to writer queue
[INFO ] 2018-09-30 00:47:45,541 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239470000
[INFO ] 2018-09-30 00:47:45,541 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239665000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000', took 4532 bytes and 28 ms
[INFO ] 2018-09-30 00:47:45,542 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,544 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239665000 ms
[INFO ] 2018-09-30 00:47:45,546 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:47:45,559 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239660000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538239455963-1538239515963
[INFO ] 2018-09-30 00:47:45,562 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:47:45,563 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239660000
[INFO ] 2018-09-30 00:47:50,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239670000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000'
[INFO ] 2018-09-30 00:47:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239670000 ms.0 from job set of time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239670000 ms to writer queue
[INFO ] 2018-09-30 00:47:50,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:47:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:47:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:47:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:47:50,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:47:50,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:47:50,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:50,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:50,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:46435 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:47:50,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:47:50,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:47:50,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 00:47:50,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:47:50,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 00:47:50,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:47:50,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000.bk
[INFO ] 2018-09-30 00:47:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239670000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000', took 4572 bytes and 30 ms
[INFO ] 2018-09-30 00:47:50,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:47:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:47:50,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:47:50,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:58) finished in 0.010 s
[INFO ] 2018-09-30 00:47:50,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:58, took 0.024012 s
[INFO ] 2018-09-30 00:47:50,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239670000 ms.0 from job set of time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.048 s for time 1538239670000 ms (execution: 0.032 s)
[INFO ] 2018-09-30 00:47:50,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 00:47:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 00:47:50,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 00:47:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 00:47:50,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 00:47:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 00:47:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239670000 ms to writer queue
[INFO ] 2018-09-30 00:47:50,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239670000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000'
[INFO ] 2018-09-30 00:47:50,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239475000
[INFO ] 2018-09-30 00:47:50,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239670000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000', took 4526 bytes and 22 ms
[INFO ] 2018-09-30 00:47:50,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239670000 ms
[INFO ] 2018-09-30 00:47:50,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:47:50,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239665000: 
[INFO ] 2018-09-30 00:47:50,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 00:47:55,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239675000 ms.0 from job set of time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239675000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000'
[INFO ] 2018-09-30 00:47:55,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239675000 ms to writer queue
[INFO ] 2018-09-30 00:47:55,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:47:55,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:47:55,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:47:55,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:47:55,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:47:55,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:47:55,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:55,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:47:55,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:46435 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:47:55,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:47:55,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:47:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 00:47:55,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:47:55,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 00:47:55,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000.bk
[INFO ] 2018-09-30 00:47:55,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239675000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000', took 4564 bytes and 29 ms
[INFO ] 2018-09-30 00:47:55,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:47:55,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:47:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:47:55,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:47:55,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:58) finished in 0.013 s
[INFO ] 2018-09-30 00:47:55,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:58, took 0.030198 s
[INFO ] 2018-09-30 00:47:55,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239675000 ms.0 from job set of time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.046 s for time 1538239675000 ms (execution: 0.036 s)
[INFO ] 2018-09-30 00:47:55,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 00:47:55,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 00:47:55,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 00:47:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 00:47:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 00:47:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 00:47:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239675000 ms to writer queue
[INFO ] 2018-09-30 00:47:55,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239675000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000'
[INFO ] 2018-09-30 00:47:55,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239480000
[INFO ] 2018-09-30 00:47:55,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239675000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000', took 4526 bytes and 14 ms
[INFO ] 2018-09-30 00:47:55,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239675000 ms
[INFO ] 2018-09-30 00:47:55,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:47:55,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239670000: 
[INFO ] 2018-09-30 00:47:55,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239665000 ms
[INFO ] 2018-09-30 00:48:00,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239680000 ms.0 from job set of time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239680000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000'
[INFO ] 2018-09-30 00:48:00,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239680000 ms to writer queue
[INFO ] 2018-09-30 00:48:00,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:48:00,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:48:00,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:48:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:48:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:48:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:48:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:00,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:46435 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:48:00,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:48:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:48:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-09-30 00:48:00,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:48:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-09-30 00:48:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:48:00,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000.bk
[INFO ] 2018-09-30 00:48:00,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239680000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000', took 4564 bytes and 28 ms
[INFO ] 2018-09-30 00:48:00,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:48:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:48:00,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:48:00,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (collect at StoreMovieEssay.scala:58) finished in 0.010 s
[INFO ] 2018-09-30 00:48:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: collect at StoreMovieEssay.scala:58, took 0.027048 s
[INFO ] 2018-09-30 00:48:00,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239680000 ms.0 from job set of time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.044 s for time 1538239680000 ms (execution: 0.032 s)
[INFO ] 2018-09-30 00:48:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-09-30 00:48:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-09-30 00:48:00,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239680000 ms to writer queue
[INFO ] 2018-09-30 00:48:00,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239680000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000'
[INFO ] 2018-09-30 00:48:00,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239485000
[INFO ] 2018-09-30 00:48:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239680000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000', took 4526 bytes and 9 ms
[INFO ] 2018-09-30 00:48:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239680000 ms
[INFO ] 2018-09-30 00:48:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 00:48:00,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538239675000: 
[INFO ] 2018-09-30 00:48:00,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239670000 ms
[INFO ] 2018-09-30 00:48:04,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 00:48:04,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538239684104.
[WARN ] 2018-09-30 00:48:04,104 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 00:48:04,105 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 00:48:04,106 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 00:48:04,106 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 00:48:04,107 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 00:48:04,107 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:48:04,108 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 00:48:05,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239685000 ms.0 from job set of time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:48:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:48:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:48:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:48:05,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:48:05,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239685000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000'
[INFO ] 2018-09-30 00:48:05,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:48:05,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239685000 ms to writer queue
[INFO ] 2018-09-30 00:48:05,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:05,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:05,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:46435 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:48:05,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:48:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:48:05,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-09-30 00:48:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:48:05,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-09-30 00:48:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000.bk
[INFO ] 2018-09-30 00:48:05,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239685000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000', took 4564 bytes and 14 ms
[INFO ] 2018-09-30 00:48:05,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:48:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:48:05,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:48:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:48:05,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (collect at StoreMovieEssay.scala:58) finished in 0.007 s
[INFO ] 2018-09-30 00:48:05,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: collect at StoreMovieEssay.scala:58, took 0.019543 s
[INFO ] 2018-09-30 00:48:05,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239685000 ms.0 from job set of time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.034 s for time 1538239685000 ms (execution: 0.024 s)
[INFO ] 2018-09-30 00:48:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11 from persistence list
[INFO ] 2018-09-30 00:48:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 11
[INFO ] 2018-09-30 00:48:05,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10 from persistence list
[INFO ] 2018-09-30 00:48:05,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 10
[INFO ] 2018-09-30 00:48:05,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9 from persistence list
[INFO ] 2018-09-30 00:48:05,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 9
[INFO ] 2018-09-30 00:48:05,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239685000 ms to writer queue
[INFO ] 2018-09-30 00:48:05,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239685000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000'
[INFO ] 2018-09-30 00:48:05,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239490000
[INFO ] 2018-09-30 00:48:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239685000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239685000', took 4526 bytes and 8 ms
[INFO ] 2018-09-30 00:48:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239685000 ms
[INFO ] 2018-09-30 00:48:05,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:48:05,048 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239685047 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:48:05,050 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:48:05,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239675000 ms
[INFO ] 2018-09-30 00:48:10,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538239690000
[INFO ] 2018-09-30 00:48:10,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538239690000 ms.0 from job set of time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:58
[INFO ] 2018-09-30 00:48:10,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 5 (collect at StoreMovieEssay.scala:58) with 1 output partitions
[INFO ] 2018-09-30 00:48:10,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 5 (collect at StoreMovieEssay.scala:58)
[INFO ] 2018-09-30 00:48:10,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 00:48:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 00:48:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:58), which has no missing parents
[INFO ] 2018-09-30 00:48:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 00:48:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:48:10,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_5_piece0 stored as bytes in memory (estimated size 2027.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 00:48:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239690000 ms to writer queue
[INFO ] 2018-09-30 00:48:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239690000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000'
[INFO ] 2018-09-30 00:48:10,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_5_piece0 in memory on 192.168.0.100:46435 (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 00:48:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 00:48:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at StoreMovieEssay.scala:58) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 00:48:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 5.0 with 1 tasks
[INFO ] 2018-09-30 00:48:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 00:48:10,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 5.0 (TID 5)
[INFO ] 2018-09-30 00:48:10,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 00:48:10,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5). 723 bytes result sent to driver
[INFO ] 2018-09-30 00:48:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 5.0 (TID 5) in 11 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 00:48:10,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 00:48:10,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 5 (collect at StoreMovieEssay.scala:58) finished in 0.011 s
[INFO ] 2018-09-30 00:48:10,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 5 finished: collect at StoreMovieEssay.scala:58, took 0.020786 s
[INFO ] 2018-09-30 00:48:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538239690000 ms.0 from job set of time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.035 s for time 1538239690000 ms (execution: 0.026 s)
[INFO ] 2018-09-30 00:48:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14 from persistence list
[INFO ] 2018-09-30 00:48:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 14
[INFO ] 2018-09-30 00:48:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13 from persistence list
[INFO ] 2018-09-30 00:48:10,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 13
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12 from persistence list
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 12
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000.bk
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239690000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000', took 4562 bytes and 19 ms
[INFO ] 2018-09-30 00:48:10,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538239690000 ms to writer queue
[INFO ] 2018-09-30 00:48:10,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538239690000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000'
[INFO ] 2018-09-30 00:48:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239665000
[INFO ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538239690000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239690000', took 4526 bytes and 9 ms
[INFO ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538239690000 ms
[INFO ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538239690051 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 00:48:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538239680000 ms
[INFO ] 2018-09-30 00:48:10,119 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 00:48:10,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-09-30 00:48:10,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 00:48:10,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-09-30 00:48:10,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-09-30 00:48:10,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 00:48:10,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 00:48:10,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 00:48:10,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 00:48:10,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 00:48:10,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 00:48:10,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 00:48:10,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 00:48:10,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 00:48:10,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-c94f5537-de52-498f-b182-bd3414851cb8
[INFO ] 2018-09-30 01:24:40,371 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 01:24:40,805 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 01:24:40,931 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 01:24:40,931 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 01:24:41,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 01:24:41,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 01:24:41,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 01:24:41,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 01:24:41,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 01:24:41,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 01:24:41,345 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 34533.
[INFO ] 2018-09-30 01:24:41,373 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 01:24:41,389 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 01:24:41,391 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 01:24:41,392 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 01:24:41,411 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-ea1837c1-e87e-4069-8ecc-4c81f39be016
[INFO ] 2018-09-30 01:24:41,479 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 01:24:41,566 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 01:24:41,750 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 01:24:41,860 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:24:41,935 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 01:24:41,957 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40837.
[INFO ] 2018-09-30 01:24:41,958 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:40837
[INFO ] 2018-09-30 01:24:41,959 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 01:24:41,960 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 40837, None)
[INFO ] 2018-09-30 01:24:41,963 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:40837 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 40837, None)
[INFO ] 2018-09-30 01:24:41,965 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 40837, None)
[INFO ] 2018-09-30 01:24:41,965 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 40837, None)
[INFO ] 2018-09-30 01:24:42,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 01:24:42,246 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 01:24:42,814 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-09-30 01:24:43,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:44
[INFO ] 2018-09-30 01:24:43,135 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:44) with 1 output partitions
[INFO ] 2018-09-30 01:24:43,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:44)
[INFO ] 2018-09-30 01:24:43,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:24:43,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:24:43,140 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:43), which has no missing parents
[INFO ] 2018-09-30 01:24:43,229 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1392.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:24:43,252 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 924.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:24:43,254 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:40837 (size: 924.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:24:43,260 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:24:43,277 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:43) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:24:43,278 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 01:24:43,314 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4776 bytes)
[INFO ] 2018-09-30 01:24:43,319 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 01:24:43,407 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 715 bytes result sent to driver
[INFO ] 2018-09-30 01:24:43,420 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:24:43,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:24:43,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:44) finished in 0.133 s
[INFO ] 2018-09-30 01:24:43,436 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:44, took 0.314578 s
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
1
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
2
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
3
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
4
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
5
[INFO ] 2018-09-30 01:24:43,438 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
6
[INFO ] 2018-09-30 01:24:43,440 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 01:24:43,453 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:24:43,461 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 01:24:43,469 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 01:24:43,469 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 01:24:43,480 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 01:24:43,482 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 01:24:43,493 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 01:24:43,493 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 01:24:43,494 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-b6e701f0-8e3b-4336-83ae-3d9acff64b71
[INFO ] 2018-09-30 01:25:42,589 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 01:25:43,043 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 01:25:43,171 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 01:25:43,172 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 01:25:43,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 01:25:43,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 01:25:43,214 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 01:25:43,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 01:25:43,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 01:25:43,216 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 01:25:43,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 36125.
[INFO ] 2018-09-30 01:25:43,498 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 01:25:43,512 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 01:25:43,515 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 01:25:43,515 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 01:25:43,521 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-b77beca4-9e86-4c3d-83fd-a3b3deb0134a
[INFO ] 2018-09-30 01:25:43,567 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 01:25:43,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 01:25:43,766 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 01:25:43,828 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:25:43,939 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 01:25:43,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38707.
[INFO ] 2018-09-30 01:25:43,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:38707
[INFO ] 2018-09-30 01:25:43,970 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 01:25:43,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 38707, None)
[INFO ] 2018-09-30 01:25:43,997 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:38707 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 38707, None)
[INFO ] 2018-09-30 01:25:44,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 38707, None)
[INFO ] 2018-09-30 01:25:44,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 38707, None)
[INFO ] 2018-09-30 01:25:44,293 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 01:25:44,293 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 01:25:44,865 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-09-30 01:25:45,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:44
[INFO ] 2018-09-30 01:25:45,177 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:44) with 1 output partitions
[INFO ] 2018-09-30 01:25:45,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:44)
[INFO ] 2018-09-30 01:25:45,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:25:45,179 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:25:45,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:43), which has no missing parents
[INFO ] 2018-09-30 01:25:45,258 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1616.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:25:45,280 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 1050.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:25:45,282 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:38707 (size: 1050.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:25:45,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:25:45,300 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:43) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:25:45,301 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 01:25:45,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4915 bytes)
[INFO ] 2018-09-30 01:25:45,342 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 01:25:45,405 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 863 bytes result sent to driver
[INFO ] 2018-09-30 01:25:45,411 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 88 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:25:45,413 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:25:45,417 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:44) finished in 0.103 s
[INFO ] 2018-09-30 01:25:45,424 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:44, took 0.260100 s
[INFO ] 2018-09-30 01:25:45,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
1
[INFO ] 2018-09-30 01:25:45,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
2
[INFO ] 2018-09-30 01:25:45,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
3
[INFO ] 2018-09-30 01:25:45,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
4
[INFO ] 2018-09-30 01:25:45,426 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
5
[INFO ] 2018-09-30 01:25:45,427 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
dddddddddd-------------------------
[INFO ] 2018-09-30 01:25:45,429 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 01:25:45,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:25:45,460 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 01:25:45,471 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 01:25:45,471 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 01:25:45,480 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 01:25:45,481 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 01:25:45,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 01:25:45,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 01:25:45,490 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-027aae7e-dd8a-43ea-8281-fba556f1d926
[INFO ] 2018-09-30 01:26:10,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 01:26:10,550 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 01:26:10,651 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 01:26:10,651 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 01:26:10,688 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 01:26:10,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 01:26:10,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 01:26:10,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 01:26:10,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 01:26:10,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 01:26:10,972 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 36731.
[INFO ] 2018-09-30 01:26:10,991 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 01:26:11,005 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 01:26:11,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 01:26:11,007 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 01:26:11,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-7b29759b-e45d-4c35-a31c-e49d9d5b29ef
[INFO ] 2018-09-30 01:26:11,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 01:26:11,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 01:26:11,303 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 01:26:11,352 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:26:11,452 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 01:26:11,473 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36003.
[INFO ] 2018-09-30 01:26:11,474 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:36003
[INFO ] 2018-09-30 01:26:11,475 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 01:26:11,476 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 36003, None)
[INFO ] 2018-09-30 01:26:11,479 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:36003 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 36003, None)
[INFO ] 2018-09-30 01:26:11,480 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 36003, None)
[INFO ] 2018-09-30 01:26:11,481 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 36003, None)
[INFO ] 2018-09-30 01:26:11,674 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 01:26:11,674 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 01:26:12,227 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-09-30 01:26:12,486 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:44
[INFO ] 2018-09-30 01:26:12,501 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:44) with 1 output partitions
[INFO ] 2018-09-30 01:26:12,501 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:44)
[INFO ] 2018-09-30 01:26:12,502 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:26:12,502 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:26:12,506 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:42), which has no missing parents
[INFO ] 2018-09-30 01:26:12,586 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1440.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:26:12,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 967.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:26:12,611 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:36003 (size: 967.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:26:12,613 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:26:12,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:42) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:26:12,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 01:26:12,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4953 bytes)
[INFO ] 2018-09-30 01:26:12,671 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 01:26:12,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 850 bytes result sent to driver
[INFO ] 2018-09-30 01:26:12,756 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 108 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:26:12,762 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:44) finished in 0.123 s
[INFO ] 2018-09-30 01:26:12,772 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:26:12,779 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:44, took 0.292488 s
[INFO ] 2018-09-30 01:26:12,785 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Review(1212,242,sdfsd)
[INFO ] 2018-09-30 01:26:12,785 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Review(54,d42,635456)
[INFO ] 2018-09-30 01:26:12,790 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 01:26:12,798 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:26:12,813 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 01:26:12,827 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 01:26:12,827 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 01:26:12,831 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 01:26:12,832 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 01:26:12,840 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 01:26:12,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 01:26:12,842 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-4dd83eb1-fd40-473c-a65b-ff1d5d8d22a8
[INFO ] 2018-09-30 01:28:07,277 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 01:28:07,687 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 01:28:07,802 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 01:28:07,803 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 01:28:07,830 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 01:28:07,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 01:28:07,844 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 01:28:07,844 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 01:28:07,845 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 01:28:07,845 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 01:28:08,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 46333.
[INFO ] 2018-09-30 01:28:08,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 01:28:08,178 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 01:28:08,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 01:28:08,181 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 01:28:08,193 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-cbac77f0-ac47-43f6-8089-f748d1cfabd8
[INFO ] 2018-09-30 01:28:08,240 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 01:28:08,290 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 01:28:08,461 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 01:28:08,544 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:28:08,648 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 01:28:08,673 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34441.
[INFO ] 2018-09-30 01:28:08,674 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:34441
[INFO ] 2018-09-30 01:28:08,675 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 01:28:08,677 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 34441, None)
[INFO ] 2018-09-30 01:28:08,679 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:34441 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 34441, None)
[INFO ] 2018-09-30 01:28:08,682 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 34441, None)
[INFO ] 2018-09-30 01:28:08,682 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 34441, None)
[INFO ] 2018-09-30 01:28:08,883 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 01:28:08,884 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 01:28:09,506 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[WARN ] 2018-09-30 01:28:09,515 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[WARN ] 2018-09-30 01:28:09,716 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-09-30 01:28:09,716 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-09-30 01:28:09,717 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-09-30 01:28:09,717 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-09-30 01:28:09,722 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-09-30 01:28:09,869 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-09-30 01:28:09,879 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 01:28:09,879 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 01:28:09,880 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 01:28:09,880 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6e4f263e
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@70730db
[INFO ] 2018-09-30 01:28:09,881 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@2e869098
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 5000 ms
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 5000 ms
[INFO ] 2018-09-30 01:28:09,882 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5a3a1bf9
[INFO ] 2018-09-30 01:28:09,932 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 01:28:10,005 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-09-30 01:28:10,030 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 01:28:10,030 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 01:28:10,166 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-09-30 01:28:10,167 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 01:28:10,168 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-09-30 01:28:10,181 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 11
[INFO ] 2018-09-30 01:28:10,182 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-09-30 01:28:10,197 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538242090000
[INFO ] 2018-09-30 01:28:10,197 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,198 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-09-30 01:28:10,207 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-09-30 01:28:10,278 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,281 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,282 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538242090000 ms.0 from job set of time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,287 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,298 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242090000 ms to writer queue
[INFO ] 2018-09-30 01:28:10,298 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242090000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000'
[INFO ] 2018-09-30 01:28:10,327 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-09-30 01:28:10,336 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000.bk
[INFO ] 2018-09-30 01:28:10,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242090000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000', took 4525 bytes and 39 ms
[INFO ] 2018-09-30 01:28:10,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-09-30 01:28:10,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-09-30 01:28:10,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:28:10,340 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:28:10,344 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-09-30 01:28:10,380 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:10,392 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2027.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:10,394 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:34441 (size: 2027.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:28:10,397 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:28:10,411 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:28:10,411 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 01:28:10,440 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 01:28:10,452 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 01:28:10,499 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 63 -> 77
[INFO ] 2018-09-30 01:28:10,501 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-09-30 01:28:10,502 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-09-30 01:28:10,504 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-09-30 01:28:10,506 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-09-30 01:28:10,507 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-09-30 01:28:10,508 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-09-30 01:28:10,510 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 63
[INFO ] 2018-09-30 01:28:10,614 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-09-30 01:28:10,634 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291558654 
[INFO ] 2018-09-30 01:28:10,634 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291558654 
[INFO ] 2018-09-30 01:28:10,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291558654 
[INFO ] 2018-09-30 01:28:10,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileT/media/feng/资源/bigdata/test/1291558654 ajkhdkfj
[INFO ] 2018-09-30 01:28:10,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileV/media/feng/资源/bigdata/test/12915586562 
[INFO ] 2018-09-30 01:28:10,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileV/media/feng/资源/bigdata/test/12915586562 
[INFO ] 2018-09-30 01:28:10,635 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileV/media/feng/资源/bigdata/test/12915586562 
[INFO ] 2018-09-30 01:28:10,636 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileV/media/feng/资源/bigdata/test/12915586562 ajkhdkfj
[INFO ] 2018-09-30 01:28:10,649 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 46246 bytes result sent to driver
[INFO ] 2018-09-30 01:28:10,656 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 223 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:28:10,658 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:28:10,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:64) finished in 0.234 s
[INFO ] 2018-09-30 01:28:10,682 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:64, took 0.354771 s
[INFO ] 2018-09-30 01:28:10,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291558654,1627814,从悖反到服从，于毁灭得拯救，再到彼岸。文/caesarphoenix有些片子需要沉下头潜进去才会发现大有可观，《新桥恋人》就是这样。该片声音和画面剪辑都很特别，错位剪辑运用很多，有的是“闪前”式，还有很多极短不稳定的快速剪辑，这些都营造了一种边缘的秩序外的，后城市化时代里的精神焦虑和生活困境。音乐声效的运用及画面色彩光线，乃至人物造型等等都体现了某种难以言喻的东西，也许是某种病症或者说是亚文化，也许是每个人内心隐处的情思。说实话有些镜头组接甚至惊吓到了我，大概是最近看的影片剪辑都比较传统，不适应吧。不过该片如果在电影院看，应该更为惊人吧。现有社会的生存法则（秩序规范）的存续（存在、维持、发展）是一种社会成本最优的需要，但它的存在无疑是对自由心灵的一种束缚，它指归了一种生活方式、树立了一个标杆，逾越它的尺寸将被视为异类。亚历是秩序外的人，米雪对秩序进行反叛的人。这两个人在一个时期融入了彼此的生活，行为非常相似。但这两人相似的行为的根源却是不同的。米雪主要是因为眼睛即将失明，兼有失恋打击，对生活产生绝望，在艺术家（画家）的神经质天性作用下，以流浪的方式没有目的没有方向的行进，也许冀望于某种体验和超脱，抑或只是在行走中麻木自己被绝望占据的心灵。米雪的这种对现有秩序的悖反是后天因素造成的。一般来说人们不愿意特立独行于公共秩序之外，因为和其他人一样会给我带来一种免于孤立的安全感，而这种安全感往往是幸福的基础。所以一旦米雪从广播中得知她的眼疾有可能治好（这一消息使得她有了重回公共秩序的希望），她便不惜在酒里下药，离开亚历，去进行治疗。亚历的独立于秩序之外则不同，他的绝望异常深刻，影片中并没有交代，有两种可能：一是在生命之初他就生活在秩序之外（没有接触，就没有认同），二是遭遇过极大的变故打击（比如汉斯的丧女丧妻之痛）。所以他会想尽一切办法撕毁烧毁寻找米雪的海报，因为在他的心目中一旦米雪康复就必然会离开他这个和现实格格不入的跛子。因为在秩序外，所以影片的前大半部分呈现出神经质的影像风格，我认为还带点超现实主义。比如米雪是否杀了前男友朱利安？拉提琴的朱利安确实被米雪追到（亚历看到米雪赶上了朱利安坐的地铁气愤得把拐杖都扔了）。我们也看到米雪尾随到朱利安住处后对着猫眼开了枪。但在新桥烟火庆祝时，亚历检查手枪说15发都在，其后亚历、米雪各开了七枪，在米雪走后，亚历还开了一枪打伤了自己的手，于是15发子弹确实都在。那杀朱利安的那颗子弹是从哪里来的呢？杀朱利安这个场景是幻想的还是真实的呢？同样寻找米雪的海报也是，一开始只有一幅，但转过头来其他的广告墙面也都被贴满了，每个电线杆、墙面都贴的满满的，一般来说就算再有钱，也不可能这样贴寻人启事，打开广播也立刻听到了相关信息，这样密集在现实生活中也几乎是不可能的。这大概是对亚历预感即将要失去米雪的精神焦虑的放大表现。米雪的眼疾康复了，判了三年刑的亚历在狱中也治好了腿，并学会了焊接技能。生理上的缺陷消失后，米雪表现出了对秩序的服从。然而在那些悖反的日子里，和亚历在一起时那种精神身体上的自由却是难以抹去的记忆。于是他们再相见了。从秩序外到秩序内，两个人的感情并没有消失。但在新桥上亚历发现米雪已经不是以前的米雪，将她扑下新桥，两人一起落入水中。也许是在冰冷的水里，更能够接近灵魂还是别的什么，总之于须臾死生之中得大智慧，顿悟跳脱，对生命有了更本质的认识，情感也进入一个新的层次。去大西洋或是别的什么地方，将腐烂的城市遗留在身后，就不再是一种单纯的反叛，而是一种证悟。秩序内抑或秩序外不再重要，两人在一起的喜悦幸福才是历久弥新永恒不变的。))
[INFO ] 2018-09-30 01:28:10,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291558654,1118154,影片开始的长镜头的黑暗,把我的心陷入沉默中.混乱不堪的社会底层流浪生活 似乎无处洁净 落迫肮脏市侩流血曾经写过气质相通的人定会在一起 命运保佑michelle的出现 象一束光突然来到  这个盲眼的脏脏的画手 红色的大衣突然让新桥上明亮起来电影中落泊的美学 让人心醉 这是如此纯洁的作品 法国影片有相当部分以情欲为主 而我更仰慕这样纯洁的影片和落泊的美学暗色的夜幕 鲜艳的衣物 或红或橙或蓝或黄 映照出彩虹般疯狂的爱情总是对这样的镜头印象深刻 michelle穿着黄色的衬衫在自己爱人julian的急促的大提琴声中奔跑风鼓满了皱的衫 当眼睛失去印象 总会有声音去记得第一次看新桥恋人的时候爱好于其纯洁的感情和疯狂的拥抱爱好于绚丽的烟火以及孩子般的天真而再次看的时候却开始关注导演所崇尚的美学 非主流的美学 阴郁的明亮当michelle跟踪julian到家用枪对准他企求般的说着她需要见他的时候 丧心病狂的爱情 暴力的美学当michelle和同在新桥露宿的alex烟花中和着巴黎的音乐颠狂的起舞的时候 我似看见在黑暗里的绚丽这是导演力求的美学 落泊的生活却拥有灿烂纯真的爱情 我们配拥有爱情么?我们这样的流浪的人难道有空间给爱情么?爱情需要房间,而我们只有新桥生活教会人们的总是奢望 却没有告诉我们拥有的平凡的已经足够我们只需要奔跑 需要疯狂 需要爱 其余是生活的背景而已有人爱上你了 如果你也爱他 那么说今天的天是白的 如果我就是他 那么我会回答但云是暗的这样的对白让我突然想微笑 温暖的孩子气 正是alex的性格魅力正是这种孩子气 可以和michelle奔去看海 对michelle说抱紧我 在黑暗中找不到她的时候喝酒打架导演心里的乌托帮正是充满孩子气的疯狂 绝望的明亮 甜蜜的爱恋.爱人怕失去 烧掉所有的寻人启示却不能挽留住自己的michelle 抱得再紧也没有明天亲爱的 你教会了我入睡 却没有教会我忘记 你的子弹带走了我的手指 带走了我的心当你再次出现的时候 我怎么也不会再放开 我们去往最美的天堂 以新桥的名义))
[INFO ] 2018-09-30 01:28:10,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1291558654,5834885,1、《新桥恋人》把背景设置在了1989－91年间关闭大修的“新桥”——巴黎最古老的一座桥上；它于塞纳河最宽阔处横跨西岱岛，至今仍沿用着亨利四世1607年给它取的名字。故事的大部分场景都设置在桥的北段：一面闪烁着Samaritaine百货大楼不眠的霓虹，远处是卢浮宫巍峨的剪影；一面响荡着圣母院的钟声，凝重的司法建筑群无言矗立。——无疑，如果有什么地点天然为与国家力量对抗的个人提供栖息之所的话，那么就是新桥。它是一种在历史与现实之间漂浮的能指，奇妙地把商业、艺术、宗教和政治剪接在一起；而它身下塞纳河的流水，则是孤独心灵永不枯竭的力量源泉。2、导演显然有意在影片中留下拼贴的痕迹。从某种意义上说，“拼贴”，就是他的签名。Alex寻访Michèle过去的一段，他进入Marion的房间，看见四幅Michèle为她的初恋情人Julien画的肖像。镜头长长凝视着肖像，足以让观众看清楚这是位黑发、相貌敏感的男子。然而我们后来在地铁站见到的那位Julien却是满头金发。事实上，四幅肖像画的脸庞大约画的都是现实中的导演本人。　　此外，Alex从Marion桌上取走的红色封面手稿《米雪与朱利安，或豆蔻之恋》(Michèle et Julien, ou L’Amour de la Fille et du Gar&ccedil;on)显然暗示着导演1984年制作的影片Boy Meets Girl，而旁边还有一本蓝色封皮的书：《干涩的眼睛》（L’Oeil sec），则指向这部影片本身，因而富有“元创作”的色彩。更不用说导演具有自传意味的这三部曲（Boy Meets Girl，1984; Mauvais sang, 1986; Les Amants du Pont-Neuf, 1991）之间存在着的丰富互文：Hans、Alex、Marion、Julien都在前面的两部影片中出现过，而Hans回忆中他死去的妻子Florence，则既是第一部影片的女主角，又是导演少年时代的恋人，她的面容在三部影片中无所不在。　　文本、前文本与文本外的现实，都通过人物的记忆（被发掘或被阐述的记忆）铰接在一起。3、国家机器的修补力量通过镜头语言得到清晰的表达。影片开头，一辆小汽车碾伤了倒在地上的醉汉Alex的脚，后者晕阙过去——汽车象征着消费主义文化的破坏力量，暗示出它与政治的共谋——于是Alex被巡逻车带往收容所，与许多其它带着刺青、伤口，孱弱衰老肮脏的肉体一起被洗涮干净，分格存放，统一包装。第二天，镜头中出现一条膝盖以下打着石膏的腿，医生仔细地用纱布把它打磨光滑，缠上绷带。“修好”的Alex获许返回社会，返回同样正在“修补”之中的新桥。　　然而“修补”并不意味着慈悲。有些“无法修补”的肉体就被放任自流、成为社会边缘漂浮的碎片。Michèle，出身富室，但在因眼疾离家流浪的一年多的时间里，她的家人似乎并没有付出努力来寻找她。然而，当出现一种新的治疗手段可能治愈Michèle眼疾的时候，我们见识了国家机器无孔不入的力量：每一条隧道、每一根电线杆、每一寸墙壁，都贴满了Michèle未经破坏的美丽脸庞，在雨中如同梦魇。即便Alex将这些寻人启事全部付之一炬，Michèle依然在不经意中从电台广播里听见了这个消息。“回来吧！”整个社会都在召唤，“你是可以修补的！”　　修补术正是国家意识形态强大整合力量的体现。影片的高潮部分——庆祝法国革命二百周年的巴士底日那天，我们看见，九架战斗机排成锲形，在长空中拉出红白蓝三色烟幕。Michèle冲出地铁——鸟群惊飞，直升机、坦克、步兵、骑兵，在节奏感强烈的大提琴伴奏下各军种以整齐的阅兵队形前进。导演Leos Carax在1991年接受一次记者采访时承认，这里有一些影射Le Pen及其法西斯主义的意味；但并不仅仅如此，这一场景更是国家机器强大压迫力量的象征。 “个人意志”在这种压抑下窒息，正如Alex的脚被车碾伤晕阙之后，Michèle所画的他的脸：扭曲、变形，半边陷入模糊，仿佛被一种无形的力量所挤压，张着的嘴形成画面中央一个不规则的椭圆形黑洞。4、Michèle穿过军队、坦克的队列奔向新桥，奔向Alex；他们对抗这种外界强大恐怖力量的方式，就是沉入酒精忘却现实，或者用他们自己的方式狂欢。因而影片中长达二十分钟的巴士底日之夜也成了超现实主义拼贴的经典：导演使用了数十种完全不同风格的音乐，将拉丁舞曲、民歌、圆舞曲、小步舞曲、摇滚乐、说唱乐乃至交响乐，等等，不加任何过渡地剪辑在一起，而Michèle与Alex在满天灿烂焰火之中，随着不同的音乐改变舞步和节奏，用张狂的肢体语言尽情抖露他们苍白沉默的身体内部生命力的蓬勃。　　这个场景同时也标志出导演对类型化的彻底拒斥。影片开头似乎具有现实主义风格，甚至“造物现实主义”风格（如上面描述的收容所片断）；之后，Alex所发现的潜文本——Michèle与Julien的故事，则具有罗曼蒂克情节剧的所有因素（初恋、生死不渝、背叛、情杀，等等）。而巴士底日之夜的场景表面上是写实的——Michèle说：“城中弦歌不断”，在市中心的新桥上欣赏国庆之夜的焰火歌声仿佛都是很自然的事情——实际却是一种现实中不可能存在的状况（各种音乐的拼接，塞纳河上美丽的焰火，偷窃游艇的典型喜剧方式），展现出超现实的诗意。在这个场景之后合情合理的叙事逐渐变得不可能，情节越来越多地融合了喜剧与荒诞的色彩：Michèle与Alex用麻醉剂Alcyon窃取钱财，从而在海边沙滩上度过一段伊甸园的日子；老流浪汉Hans从口袋里掏出了几乎全巴黎——甚至包括卢浮宫——的钥匙（很有几分巴尔扎克的味道，不是吗？）；还有前面描述过的，Alex的雨中噩梦，到处贴满Michèle凝视的眼睛；结尾，两人双双从新桥跳入塞纳河后，神秘的运沙船；等等。在场景之间存在着明显的时间、空间以及情节合理性的裂痕。5、荒诞的拼贴同样体现在人物的身份上：Alex，吐火者，杂耍艺人，身手矫健的跛子；Michèle，上校的女儿，街头流浪艺术家，即将失明的画家；Hans，掌握了全巴黎的钥匙却无家可归的新桥国王。并且，如果说Michèle的流浪多少出于被动（由于失恋或失明，二者孰先孰后我们并不知晓），那么Alex与Hans的选择则是完全主动的。正如Hans所说：“这种生活，对于我，对于Alex，都是唯一的生活。”选择没有家庭、固定的工作和遮风蔽雨的住所的生活，所追求的只有一个，就是自由。　　自由，不仅是导演指导创作的主旨，也是影片所要表达的主旨。 导演说：“我只有一个念头，要讲述一种没有化妆、没有电话、没有卧室的爱情，一种纯粹、迅猛的爱的状态。……有的人，一无所有，处于爱的年龄，发现这仿佛是种吞噬身心的陌生病毒。”自由，就是不依赖于任何有形的物质条件和精神凭藉。从这个意义上说，Michèle接受眼疾手术离开Alex是有理由的，因为眼疾使她日益依赖Alex，而后者因为害怕失去她企图使她成为新桥的囚徒。只有摆脱了从属地位，才能有真正的爱。　　然而这种自由，并不同于他们身后西岱岛上，司法宫（Palais de Justice）门楣上刻的、作为国家理念的自由。自由，与其让它承担起宗教的使命，不如只把它作为深藏在个人心中的信仰。（别忘了法国大革命的经典名句：“公民不自由，就强迫他自由。”正如Gottfried Keller针对现代民主社会的预言：“自由的最终胜利将是贫瘠的。”）是以，富有讽刺意味的一幕发生在影片后半部分，Michèle在Hans陪同下进入卢浮宫看她“失明前还想看最后一眼”的画时，她径直走过德拉克洛瓦的名画《自由引导人民》，都懒得顺便看一眼——虽然这幅画论尺径远大于那幅伦勃朗自画像，因而比后者更加适合于她惨淡的视力。6、导演在影片中揭示的意识形态的载体，不仅仅是影像，还有音乐和色彩。除了影片开头巡逻车上穿制服者得意洋洋的咏叹调与老妇人哼的迷茫的儿歌——影片中间Michèle视力日益恶化，在早锻炼的时候她哼唱的也是这同一首儿歌——的对立之外，大提琴低沉辽阔的音色被选中成为“国家力量”的象征，而摇滚乐、低音吉他则是分别属于Michèle和Alex的音乐。红、白、蓝，法国的象征，这里也被指摘出了其中蕴涵的意识形态成分。飞行表演的红白蓝烟雾宣布巴士底日开始；而影片结尾，Michèle和Alex在桥上约会， Michèle穿着白色短大衣走下出租车（与开头相呼应），Alex穿着黑色外套走出地铁；午夜过后，桥上只剩下他们时，Alex只穿着红色毛衣在桥栏上翻跟头，里面露出蓝色套头衫领；而当Alex识破Michèle的谎言向她走去时，风吹开Michèle的衣襟，她里面穿的也是蓝色毛衣——红、白、蓝跌落水底。被运沙船救起后，他们脱下湿衣，换上干燥的衣物，虽然还是以白色为主，间有橙红蔚蓝，但已经不是明显的三色对比搭配了。　　导演所反对的并非自由、平等、博爱本身，而是反对将它们作为国家理想，从而剥夺了原本属于个人的价值判断权，蜕变为意识形态。这里，在色彩的变换中，同样体现了这一立场。7、意识形态无所不在。作为渺小的个人，为了对抗这种无孔不入的力量，唯一的手段，就是给自己留下无法愈合的伤疤。Alex在Michèle走后开枪打断了自己的左手无名指，通过这一举动，他拒绝了被修补——这一枪的时机颇为到位，我们看到，虽然拥有酒、麻醉剂、手枪，Michèle、Alex和Hans对社会的拒斥始终是和平的，没有陷入性、暴力和毒品的纠葛。只有Alex放火焚烧Michèle的寻人启事时，阴差阳错烧死了贴广告的工人（这一段导演的处理不可思议地平静），因此，第二天早上，也就是Michèle离开和Alex自残之后，巴黎警察以惊人的效率找到他，把他从梦中唤醒，逮捕、殴打、审判、监禁。　　在监狱里度过的三年使Alex的跛腿痊愈——狱医显然和收容所的医生一样能干负责——但残缺的手依然固执住爱的记忆。而Michèle虽然治好了自己的眼睛，并且嫁给了她的眼科医生，曾被Alex的火焰灼伤的心灵却保留住他清晰的身影。最终，就像萨特（Jean-Paul Sartre）的剧本《脏手》（Les Mains sales）的最后一句台词：“不可回收！”一样，三年后的新年午夜，他们相拥从修整一新的新桥上纵身跳入塞纳河的举动，仿佛大声宣告：“不可修补！”8、个人通过保护肉体与精神的“裂痕”与国家修补术对抗，某种意义上说是一种自残、自伤的行为。因而“拼贴图”的主角必然是反英雄的形象：他/她的对抗方式是非组织、非暴力的，与“国家力量”的辉煌灿烂、意志坚强相比，他们渺小、不美。只有在对自己立场的坚持里，体现出一种孤独的不妥协，一种真正的英雄气质。　　因此，一个不可回避的困境是，在意识形态的象征因素被充分意识并彻底摒弃后，乌托邦也相应成为一片废墟——意识形态与乌托邦，它们本是一对相伴而生、彼此对立的兄弟。抛弃一切“错误意识”，也同时意味着抛弃乌托邦意识。“此岸－彼岸”的二元对立随之消失，乌托邦被“此时此地”（here-and-now）的幸福哲学所取代。拼贴作品中“此时此地”的力量是令人惊异的。它反映了对暂时性事件的更加平庸、即刻、日常的感觉，艺术家因此编织了一个浮动拆析的世界。临近尾声时Michèle所讲的“幸福人的故事”——“因为就是今晚！”正是这种哲学的体现。然而，随着圣母院凌晨三点的钟声敲响，提醒她外部世界的存在，她仿佛又忘记了自己刚刚所说的故事：“Alex，我必须回去了。……你对我要有耐心。有一天我会告诉你的，但不是今晚。……不，不，今晚不行！”　　“伊甸园”在人间寻不到位置：Michèle与Alex度过赤身裸体、天真无邪的几个月生活的“海边”并非一个具体地点，也不可能是一个真实的地点；而结尾他们乘运沙船离开巴黎，Michèle问船主：“你们去哪里？”回答：“直到尽头。”“哪儿？勒阿弗尔吗？”勒阿弗尔，塞纳河的入海口。这里指涉的显然也并非真实的、同样处在法国政府管理之下的勒阿弗尔市，而是取其位于陆地与大海之间的意义。就像“新桥”一样，它同样是一个漂浮的、不确定的能指。9、或许正是凭借超现实主义“轻盈”（légèreté）的伪装，我们才难以察觉最后一幕的沉重：在现实中的个人是无法战胜国家力量，或摆脱承载其合法性的一整套意识形态的，永远不可能。唯一的出路就是死亡。在冰凉蔚蓝的河水中，经过一番挣扎，两人忽然互相凝视，上方传来螺旋桨有力的声音。这似乎暗示着，他们的生命已经结束了。然而影片并没有结束。两人分别浮出水面，被救上这艘神秘的运沙船，见到两位神秘的船主，Michèle问：“你们是卖沙子的吗？”“不，我们只负责运送。”“这是我们最后一趟航行了。”另一个说。——“沙”是时间的象征，《圣经&#8226;约伯记》：“……必增添我的日子，多如尘沙。” ——Michèle要求：“能带上我们吗？”“可以。”“当然可以。我们安排一下。”于是，他们钻出船舱在沙堆上奔跑，如同跑过平原和山脉，来到船头大声歌唱那首在影片中第三次出现、然而情绪已经大不一样的儿歌，并快乐地喊道：“巴黎，你留在我心里！”但前方是什么？如儿歌所唱的，他们并不知道，只是随着塞纳河的波浪前行，“……然而塞纳河如此平静/永无忧扰/不分日夜/她温柔的流波/向着勒阿弗尔/向着大海/如同一个梦幻/滑过巴黎的神秘/和悲惨。”（节译自Jacques Prévert, Chanson de la Seine）10、影片中所展示出的世纪末情怀、意识形态的死亡留下的真空，只有审美的形式能够填补。有的批评家虽然认为这部影片的叙事充满缺陷，但也承认其视觉效果足以弥补这一缺陷。事实上，“有缺陷的叙事”，正是拼贴的典型特征；而令人愉悦的视觉效果，却是作品本身得以超越日常、易逝的材料碎片确立其艺术价值的保证。正如影片的这最后一幕所暗示的，漂浮的“碎片”不再孤独，它在永恒的时间之流中获得了意义；伤痕依然存在，但影片本身作为一件“拼贴”的艺术作品，获得了永恒的价值。 (END)))
[INFO ] 2018-09-30 01:28:10,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915586562,1627814,从悖反到服从，于毁灭得拯救，再到彼岸。文/caesarphoenix有些片子需要沉下头潜进去才会发现大有可观，《新桥恋人》就是这样。该片声音和画面剪辑都很特别，错位剪辑运用很多，有的是“闪前”式，还有很多极短不稳定的快速剪辑，这些都营造了一种边缘的秩序外的，后城市化时代里的精神焦虑和生活困境。音乐声效的运用及画面色彩光线，乃至人物造型等等都体现了某种难以言喻的东西，也许是某种病症或者说是亚文化，也许是每个人内心隐处的情思。说实话有些镜头组接甚至惊吓到了我，大概是最近看的影片剪辑都比较传统，不适应吧。不过该片如果在电影院看，应该更为惊人吧。现有社会的生存法则（秩序规范）的存续（存在、维持、发展）是一种社会成本最优的需要，但它的存在无疑是对自由心灵的一种束缚，它指归了一种生活方式、树立了一个标杆，逾越它的尺寸将被视为异类。亚历是秩序外的人，米雪对秩序进行反叛的人。这两个人在一个时期融入了彼此的生活，行为非常相似。但这两人相似的行为的根源却是不同的。米雪主要是因为眼睛即将失明，兼有失恋打击，对生活产生绝望，在艺术家（画家）的神经质天性作用下，以流浪的方式没有目的没有方向的行进，也许冀望于某种体验和超脱，抑或只是在行走中麻木自己被绝望占据的心灵。米雪的这种对现有秩序的悖反是后天因素造成的。一般来说人们不愿意特立独行于公共秩序之外，因为和其他人一样会给我带来一种免于孤立的安全感，而这种安全感往往是幸福的基础。所以一旦米雪从广播中得知她的眼疾有可能治好（这一消息使得她有了重回公共秩序的希望），她便不惜在酒里下药，离开亚历，去进行治疗。亚历的独立于秩序之外则不同，他的绝望异常深刻，影片中并没有交代，有两种可能：一是在生命之初他就生活在秩序之外（没有接触，就没有认同），二是遭遇过极大的变故打击（比如汉斯的丧女丧妻之痛）。所以他会想尽一切办法撕毁烧毁寻找米雪的海报，因为在他的心目中一旦米雪康复就必然会离开他这个和现实格格不入的跛子。因为在秩序外，所以影片的前大半部分呈现出神经质的影像风格，我认为还带点超现实主义。比如米雪是否杀了前男友朱利安？拉提琴的朱利安确实被米雪追到（亚历看到米雪赶上了朱利安坐的地铁气愤得把拐杖都扔了）。我们也看到米雪尾随到朱利安住处后对着猫眼开了枪。但在新桥烟火庆祝时，亚历检查手枪说15发都在，其后亚历、米雪各开了七枪，在米雪走后，亚历还开了一枪打伤了自己的手，于是15发子弹确实都在。那杀朱利安的那颗子弹是从哪里来的呢？杀朱利安这个场景是幻想的还是真实的呢？同样寻找米雪的海报也是，一开始只有一幅，但转过头来其他的广告墙面也都被贴满了，每个电线杆、墙面都贴的满满的，一般来说就算再有钱，也不可能这样贴寻人启事，打开广播也立刻听到了相关信息，这样密集在现实生活中也几乎是不可能的。这大概是对亚历预感即将要失去米雪的精神焦虑的放大表现。米雪的眼疾康复了，判了三年刑的亚历在狱中也治好了腿，并学会了焊接技能。生理上的缺陷消失后，米雪表现出了对秩序的服从。然而在那些悖反的日子里，和亚历在一起时那种精神身体上的自由却是难以抹去的记忆。于是他们再相见了。从秩序外到秩序内，两个人的感情并没有消失。但在新桥上亚历发现米雪已经不是以前的米雪，将她扑下新桥，两人一起落入水中。也许是在冰冷的水里，更能够接近灵魂还是别的什么，总之于须臾死生之中得大智慧，顿悟跳脱，对生命有了更本质的认识，情感也进入一个新的层次。去大西洋或是别的什么地方，将腐烂的城市遗留在身后，就不再是一种单纯的反叛，而是一种证悟。秩序内抑或秩序外不再重要，两人在一起的喜悦幸福才是历久弥新永恒不变的。))
[INFO ] 2018-09-30 01:28:10,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915586562,1118154,影片开始的长镜头的黑暗,把我的心陷入沉默中.混乱不堪的社会底层流浪生活 似乎无处洁净 落迫肮脏市侩流血曾经写过气质相通的人定会在一起 命运保佑michelle的出现 象一束光突然来到  这个盲眼的脏脏的画手 红色的大衣突然让新桥上明亮起来电影中落泊的美学 让人心醉 这是如此纯洁的作品 法国影片有相当部分以情欲为主 而我更仰慕这样纯洁的影片和落泊的美学暗色的夜幕 鲜艳的衣物 或红或橙或蓝或黄 映照出彩虹般疯狂的爱情总是对这样的镜头印象深刻 michelle穿着黄色的衬衫在自己爱人julian的急促的大提琴声中奔跑风鼓满了皱的衫 当眼睛失去印象 总会有声音去记得第一次看新桥恋人的时候爱好于其纯洁的感情和疯狂的拥抱爱好于绚丽的烟火以及孩子般的天真而再次看的时候却开始关注导演所崇尚的美学 非主流的美学 阴郁的明亮当michelle跟踪julian到家用枪对准他企求般的说着她需要见他的时候 丧心病狂的爱情 暴力的美学当michelle和同在新桥露宿的alex烟花中和着巴黎的音乐颠狂的起舞的时候 我似看见在黑暗里的绚丽这是导演力求的美学 落泊的生活却拥有灿烂纯真的爱情 我们配拥有爱情么?我们这样的流浪的人难道有空间给爱情么?爱情需要房间,而我们只有新桥生活教会人们的总是奢望 却没有告诉我们拥有的平凡的已经足够我们只需要奔跑 需要疯狂 需要爱 其余是生活的背景而已有人爱上你了 如果你也爱他 那么说今天的天是白的 如果我就是他 那么我会回答但云是暗的这样的对白让我突然想微笑 温暖的孩子气 正是alex的性格魅力正是这种孩子气 可以和michelle奔去看海 对michelle说抱紧我 在黑暗中找不到她的时候喝酒打架导演心里的乌托帮正是充满孩子气的疯狂 绝望的明亮 甜蜜的爱恋.爱人怕失去 烧掉所有的寻人启示却不能挽留住自己的michelle 抱得再紧也没有明天亲爱的 你教会了我入睡 却没有教会我忘记 你的子弹带走了我的手指 带走了我的心当你再次出现的时候 我怎么也不会再放开 我们去往最美的天堂 以新桥的名义))
[INFO ] 2018-09-30 01:28:10,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(12915586562,5834885,1、《新桥恋人》把背景设置在了1989－91年间关闭大修的“新桥”——巴黎最古老的一座桥上；它于塞纳河最宽阔处横跨西岱岛，至今仍沿用着亨利四世1607年给它取的名字。故事的大部分场景都设置在桥的北段：一面闪烁着Samaritaine百货大楼不眠的霓虹，远处是卢浮宫巍峨的剪影；一面响荡着圣母院的钟声，凝重的司法建筑群无言矗立。——无疑，如果有什么地点天然为与国家力量对抗的个人提供栖息之所的话，那么就是新桥。它是一种在历史与现实之间漂浮的能指，奇妙地把商业、艺术、宗教和政治剪接在一起；而它身下塞纳河的流水，则是孤独心灵永不枯竭的力量源泉。2、导演显然有意在影片中留下拼贴的痕迹。从某种意义上说，“拼贴”，就是他的签名。Alex寻访Michèle过去的一段，他进入Marion的房间，看见四幅Michèle为她的初恋情人Julien画的肖像。镜头长长凝视着肖像，足以让观众看清楚这是位黑发、相貌敏感的男子。然而我们后来在地铁站见到的那位Julien却是满头金发。事实上，四幅肖像画的脸庞大约画的都是现实中的导演本人。　　此外，Alex从Marion桌上取走的红色封面手稿《米雪与朱利安，或豆蔻之恋》(Michèle et Julien, ou L’Amour de la Fille et du Gar&ccedil;on)显然暗示着导演1984年制作的影片Boy Meets Girl，而旁边还有一本蓝色封皮的书：《干涩的眼睛》（L’Oeil sec），则指向这部影片本身，因而富有“元创作”的色彩。更不用说导演具有自传意味的这三部曲（Boy Meets Girl，1984; Mauvais sang, 1986; Les Amants du Pont-Neuf, 1991）之间存在着的丰富互文：Hans、Alex、Marion、Julien都在前面的两部影片中出现过，而Hans回忆中他死去的妻子Florence，则既是第一部影片的女主角，又是导演少年时代的恋人，她的面容在三部影片中无所不在。　　文本、前文本与文本外的现实，都通过人物的记忆（被发掘或被阐述的记忆）铰接在一起。3、国家机器的修补力量通过镜头语言得到清晰的表达。影片开头，一辆小汽车碾伤了倒在地上的醉汉Alex的脚，后者晕阙过去——汽车象征着消费主义文化的破坏力量，暗示出它与政治的共谋——于是Alex被巡逻车带往收容所，与许多其它带着刺青、伤口，孱弱衰老肮脏的肉体一起被洗涮干净，分格存放，统一包装。第二天，镜头中出现一条膝盖以下打着石膏的腿，医生仔细地用纱布把它打磨光滑，缠上绷带。“修好”的Alex获许返回社会，返回同样正在“修补”之中的新桥。　　然而“修补”并不意味着慈悲。有些“无法修补”的肉体就被放任自流、成为社会边缘漂浮的碎片。Michèle，出身富室，但在因眼疾离家流浪的一年多的时间里，她的家人似乎并没有付出努力来寻找她。然而，当出现一种新的治疗手段可能治愈Michèle眼疾的时候，我们见识了国家机器无孔不入的力量：每一条隧道、每一根电线杆、每一寸墙壁，都贴满了Michèle未经破坏的美丽脸庞，在雨中如同梦魇。即便Alex将这些寻人启事全部付之一炬，Michèle依然在不经意中从电台广播里听见了这个消息。“回来吧！”整个社会都在召唤，“你是可以修补的！”　　修补术正是国家意识形态强大整合力量的体现。影片的高潮部分——庆祝法国革命二百周年的巴士底日那天，我们看见，九架战斗机排成锲形，在长空中拉出红白蓝三色烟幕。Michèle冲出地铁——鸟群惊飞，直升机、坦克、步兵、骑兵，在节奏感强烈的大提琴伴奏下各军种以整齐的阅兵队形前进。导演Leos Carax在1991年接受一次记者采访时承认，这里有一些影射Le Pen及其法西斯主义的意味；但并不仅仅如此，这一场景更是国家机器强大压迫力量的象征。 “个人意志”在这种压抑下窒息，正如Alex的脚被车碾伤晕阙之后，Michèle所画的他的脸：扭曲、变形，半边陷入模糊，仿佛被一种无形的力量所挤压，张着的嘴形成画面中央一个不规则的椭圆形黑洞。4、Michèle穿过军队、坦克的队列奔向新桥，奔向Alex；他们对抗这种外界强大恐怖力量的方式，就是沉入酒精忘却现实，或者用他们自己的方式狂欢。因而影片中长达二十分钟的巴士底日之夜也成了超现实主义拼贴的经典：导演使用了数十种完全不同风格的音乐，将拉丁舞曲、民歌、圆舞曲、小步舞曲、摇滚乐、说唱乐乃至交响乐，等等，不加任何过渡地剪辑在一起，而Michèle与Alex在满天灿烂焰火之中，随着不同的音乐改变舞步和节奏，用张狂的肢体语言尽情抖露他们苍白沉默的身体内部生命力的蓬勃。　　这个场景同时也标志出导演对类型化的彻底拒斥。影片开头似乎具有现实主义风格，甚至“造物现实主义”风格（如上面描述的收容所片断）；之后，Alex所发现的潜文本——Michèle与Julien的故事，则具有罗曼蒂克情节剧的所有因素（初恋、生死不渝、背叛、情杀，等等）。而巴士底日之夜的场景表面上是写实的——Michèle说：“城中弦歌不断”，在市中心的新桥上欣赏国庆之夜的焰火歌声仿佛都是很自然的事情——实际却是一种现实中不可能存在的状况（各种音乐的拼接，塞纳河上美丽的焰火，偷窃游艇的典型喜剧方式），展现出超现实的诗意。在这个场景之后合情合理的叙事逐渐变得不可能，情节越来越多地融合了喜剧与荒诞的色彩：Michèle与Alex用麻醉剂Alcyon窃取钱财，从而在海边沙滩上度过一段伊甸园的日子；老流浪汉Hans从口袋里掏出了几乎全巴黎——甚至包括卢浮宫——的钥匙（很有几分巴尔扎克的味道，不是吗？）；还有前面描述过的，Alex的雨中噩梦，到处贴满Michèle凝视的眼睛；结尾，两人双双从新桥跳入塞纳河后，神秘的运沙船；等等。在场景之间存在着明显的时间、空间以及情节合理性的裂痕。5、荒诞的拼贴同样体现在人物的身份上：Alex，吐火者，杂耍艺人，身手矫健的跛子；Michèle，上校的女儿，街头流浪艺术家，即将失明的画家；Hans，掌握了全巴黎的钥匙却无家可归的新桥国王。并且，如果说Michèle的流浪多少出于被动（由于失恋或失明，二者孰先孰后我们并不知晓），那么Alex与Hans的选择则是完全主动的。正如Hans所说：“这种生活，对于我，对于Alex，都是唯一的生活。”选择没有家庭、固定的工作和遮风蔽雨的住所的生活，所追求的只有一个，就是自由。　　自由，不仅是导演指导创作的主旨，也是影片所要表达的主旨。 导演说：“我只有一个念头，要讲述一种没有化妆、没有电话、没有卧室的爱情，一种纯粹、迅猛的爱的状态。……有的人，一无所有，处于爱的年龄，发现这仿佛是种吞噬身心的陌生病毒。”自由，就是不依赖于任何有形的物质条件和精神凭藉。从这个意义上说，Michèle接受眼疾手术离开Alex是有理由的，因为眼疾使她日益依赖Alex，而后者因为害怕失去她企图使她成为新桥的囚徒。只有摆脱了从属地位，才能有真正的爱。　　然而这种自由，并不同于他们身后西岱岛上，司法宫（Palais de Justice）门楣上刻的、作为国家理念的自由。自由，与其让它承担起宗教的使命，不如只把它作为深藏在个人心中的信仰。（别忘了法国大革命的经典名句：“公民不自由，就强迫他自由。”正如Gottfried Keller针对现代民主社会的预言：“自由的最终胜利将是贫瘠的。”）是以，富有讽刺意味的一幕发生在影片后半部分，Michèle在Hans陪同下进入卢浮宫看她“失明前还想看最后一眼”的画时，她径直走过德拉克洛瓦的名画《自由引导人民》，都懒得顺便看一眼——虽然这幅画论尺径远大于那幅伦勃朗自画像，因而比后者更加适合于她惨淡的视力。6、导演在影片中揭示的意识形态的载体，不仅仅是影像，还有音乐和色彩。除了影片开头巡逻车上穿制服者得意洋洋的咏叹调与老妇人哼的迷茫的儿歌——影片中间Michèle视力日益恶化，在早锻炼的时候她哼唱的也是这同一首儿歌——的对立之外，大提琴低沉辽阔的音色被选中成为“国家力量”的象征，而摇滚乐、低音吉他则是分别属于Michèle和Alex的音乐。红、白、蓝，法国的象征，这里也被指摘出了其中蕴涵的意识形态成分。飞行表演的红白蓝烟雾宣布巴士底日开始；而影片结尾，Michèle和Alex在桥上约会， Michèle穿着白色短大衣走下出租车（与开头相呼应），Alex穿着黑色外套走出地铁；午夜过后，桥上只剩下他们时，Alex只穿着红色毛衣在桥栏上翻跟头，里面露出蓝色套头衫领；而当Alex识破Michèle的谎言向她走去时，风吹开Michèle的衣襟，她里面穿的也是蓝色毛衣——红、白、蓝跌落水底。被运沙船救起后，他们脱下湿衣，换上干燥的衣物，虽然还是以白色为主，间有橙红蔚蓝，但已经不是明显的三色对比搭配了。　　导演所反对的并非自由、平等、博爱本身，而是反对将它们作为国家理想，从而剥夺了原本属于个人的价值判断权，蜕变为意识形态。这里，在色彩的变换中，同样体现了这一立场。7、意识形态无所不在。作为渺小的个人，为了对抗这种无孔不入的力量，唯一的手段，就是给自己留下无法愈合的伤疤。Alex在Michèle走后开枪打断了自己的左手无名指，通过这一举动，他拒绝了被修补——这一枪的时机颇为到位，我们看到，虽然拥有酒、麻醉剂、手枪，Michèle、Alex和Hans对社会的拒斥始终是和平的，没有陷入性、暴力和毒品的纠葛。只有Alex放火焚烧Michèle的寻人启事时，阴差阳错烧死了贴广告的工人（这一段导演的处理不可思议地平静），因此，第二天早上，也就是Michèle离开和Alex自残之后，巴黎警察以惊人的效率找到他，把他从梦中唤醒，逮捕、殴打、审判、监禁。　　在监狱里度过的三年使Alex的跛腿痊愈——狱医显然和收容所的医生一样能干负责——但残缺的手依然固执住爱的记忆。而Michèle虽然治好了自己的眼睛，并且嫁给了她的眼科医生，曾被Alex的火焰灼伤的心灵却保留住他清晰的身影。最终，就像萨特（Jean-Paul Sartre）的剧本《脏手》（Les Mains sales）的最后一句台词：“不可回收！”一样，三年后的新年午夜，他们相拥从修整一新的新桥上纵身跳入塞纳河的举动，仿佛大声宣告：“不可修补！”8、个人通过保护肉体与精神的“裂痕”与国家修补术对抗，某种意义上说是一种自残、自伤的行为。因而“拼贴图”的主角必然是反英雄的形象：他/她的对抗方式是非组织、非暴力的，与“国家力量”的辉煌灿烂、意志坚强相比，他们渺小、不美。只有在对自己立场的坚持里，体现出一种孤独的不妥协，一种真正的英雄气质。　　因此，一个不可回避的困境是，在意识形态的象征因素被充分意识并彻底摒弃后，乌托邦也相应成为一片废墟——意识形态与乌托邦，它们本是一对相伴而生、彼此对立的兄弟。抛弃一切“错误意识”，也同时意味着抛弃乌托邦意识。“此岸－彼岸”的二元对立随之消失，乌托邦被“此时此地”（here-and-now）的幸福哲学所取代。拼贴作品中“此时此地”的力量是令人惊异的。它反映了对暂时性事件的更加平庸、即刻、日常的感觉，艺术家因此编织了一个浮动拆析的世界。临近尾声时Michèle所讲的“幸福人的故事”——“因为就是今晚！”正是这种哲学的体现。然而，随着圣母院凌晨三点的钟声敲响，提醒她外部世界的存在，她仿佛又忘记了自己刚刚所说的故事：“Alex，我必须回去了。……你对我要有耐心。有一天我会告诉你的，但不是今晚。……不，不，今晚不行！”　　“伊甸园”在人间寻不到位置：Michèle与Alex度过赤身裸体、天真无邪的几个月生活的“海边”并非一个具体地点，也不可能是一个真实的地点；而结尾他们乘运沙船离开巴黎，Michèle问船主：“你们去哪里？”回答：“直到尽头。”“哪儿？勒阿弗尔吗？”勒阿弗尔，塞纳河的入海口。这里指涉的显然也并非真实的、同样处在法国政府管理之下的勒阿弗尔市，而是取其位于陆地与大海之间的意义。就像“新桥”一样，它同样是一个漂浮的、不确定的能指。9、或许正是凭借超现实主义“轻盈”（légèreté）的伪装，我们才难以察觉最后一幕的沉重：在现实中的个人是无法战胜国家力量，或摆脱承载其合法性的一整套意识形态的，永远不可能。唯一的出路就是死亡。在冰凉蔚蓝的河水中，经过一番挣扎，两人忽然互相凝视，上方传来螺旋桨有力的声音。这似乎暗示着，他们的生命已经结束了。然而影片并没有结束。两人分别浮出水面，被救上这艘神秘的运沙船，见到两位神秘的船主，Michèle问：“你们是卖沙子的吗？”“不，我们只负责运送。”“这是我们最后一趟航行了。”另一个说。——“沙”是时间的象征，《圣经&#8226;约伯记》：“……必增添我的日子，多如尘沙。” ——Michèle要求：“能带上我们吗？”“可以。”“当然可以。我们安排一下。”于是，他们钻出船舱在沙堆上奔跑，如同跑过平原和山脉，来到船头大声歌唱那首在影片中第三次出现、然而情绪已经大不一样的儿歌，并快乐地喊道：“巴黎，你留在我心里！”但前方是什么？如儿歌所唱的，他们并不知道，只是随着塞纳河的波浪前行，“……然而塞纳河如此平静/永无忧扰/不分日夜/她温柔的流波/向着勒阿弗尔/向着大海/如同一个梦幻/滑过巴黎的神秘/和悲惨。”（节译自Jacques Prévert, Chanson de la Seine）10、影片中所展示出的世纪末情怀、意识形态的死亡留下的真空，只有审美的形式能够填补。有的批评家虽然认为这部影片的叙事充满缺陷，但也承认其视觉效果足以弥补这一缺陷。事实上，“有缺陷的叙事”，正是拼贴的典型特征；而令人愉悦的视觉效果，却是作品本身得以超越日常、易逝的材料碎片确立其艺术价值的保证。正如影片的这最后一幕所暗示的，漂浮的“碎片”不再孤独，它在永恒的时间之流中获得了意义；伤痕依然存在，但影片本身作为一件“拼贴”的艺术作品，获得了永恒的价值。 (END)))
[INFO ] 2018-09-30 01:28:10,688 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538242090000 ms.0 from job set of time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,689 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.687 s for time 1538242090000 ms (execution: 0.404 s)
[INFO ] 2018-09-30 01:28:10,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,693 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,696 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242090000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000'
[INFO ] 2018-09-30 01:28:10,697 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242090000 ms to writer queue
[INFO ] 2018-09-30 01:28:10,724 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239670000
[INFO ] 2018-09-30 01:28:10,725 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242090000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242090000', took 4522 bytes and 29 ms
[INFO ] 2018-09-30 01:28:10,726 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,728 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538242090000 ms
[INFO ] 2018-09-30 01:28:10,730 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-09-30 01:28:10,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538242085000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538239665552-1538239725552
[INFO ] 2018-09-30 01:28:10,741 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 01:28:10,742 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538242085000
[INFO ] 2018-09-30 01:28:13,839 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-09-30 01:28:13,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538242093841.
[WARN ] 2018-09-30 01:28:13,841 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-09-30 01:28:13,842 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-09-30 01:28:13,842 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-09-30 01:28:13,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-09-30 01:28:13,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-09-30 01:28:13,843 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 01:28:13,844 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-09-30 01:28:15,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,011 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242095000 ms to writer queue
[INFO ] 2018-09-30 01:28:15,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242095000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000'
[INFO ] 2018-09-30 01:28:15,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538242095000 ms.0 from job set of time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-09-30 01:28:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-09-30 01:28:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-09-30 01:28:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:28:15,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:28:15,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-09-30 01:28:15,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:15,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:15,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:34441 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:28:15,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:28:15,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000.bk
[INFO ] 2018-09-30 01:28:15,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242095000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000', took 4568 bytes and 32 ms
[INFO ] 2018-09-30 01:28:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:28:15,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-09-30 01:28:15,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 01:28:15,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-09-30 01:28:15,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 77 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 01:28:15,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 723 bytes result sent to driver
[INFO ] 2018-09-30 01:28:15,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:28:15,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:28:15,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (collect at StoreMovieEssay.scala:64) finished in 0.008 s
[INFO ] 2018-09-30 01:28:15,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: collect at StoreMovieEssay.scala:64, took 0.034566 s
[INFO ] 2018-09-30 01:28:15,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538242095000 ms.0 from job set of time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.056 s for time 1538242095000 ms (execution: 0.042 s)
[INFO ] 2018-09-30 01:28:15,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-09-30 01:28:15,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-09-30 01:28:15,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-09-30 01:28:15,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-09-30 01:28:15,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-09-30 01:28:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-09-30 01:28:15,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242095000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000'
[INFO ] 2018-09-30 01:28:15,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242095000 ms to writer queue
[INFO ] 2018-09-30 01:28:15,103 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239675000
[INFO ] 2018-09-30 01:28:15,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242095000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242095000', took 4524 bytes and 16 ms
[INFO ] 2018-09-30 01:28:15,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538242095000 ms
[INFO ] 2018-09-30 01:28:15,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 01:28:15,106 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538242095104 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 01:28:15,108 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 01:28:15,109 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-09-30 01:28:20,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538242100000
[INFO ] 2018-09-30 01:28:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538242100000 ms.0 from job set of time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:64
[INFO ] 2018-09-30 01:28:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-09-30 01:28:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (collect at StoreMovieEssay.scala:64) with 1 output partitions
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (collect at StoreMovieEssay.scala:64)
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:28:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:62), which has no missing parents
[INFO ] 2018-09-30 01:28:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242100000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000'
[INFO ] 2018-09-30 01:28:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242100000 ms to writer queue
[INFO ] 2018-09-30 01:28:20,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:20,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2026.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:28:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:34441 (size: 2026.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:28:20,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:28:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:28:20,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-09-30 01:28:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-09-30 01:28:20,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-09-30 01:28:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000.bk
[INFO ] 2018-09-30 01:28:20,038 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242100000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000', took 4560 bytes and 13 ms
[INFO ] 2018-09-30 01:28:20,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 77 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-09-30 01:28:20,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 723 bytes result sent to driver
[INFO ] 2018-09-30 01:28:20,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:28:20,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:28:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (collect at StoreMovieEssay.scala:64) finished in 0.007 s
[INFO ] 2018-09-30 01:28:20,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: collect at StoreMovieEssay.scala:64, took 0.021481 s
[INFO ] 2018-09-30 01:28:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538242100000 ms.0 from job set of time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.045 s for time 1538242100000 ms (execution: 0.028 s)
[INFO ] 2018-09-30 01:28:20,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-09-30 01:28:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-09-30 01:28:20,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-09-30 01:28:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-09-30 01:28:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-09-30 01:28:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-09-30 01:28:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538242100000 ms to writer queue
[INFO ] 2018-09-30 01:28:20,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538242100000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000'
[INFO ] 2018-09-30 01:28:20,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538239680000
[INFO ] 2018-09-30 01:28:20,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538242100000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538242100000', took 4522 bytes and 15 ms
[INFO ] 2018-09-30 01:28:20,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538242100000 ms
[INFO ] 2018-09-30 01:28:20,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-09-30 01:28:20,066 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538242100066 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-09-30 01:28:20,067 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-09-30 01:28:20,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538242090000 ms
[INFO ] 2018-09-30 01:28:20,125 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-09-30 01:28:20,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-09-30 01:28:20,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-09-30 01:28:20,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-09-30 01:28:20,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-09-30 01:28:20,136 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 01:28:20,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:28:20,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 01:28:20,160 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 01:28:20,161 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 01:28:20,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 01:28:20,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 01:28:20,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 01:28:20,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 01:28:20,168 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-a4eec785-b304-4e95-b84a-8427843079d7
[INFO ] 2018-09-30 01:30:39,185 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-09-30 01:30:39,608 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-09-30 01:30:39,736 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-09-30 01:30:39,736 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-09-30 01:30:39,793 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-09-30 01:30:39,808 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-09-30 01:30:39,809 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-09-30 01:30:39,810 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-09-30 01:30:39,810 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-09-30 01:30:39,811 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-09-30 01:30:40,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 42379.
[INFO ] 2018-09-30 01:30:40,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-09-30 01:30:40,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-09-30 01:30:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-09-30 01:30:40,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-09-30 01:30:40,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-047e71dc-fa26-4d83-949f-46128e972144
[INFO ] 2018-09-30 01:30:40,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-09-30 01:30:40,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-09-30 01:30:40,295 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-09-30 01:30:40,347 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:30:40,440 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-09-30 01:30:40,457 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35853.
[INFO ] 2018-09-30 01:30:40,457 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:35853
[INFO ] 2018-09-30 01:30:40,458 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-09-30 01:30:40,459 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 35853, None)
[INFO ] 2018-09-30 01:30:40,462 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:35853 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 35853, None)
[INFO ] 2018-09-30 01:30:40,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 35853, None)
[INFO ] 2018-09-30 01:30:40,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 35853, None)
[INFO ] 2018-09-30 01:30:40,671 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-09-30 01:30:40,672 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-09-30 01:30:41,259 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-09-30 01:30:41,501 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at StoreMovieEssay.scala:44
[INFO ] 2018-09-30 01:30:41,516 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at StoreMovieEssay.scala:44) with 1 output partitions
[INFO ] 2018-09-30 01:30:41,516 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at StoreMovieEssay.scala:44)
[INFO ] 2018-09-30 01:30:41,517 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-09-30 01:30:41,518 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-09-30 01:30:41,521 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:42), which has no missing parents
[INFO ] 2018-09-30 01:30:41,594 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1392.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:30:41,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 912.0 B, free 1406.4 MB)
[INFO ] 2018-09-30 01:30:41,618 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:35853 (size: 912.0 B, free: 1406.4 MB)
[INFO ] 2018-09-30 01:30:41,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-09-30 01:30:41,636 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at StoreMovieEssay.scala:42) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-09-30 01:30:41,637 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-09-30 01:30:41,671 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5097 bytes)
[INFO ] 2018-09-30 01:30:41,676 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-09-30 01:30:41,725 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 997 bytes result sent to driver
[INFO ] 2018-09-30 01:30:41,734 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 77 ms on localhost (executor driver) (1/1)
[INFO ] 2018-09-30 01:30:41,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-09-30 01:30:41,740 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at StoreMovieEssay.scala:44) finished in 0.091 s
[INFO ] 2018-09-30 01:30:41,746 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at StoreMovieEssay.scala:44, took 0.244597 s
[INFO ] 2018-09-30 01:30:41,749 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
List(Review(1212,242,sdfsd), Review(54,d42,635456))
[INFO ] 2018-09-30 01:30:41,758 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-09-30 01:30:41,772 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-09-30 01:30:41,797 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-09-30 01:30:41,805 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-09-30 01:30:41,806 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-09-30 01:30:41,813 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-09-30 01:30:41,814 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-09-30 01:30:41,816 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-09-30 01:30:41,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-09-30 01:30:41,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-bfd576be-cd29-4f09-a7da-d0df33b101ff
