[INFO ] 2018-10-02 00:14:38,790 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 00:14:39,182 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 00:14:39,334 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 00:14:39,334 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 00:14:39,365 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-02 00:14:39,379 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 00:14:39,380 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 00:14:39,381 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 00:14:39,381 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 00:14:39,382 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 00:14:39,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 41585.
[INFO ] 2018-10-02 00:14:39,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 00:14:39,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 00:14:39,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 00:14:39,664 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 00:14:39,670 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-3643aecf-a478-4a17-bac9-fb342e52f707
[INFO ] 2018-10-02 00:14:39,734 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 00:14:39,789 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 00:14:39,975 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 00:14:40,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:14:40,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 00:14:40,191 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36915.
[INFO ] 2018-10-02 00:14:40,192 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:36915
[INFO ] 2018-10-02 00:14:40,193 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 00:14:40,194 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 36915, None)
[INFO ] 2018-10-02 00:14:40,197 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:36915 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 36915, None)
[INFO ] 2018-10-02 00:14:40,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 36915, None)
[INFO ] 2018-10-02 00:14:40,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 36915, None)
[INFO ] 2018-10-02 00:14:40,392 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 00:14:40,393 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 00:14:40,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 00:14:40,931 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is :192.168.0.100:9092
[INFO ] 2018-10-02 00:14:40,932 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
topic:test-flume-topic
[WARN ] 2018-10-02 00:14:40,937 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[INFO ] 2018-10-02 00:14:41,000 utils.CommonUtil$.getCheckpointDir(CommonUtil.scala:54)
checkpointDir is :/home/feng/software/code/bigdata/spark-warehouse
[WARN ] 2018-10-02 00:14:41,147 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-02 00:14:41,148 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-02 00:14:41,148 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-02 00:14:41,149 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-02 00:14:41,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-02 00:14:41,323 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 2 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-02 00:14:41,335 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-02 00:14:41,336 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 00:14:41,336 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 00:14:41,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-02 00:14:41,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-02 00:14:41,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-02 00:14:41,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 00:14:41,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c87fdf2
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 10000 ms
[INFO ] 2018-10-02 00:14:41,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 00:14:41,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 00:14:41,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 10000 ms
[INFO ] 2018-10-02 00:14:41,339 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-02 00:14:41,390 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 00:14:41,455 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 00:14:41,475 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 00:14:41,476 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 00:14:41,621 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-02 00:14:41,622 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 00:14:41,622 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-02 00:14:41,636 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 17
[INFO ] 2018-10-02 00:14:41,637 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 00:14:41,647 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538410490000
[INFO ] 2018-10-02 00:14:41,647 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538410490000 ms
[INFO ] 2018-10-02 00:14:41,648 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-02 00:14:41,653 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-02 00:14:50,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:50,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:50,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:50,077 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538410490000 ms.0 from job set of time 1538410490000 ms
[INFO ] 2018-10-02 00:14:50,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:50,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410490000 ms to writer queue
[INFO ] 2018-10-02 00:14:50,094 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000'
[INFO ] 2018-10-02 00:14:50,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-02 00:14:50,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000.bk
[INFO ] 2018-10-02 00:14:50,147 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000', took 4697 bytes and 52 ms
[INFO ] 2018-10-02 00:14:50,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-02 00:14:50,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-02 00:14:50,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:14:50,150 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:14:50,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-02 00:14:50,209 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,331 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:36915 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,341 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:14:50,355 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:14:50,356 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 00:14:50,382 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 00:14:50,387 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 00:14:50,440 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 49 -> 56
[INFO ] 2018-10-02 00:14:50,442 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-02 00:14:50,443 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-02 00:14:50,445 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 00:14:50,447 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 00:14:50,448 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 00:14:50,449 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 00:14:50,451 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 49
[INFO ] 2018-10-02 00:14:50,554 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-02 00:14:50,598 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 5267 bytes result sent to driver
[INFO ] 2018-10-02 00:14:50,623 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 248 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:14:50,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:14:50,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:80) finished in 0.262 s
[INFO ] 2018-10-02 00:14:50,633 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:80, took 0.505320 s
[INFO ] 2018-10-02 00:14:50,644 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:81
[INFO ] 2018-10-02 00:14:50,645 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:81) with 1 output partitions
[INFO ] 2018-10-02 00:14:50,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:81)
[INFO ] 2018-10-02 00:14:50,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:14:50,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:14:50,646 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:81), which has no missing parents
[INFO ] 2018-10-02 00:14:50,649 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,675 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2019.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,678 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:36915 (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,683 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:14:50,684 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:81) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:14:50,684 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-02 00:14:50,705 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 00:14:50,707 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-02 00:14:50,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:36915 in memory (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:50,716 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 49 -> 56
[INFO ] 2018-10-02 00:14:50,716 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 49
[INFO ] 2018-10-02 00:14:51,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/314656554544 
[INFO ] 2018-10-02 00:14:51,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/314656554544 
[INFO ] 2018-10-02 00:14:51,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/314656554544 
[INFO ] 2018-10-02 00:14:51,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/314656554544 ajkhdkfj
[INFO ] 2018-10-02 00:14:51,091 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 23310 bytes result sent to driver
[INFO ] 2018-10-02 00:14:51,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 414 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:14:51,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:14:51,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:81) finished in 0.414 s
[INFO ] 2018-10-02 00:14:51,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:81, took 0.456839 s
[INFO ] 2018-10-02 00:14:51,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
save reviewList to hbase:3
[INFO ] 2018-10-02 00:14:51,102 utils.CommonUtil$.getWriteHbaseConfig(CommonUtil.scala:29)
zookeeperQuorum is :localhost
[WARN ] 2018-10-02 00:14:51,153 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-10-02 00:14:51,198 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: saveAsHadoopDataset at StoreMovieEssay.scala:84
[INFO ] 2018-10-02 00:14:51,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (saveAsHadoopDataset at StoreMovieEssay.scala:84) with 1 output partitions
[INFO ] 2018-10-02 00:14:51,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:84)
[INFO ] 2018-10-02 00:14:51,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:14:51,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:14:51,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84), which has no missing parents
[INFO ] 2018-10-02 00:14:51,215 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-10-02 00:14:51,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.5 KB, free 1406.3 MB)
[INFO ] 2018-10-02 00:14:51,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:36915 (size: 28.5 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:51,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:14:51,223 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:84) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:14:51,224 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-02 00:14:51,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 27343 bytes)
[INFO ] 2018-10-02 00:14:51,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-02 00:14:51,398 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:122)
Process identifier=hconnection-0x5309648f connecting to ZooKeeper ensemble=localhost:2181
[WARN ] 2018-10-02 00:14:51,435 org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)
Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
[INFO ] 2018-10-02 00:14:51,452 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:376)
Scheduled snapshot period at 10 second(s).
[INFO ] 2018-10-02 00:14:51,452 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)
HBase metrics system started
[INFO ] 2018-10-02 00:14:51,468 org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:65)
Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:host.name=feng
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.version=1.8.0_181
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.vendor=Oracle Corporation
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
[INFO ] 2018-10-02 00:14:51,480 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.class.path=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/home/feng/software/scala-2.11.11/lib/akka-actor_2.11-2.3.16.jar:/home/feng/software/scala-2.11.11/lib/config-1.2.1.jar:/home/feng/software/scala-2.11.11/lib/jline-2.14.3.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-2.11.0.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-migration_2.11-1.1.0.jar:/home/feng/software/scala-2.11.11/lib/scala-compiler.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-library_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-plugin_2.11.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-library.jar:/home/feng/software/scala-2.11.11/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/scala-2.11.11/lib/scala-reflect.jar:/home/feng/software/scala-2.11.11/lib/scala-swing_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-xml_2.11-1.0.5.jar:/home/feng/software/scala-2.11.11/lib/scalap-2.11.11.jar:/home/feng/software/code/bigdata/target/classes:/home/feng/software/code/jars/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/feng/software/code/jars/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/feng/software/code/jars/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/feng/software/code/jars/org/tukaani/xz/1.0/xz-1.0.jar:/home/feng/software/code/jars/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/feng/software/code/jars/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/home/feng/software/code/jars/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/feng/software/code/jars/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/feng/software/code/jars/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/feng/software/code/jars/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/home/feng/software/code/jars/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/feng/software/code/jars/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/feng/software/code/jars/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/feng/software/code/jars/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/feng/software/code/jars/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/feng/software/code/jars/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/home/feng/software/code/jars/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/home/feng/software/code/jars/net/java/dev/jets3t/jets3t/0.9.3/jets3t-0.9.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/home/feng/software/code/jars/commons-codec/commons-codec/1.8/commons-codec-1.8.jar:/home/feng/software/code/jars/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/feng/software/code/jars/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar:/home/feng/software/code/jars/javax/mail/mail/1.4.7/mail-1.4.7.jar:/home/feng/software/code/jars/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar:/home/feng/software/code/jars/com/jamesmurty/utils/java-xmlbuilder/1.0/java-xmlbuilder-1.0.jar:/home/feng/software/code/jars/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/feng/software/code/jars/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/feng/software/code/jars/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/feng/software/code/jars/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/feng/software/code/jars/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/feng/software/code/jars/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/feng/software/code/jars/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/feng/software/code/jars/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/feng/software/code/jars/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/feng/software/code/jars/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/feng/software/code/jars/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/feng/software/code/jars/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/feng/software/code/jars/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/feng/software/code/jars/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/feng/software/code/jars/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/feng/software/code/jars/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/feng/software/code/jars/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/feng/software/code/jars/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/feng/software/code/jars/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-xml_2.11/1.0.1/scala-xml_2.11-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/feng/software/code/jars/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/feng/software/code/jars/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/feng/software/code/jars/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/feng/software/code/jars/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/feng/software/code/jars/io/netty/netty-all/4.0.43.Final/netty-all-4.0.43.Final.jar:/home/feng/software/code/jars/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/feng/software/code/jars/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/home/feng/software/code/jars/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/home/feng/software/code/jars/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/feng/software/code/jars/oro/oro/2.0.8/oro-2.0.8.jar:/home/feng/software/code/jars/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/feng/software/code/jars/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/home/feng/software/code/jars/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/feng/software/code/jars/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/home/feng/software/code/jars/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/home/feng/software/code/jars/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/home/feng/software/code/jars/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/home/feng/software/code/jars/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/home/feng/software/code/jars/com/101tec/zkclient/0.8/zkclient-0.8.jar:/home/feng/software/code/jars/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/code/jars/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-spark/4.14.0-HBase-1.4/phoenix-spark-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-core/4.14.0-HBase-1.4/phoenix-core-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-api/0.14.0-incubating/tephra-api-0.14.0-incubating.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-core/0.14.0-incubating/tephra-core-0.14.0-incubating.jar:/home/feng/software/code/jars/com/google/inject/guice/3.0/guice-3.0.jar:/home/feng/software/code/jars/javax/inject/javax.inject/1/javax.inject-1.jar:/home/feng/software/code/jars/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/feng/software/code/jars/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/feng/software/code/jars/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/feng/software/code/jars/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/feng/software/code/jars/org/apache/twill/twill-common/0.8.0/twill-common-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-core/0.8.0/twill-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-api/0.8.0/twill-api-0.8.0.jar:/home/feng/software/code/jars/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-api/0.8.0/twill-discovery-api-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-core/0.8.0/twill-discovery-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-zookeeper/0.8.0/twill-zookeeper-0.8.0.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-hbase-compat-1.4/0.14.0-incubating/tephra-hbase-compat-1.4-0.14.0-incubating.jar:/home/feng/software/code/jars/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/feng/software/code/jars/jline/jline/2.11/jline-2.11.jar:/home/feng/software/code/jars/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/feng/software/code/jars/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/feng/software/code/jars/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/feng/software/code/jars/junit/junit/4.12/junit-4.12.jar:/home/feng/software/code/jars/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/feng/software/code/jars/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/feng/software/code/jars/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/feng/software/code/jars/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/feng/software/code/jars/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/feng/software/code/jars/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/feng/software/code/jars/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-annotations/1.4.0/hbase-annotations-1.4.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-protocol/1.4.0/hbase-protocol-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-server/1.4.0/hbase-server-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-procedure/1.4.0/hbase-procedure-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0-tests.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-prefix-tree/1.4.0/hbase-prefix-tree-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics-api/1.4.0/hbase-metrics-api-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics/1.4.0/hbase-metrics-1.4.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/feng/software/code/jars/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/feng/software/code/jars/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/feng/software/code/jars/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/feng/software/code/jars/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop-compat/1.4.0/hbase-hadoop-compat-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop2-compat/1.4.0/hbase-hadoop2-compat-1.4.0.jar:/home/feng/software/code/jars/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/feng/software/code/jars/com/salesforce/i18n/i18n-util/1.0.4/i18n-util-1.0.4.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j/60.2/icu4j-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-localespi/60.2/icu4j-localespi-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-charset/60.2/icu4j-charset-60.2.jar:/home/feng/software/code/jars/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-common/2.7.5/hadoop-common-2.7.5.jar:/home/feng/software/code/jars/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/feng/software/code/jars/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/feng/software/code/jars/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/feng/software/code/jars/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/feng/software/code/jars/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/feng/software/code/jars/asm/asm/3.1/asm-3.1.jar:/home/feng/software/code/jars/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/feng/software/code/jars/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/feng/software/code/jars/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/feng/software/code/jars/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/feng/software/code/jars/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-auth/2.7.5/hadoop-auth-2.7.5.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/feng/software/code/jars/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/feng/software/code/jars/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/feng/software/code/jars/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-client/1.4.0/hbase-client-1.4.0.jar:/home/feng/software/code/jars/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/feng/software/code/jars/io/debezium/debezium-core/0.8.1.Final/debezium-core-0.8.1.Final.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar:/home/feng/software/code/jars/com/alibaba/fastjson/1.2.47/fastjson-1.2.47.jar:/home/feng/software/code/jars/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/feng/software/idea/idea-IC-181.5281.24/lib/idea_rt.jar
[INFO ] 2018-10-02 00:14:51,481 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[INFO ] 2018-10-02 00:14:51,481 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.io.tmpdir=/tmp
[INFO ] 2018-10-02 00:14:51,481 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.compiler=<NA>
[INFO ] 2018-10-02 00:14:51,481 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.name=Linux
[INFO ] 2018-10-02 00:14:51,482 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.arch=amd64
[INFO ] 2018-10-02 00:14:51,482 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.version=4.13.0-36-generic
[INFO ] 2018-10-02 00:14:51,482 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.name=feng
[INFO ] 2018-10-02 00:14:51,482 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.home=/home/feng
[INFO ] 2018-10-02 00:14:51,482 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.dir=/home/feng/software/code/bigdata
[INFO ] 2018-10-02 00:14:51,483 org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@2f33398d
[INFO ] 2018-10-02 00:14:51,496 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO ] 2018-10-02 00:14:51,497 org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:852)
Socket connection established to localhost/127.0.0.1:2181, initiating session
[INFO ] 2018-10-02 00:14:51,525 org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1235)
Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1662fedb0b6002b, negotiated timeout = 40000
[INFO ] 2018-10-02 00:14:51,942 org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.closeZooKeeperWatcher(ConnectionManager.java:1757)
Closing zookeeper sessionid=0x1662fedb0b6002b
[INFO ] 2018-10-02 00:14:51,998 org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684)
Session: 0x1662fedb0b6002b closed
[INFO ] 2018-10-02 00:14:51,998 org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
EventThread shut down
[INFO ] 2018-10-02 00:14:52,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
No need to commit output of task because needsTaskCommit=false: attempt_20181002001451_0002_m_000000_2
[INFO ] 2018-10-02 00:14:52,009 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 709 bytes result sent to driver
[INFO ] 2018-10-02 00:14:52,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 791 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:14:52,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:14:52,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:84) finished in 0.781 s
[INFO ] 2018-10-02 00:14:52,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: saveAsHadoopDataset at StoreMovieEssay.scala:84, took 0.818667 s
[WARN ] 2018-10-02 00:14:52,018 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:322)
Output Path is null in commitJob()
[INFO ] 2018-10-02 00:14:52,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty2
[INFO ] 2018-10-02 00:14:52,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538410490000 ms.0 from job set of time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 2.019 s for time 1538410490000 ms (execution: 1.943 s)
[INFO ] 2018-10-02 00:14:52,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410490000 ms to writer queue
[INFO ] 2018-10-02 00:14:52,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410490000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000'
[INFO ] 2018-10-02 00:14:52,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_2_piece0 on 192.168.0.100:36915 in memory (size: 28.5 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:52,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_1_piece0 on 192.168.0.100:36915 in memory (size: 2019.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:14:52,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407490000
[INFO ] 2018-10-02 00:14:52,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410490000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000', took 4693 bytes and 40 ms
[INFO ] 2018-10-02 00:14:52,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538410490000 ms
[INFO ] 2018-10-02 00:14:52,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-02 00:14:52,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 2 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538410480000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538407441001-1538407501001
file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538407510061-1538407570061
[INFO ] 2018-10-02 00:14:52,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 00:14:52,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538410480000
[INFO ] 2018-10-02 00:14:52,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538410480000
[INFO ] 2018-10-02 00:14:55,939 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-02 00:14:55,941 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538410495941.
[WARN ] 2018-10-02 00:14:55,941 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-02 00:14:55,942 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-02 00:14:55,943 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-02 00:14:55,943 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-02 00:14:55,944 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-02 00:14:55,944 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 00:14:55,945 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 00:15:00,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538410500000 ms.0 from job set of time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410500000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000'
[INFO ] 2018-10-02 00:15:00,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410500000 ms to writer queue
[INFO ] 2018-10-02 00:15:00,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-02 00:15:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-02 00:15:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-02 00:15:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:15:00,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:15:00,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-02 00:15:00,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-02 00:15:00,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2033.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:15:00,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:36915 (size: 2033.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:15:00,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:15:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:15:00,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-02 00:15:00,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 00:15:00,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-02 00:15:00,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000.bk
[INFO ] 2018-10-02 00:15:00,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410500000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000', took 4730 bytes and 28 ms
[INFO ] 2018-10-02 00:15:00,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 00:15:00,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 702 bytes result sent to driver
[INFO ] 2018-10-02 00:15:00,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 21 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:15:00,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:15:00,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:80) finished in 0.022 s
[INFO ] 2018-10-02 00:15:00,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:80, took 0.035327 s
[INFO ] 2018-10-02 00:15:00,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty8
[INFO ] 2018-10-02 00:15:00,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538410500000 ms.0 from job set of time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.059 s for time 1538410500000 ms (execution: 0.042 s)
[INFO ] 2018-10-02 00:15:00,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-02 00:15:00,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-02 00:15:00,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-02 00:15:00,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-02 00:15:00,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-02 00:15:00,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-02 00:15:00,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410500000 ms to writer queue
[INFO ] 2018-10-02 00:15:00,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410500000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000'
[INFO ] 2018-10-02 00:15:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407500000
[INFO ] 2018-10-02 00:15:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410500000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000', took 4686 bytes and 14 ms
[INFO ] 2018-10-02 00:15:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,078 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538410500000 ms
[INFO ] 2018-10-02 00:15:00,079 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 00:15:00,080 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538410500079 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 00:15:00,082 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 00:15:00,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 00:15:10,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538410510000
[INFO ] 2018-10-02 00:15:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538410510000 ms.0 from job set of time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:80
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:80) with 1 output partitions
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:80)
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:15:10,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:78), which has no missing parents
[INFO ] 2018-10-02 00:15:10,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 00:15:10,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410510000 ms to writer queue
[INFO ] 2018-10-02 00:15:10,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410510000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000'
[INFO ] 2018-10-02 00:15:10,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-02 00:15:10,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2035.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:15:10,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:36915 (size: 2035.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:15:10,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:15:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:78) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:15:10,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-02 00:15:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 00:15:10,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-02 00:15:10,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000.bk
[INFO ] 2018-10-02 00:15:10,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410510000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000', took 4722 bytes and 17 ms
[INFO ] 2018-10-02 00:15:10,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 56 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 00:15:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 702 bytes result sent to driver
[INFO ] 2018-10-02 00:15:10,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 12 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:15:10,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:15:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:80) finished in 0.014 s
[INFO ] 2018-10-02 00:15:10,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:80, took 0.030050 s
[INFO ] 2018-10-02 00:15:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty11
[INFO ] 2018-10-02 00:15:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538410510000 ms.0 from job set of time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.052 s for time 1538410510000 ms (execution: 0.037 s)
[INFO ] 2018-10-02 00:15:10,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-02 00:15:10,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-02 00:15:10,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-02 00:15:10,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-02 00:15:10,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-02 00:15:10,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-02 00:15:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538410510000 ms to writer queue
[INFO ] 2018-10-02 00:15:10,059 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538410510000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000'
[INFO ] 2018-10-02 00:15:10,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407510000
[INFO ] 2018-10-02 00:15:10,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538410510000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000', took 4686 bytes and 10 ms
[INFO ] 2018-10-02 00:15:10,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538410510000 ms
[INFO ] 2018-10-02 00:15:10,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 00:15:10,070 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538410510070 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 00:15:10,070 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 00:15:10,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538410490000 ms
[INFO ] 2018-10-02 00:15:10,125 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 00:15:10,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 1 ms.
[INFO ] 2018-10-02 00:15:10,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-02 00:15:10,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-02 00:15:10,135 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-02 00:15:10,135 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-02 00:15:10,141 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:15:10,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 00:15:10,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 00:15:10,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 00:15:10,159 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 00:15:10,160 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 00:15:10,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 00:15:10,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 00:15:10,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-cf7fb294-7b58-4e71-8277-2adc4892113a
[INFO ] 2018-10-02 00:25:49,773 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291559,16456,弥新永恒不变的。)
[INFO ] 2018-10-02 00:25:49,776 sample.CommonTest$$anonfun$main$1.apply(CommonTest.scala:20)
Review(1291560,1118154,影片开始的的名义)
[INFO ] 2018-10-02 00:25:49,776 sample.CommonTest$.reverse(CommonTest.scala:32)
21565
[INFO ] 2018-10-02 00:25:49,778 sample.CommonTest$.reverse(CommonTest.scala:33)
56512
[INFO ] 2018-10-02 00:25:49,779 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is : 192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092
[INFO ] 2018-10-02 00:25:49,779 sample.CommonTest$.testConfig(CommonTest.scala:26)
192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092
[INFO ] 2018-10-02 00:25:49,780 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is : 192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092
[INFO ] 2018-10-02 00:25:49,780 sample.CommonTest$.testConfig(CommonTest.scala:28)
192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092
[INFO ] 2018-10-02 00:26:18,546 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 00:26:18,972 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 00:26:19,067 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 00:26:19,067 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 00:26:19,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: WordCount
[INFO ] 2018-10-02 00:26:19,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 00:26:19,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 00:26:19,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 00:26:19,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 00:26:19,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 00:26:19,457 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 42131.
[INFO ] 2018-10-02 00:26:19,489 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 00:26:19,505 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 00:26:19,508 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 00:26:19,508 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 00:26:19,515 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-f2ddb8fa-7450-4aae-ab23-7b9fcbd9c57d
[INFO ] 2018-10-02 00:26:19,577 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 00:26:19,694 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 00:26:19,866 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 00:26:19,927 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:26:20,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 00:26:20,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37271.
[INFO ] 2018-10-02 00:26:20,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:37271
[INFO ] 2018-10-02 00:26:20,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 00:26:20,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 37271, None)
[INFO ] 2018-10-02 00:26:20,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:37271 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 37271, None)
[INFO ] 2018-10-02 00:26:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 37271, None)
[INFO ] 2018-10-02 00:26:20,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 37271, None)
[INFO ] 2018-10-02 00:26:20,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 00:26:20,235 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 00:26:20,774 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 00:26:21,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at WordCount.scala:20
[INFO ] 2018-10-02 00:26:21,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at WordCount.scala:20) with 1 output partitions
[INFO ] 2018-10-02 00:26:21,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at WordCount.scala:20)
[INFO ] 2018-10-02 00:26:21,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:26:21,058 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:26:21,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20), which has no missing parents
[INFO ] 2018-10-02 00:26:21,137 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1392.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:26:21,163 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 924.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:26:21,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:37271 (size: 924.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:26:21,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:26:21,181 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:26:21,181 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 00:26:21,217 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4776 bytes)
[INFO ] 2018-10-02 00:26:21,222 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 00:26:21,271 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 715 bytes result sent to driver
[INFO ] 2018-10-02 00:26:21,276 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 74 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:26:21,278 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:26:21,281 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at WordCount.scala:20) finished in 0.087 s
[INFO ] 2018-10-02 00:26:21,297 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at WordCount.scala:20, took 0.254765 s
[INFO ] 2018-10-02 00:26:21,300 sample.WordCount$.main(WordCount.scala:21)
-------------------sdf
[INFO ] 2018-10-02 00:26:21,309 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:26:21,318 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 00:26:21,326 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 00:26:21,327 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 00:26:21,331 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 00:26:21,332 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 00:26:21,335 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 00:26:21,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 00:26:21,338 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-0fdf998a-554a-4d4f-ad23-58d12c2d920a
[INFO ] 2018-10-02 00:33:29,217 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 00:33:29,655 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 00:33:29,759 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 00:33:29,759 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 00:33:29,783 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: WordCount
[INFO ] 2018-10-02 00:33:29,799 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 00:33:29,800 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 00:33:29,800 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 00:33:29,801 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 00:33:29,801 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 00:33:30,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 41443.
[INFO ] 2018-10-02 00:33:30,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 00:33:30,095 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 00:33:30,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 00:33:30,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 00:33:30,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-e51f0220-ddf1-4756-9983-9bdec1600660
[INFO ] 2018-10-02 00:33:30,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 00:33:30,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 00:33:30,369 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 00:33:30,430 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:33:30,530 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 00:33:30,551 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33885.
[INFO ] 2018-10-02 00:33:30,552 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:33885
[INFO ] 2018-10-02 00:33:30,553 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 00:33:30,554 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 33885, None)
[INFO ] 2018-10-02 00:33:30,557 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:33885 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 33885, None)
[INFO ] 2018-10-02 00:33:30,560 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 33885, None)
[INFO ] 2018-10-02 00:33:30,561 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 33885, None)
[INFO ] 2018-10-02 00:33:30,768 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 00:33:30,769 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 00:33:31,387 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 00:33:31,669 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at WordCount.scala:20
[INFO ] 2018-10-02 00:33:31,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at WordCount.scala:20) with 1 output partitions
[INFO ] 2018-10-02 00:33:31,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at WordCount.scala:20)
[INFO ] 2018-10-02 00:33:31,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:33:31,686 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:33:31,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20), which has no missing parents
[INFO ] 2018-10-02 00:33:31,762 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1392.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:33:31,783 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 924.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:33:31,785 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:33885 (size: 924.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:33:31,787 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:33:31,802 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:33:31,803 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 00:33:31,838 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4776 bytes)
[INFO ] 2018-10-02 00:33:31,847 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 00:33:31,898 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 715 bytes result sent to driver
[INFO ] 2018-10-02 00:33:31,907 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 83 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:33:31,913 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at WordCount.scala:20) finished in 0.097 s
[INFO ] 2018-10-02 00:33:31,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:33:31,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at WordCount.scala:20, took 0.252823 s
[INFO ] 2018-10-02 00:33:31,926 sample.WordCount$.main(WordCount.scala:21)
-------------------sdf
[INFO ] 2018-10-02 00:33:31,934 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:33:31,950 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 00:33:31,963 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 00:33:31,964 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 00:33:31,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 00:33:31,969 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 00:33:31,986 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 00:33:31,991 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 00:33:31,994 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-52d55536-8068-4a28-b644-a856b93a53a6
[INFO ] 2018-10-02 00:35:05,495 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 00:35:05,894 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 00:35:05,998 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 00:35:05,998 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 00:35:06,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: WordCount
[INFO ] 2018-10-02 00:35:06,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 00:35:06,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 00:35:06,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 00:35:06,047 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 00:35:06,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 00:35:06,286 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 36105.
[INFO ] 2018-10-02 00:35:06,304 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 00:35:06,317 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 00:35:06,319 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 00:35:06,319 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 00:35:06,325 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-ef340971-52d0-4b33-8377-d34b4ec11964
[INFO ] 2018-10-02 00:35:06,370 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 00:35:06,411 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 00:35:06,547 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 00:35:06,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:35:06,672 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 00:35:06,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46269.
[INFO ] 2018-10-02 00:35:06,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:46269
[INFO ] 2018-10-02 00:35:06,697 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 00:35:06,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 46269, None)
[INFO ] 2018-10-02 00:35:06,700 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:46269 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 46269, None)
[INFO ] 2018-10-02 00:35:06,702 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 46269, None)
[INFO ] 2018-10-02 00:35:06,703 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 46269, None)
[INFO ] 2018-10-02 00:35:06,910 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 00:35:06,910 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 00:35:07,479 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 00:35:07,721 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: collect at WordCount.scala:20
[INFO ] 2018-10-02 00:35:07,735 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (collect at WordCount.scala:20) with 1 output partitions
[INFO ] 2018-10-02 00:35:07,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (collect at WordCount.scala:20)
[INFO ] 2018-10-02 00:35:07,736 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 00:35:07,737 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 00:35:07,740 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20), which has no missing parents
[INFO ] 2018-10-02 00:35:07,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 1392.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:35:07,839 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 924.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 00:35:07,841 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:46269 (size: 924.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 00:35:07,844 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 00:35:07,858 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at makeRDD at WordCount.scala:20) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 00:35:07,859 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 00:35:07,892 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4776 bytes)
[INFO ] 2018-10-02 00:35:07,898 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 00:35:07,958 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 715 bytes result sent to driver
[INFO ] 2018-10-02 00:35:07,965 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 87 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 00:35:07,980 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 00:35:07,981 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (collect at WordCount.scala:20) finished in 0.110 s
[INFO ] 2018-10-02 00:35:07,989 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: collect at WordCount.scala:20, took 0.266843 s
[INFO ] 2018-10-02 00:35:07,994 sample.WordCount$.main(WordCount.scala:21)
-------------------sdf
[INFO ] 2018-10-02 00:35:08,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 00:35:08,013 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 00:35:08,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 00:35:08,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 00:35:08,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 00:35:08,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 00:35:08,061 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 00:35:08,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 00:35:08,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-5696120f-a271-4533-a39f-36a5aeec5c78
[INFO ] 2018-10-02 22:29:57,710 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 22:29:58,125 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 22:29:58,239 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 22:29:58,240 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 22:29:58,269 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-02 22:29:58,282 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 22:29:58,283 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 22:29:58,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 22:29:58,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 22:29:58,284 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 22:29:58,537 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 43101.
[INFO ] 2018-10-02 22:29:58,552 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 22:29:58,566 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 22:29:58,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 22:29:58,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 22:29:58,581 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-18c5f49a-b81c-4e34-8b5b-8079ebfc05ee
[INFO ] 2018-10-02 22:29:58,619 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 22:29:58,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 22:29:58,792 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 22:29:58,833 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:29:58,912 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 22:29:58,935 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45415.
[INFO ] 2018-10-02 22:29:58,936 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:45415
[INFO ] 2018-10-02 22:29:58,937 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 22:29:58,939 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 45415, None)
[INFO ] 2018-10-02 22:29:58,941 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:45415 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 45415, None)
[INFO ] 2018-10-02 22:29:58,943 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 45415, None)
[INFO ] 2018-10-02 22:29:58,943 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 45415, None)
[INFO ] 2018-10-02 22:29:59,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 22:29:59,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 22:29:59,762 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 22:29:59,767 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is : 192.168.0.100:9092
[INFO ] 2018-10-02 22:29:59,768 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
topic:test-flume-topic
[WARN ] 2018-10-02 22:29:59,773 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[INFO ] 2018-10-02 22:29:59,827 utils.CommonUtil$.getCheckpointDir(CommonUtil.scala:54)
checkpointDir is : /home/feng/software/code/bigdata/spark-warehouse
[WARN ] 2018-10-02 22:29:59,980 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-02 22:29:59,981 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-02 22:29:59,981 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-02 22:29:59,982 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-02 22:29:59,986 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-02 22:30:00,183 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-02 22:30:00,200 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:30:00,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:30:00,201 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:30:00,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:30:00,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-02 22:30:00,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:30:00,202 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:30:00,203 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:30:00,203 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:30:00,203 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@26bbe604
[INFO ] 2018-10-02 22:30:00,203 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Memory Serialized 1x Replicated
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:30:00,212 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:30:00,213 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-02 22:30:00,264 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:30:00,340 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:30:00,367 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 22:30:00,367 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 22:30:00,504 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-02 22:30:00,505 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:30:00,506 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:30:00,544 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 19
[INFO ] 2018-10-02 22:30:00,546 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:30:00,562 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538490612000
[INFO ] 2018-10-02 22:30:00,562 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538490612000 ms
[INFO ] 2018-10-02 22:30:00,563 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-02 22:30:00,571 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-02 22:30:12,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490612000 ms
[INFO ] 2018-10-02 22:30:12,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490612000 ms
[INFO ] 2018-10-02 22:30:12,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490612000 ms
[INFO ] 2018-10-02 22:30:12,090 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490612000 ms.0 from job set of time 1538490612000 ms
[INFO ] 2018-10-02 22:30:12,103 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490612000 ms
[INFO ] 2018-10-02 22:30:12,109 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490612000 ms to writer queue
[INFO ] 2018-10-02 22:30:12,110 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490612000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490612000'
[INFO ] 2018-10-02 22:30:12,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:30:12,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000.bk
[INFO ] 2018-10-02 22:30:12,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:30:12,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:30:12,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490612000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490612000', took 4792 bytes and 47 ms
[INFO ] 2018-10-02 22:30:12,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:30:12,162 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:30:12,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:30:12,199 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,333 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2029.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,335 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:45415 (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,337 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:30:12,354 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:30:12,355 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 22:30:12,390 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:30:12,401 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 22:30:12,464 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 56 -> 63
[INFO ] 2018-10-02 22:30:12,466 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-02 22:30:12,467 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-02 22:30:12,468 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 22:30:12,471 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 22:30:12,473 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 22:30:12,473 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 22:30:12,482 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 56
[INFO ] 2018-10-02 22:30:12,590 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-02 22:30:12,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng/资源/bigdata/test/64565465 
[INFO ] 2018-10-02 22:30:12,617 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng/资源/bigdata/test/64565465 
[INFO ] 2018-10-02 22:30:12,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng/资源/bigdata/test/64565465 
[INFO ] 2018-10-02 22:30:12,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileP/media/feng/资源/bigdata/test/64565465 ajkhdkfj
[INFO ] 2018-10-02 22:30:12,624 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_2_0 stored as bytes in memory (estimated size 22.0 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,624 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_2_0 in memory on 192.168.0.100:45415 (size: 22.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
1 block locks were not released by TID = 0:
[rdd_2_0]
[INFO ] 2018-10-02 22:30:12,699 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 5960 bytes result sent to driver
[INFO ] 2018-10-02 22:30:12,717 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 341 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:30:12,718 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:30:12,722 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:89) finished in 0.355 s
[INFO ] 2018-10-02 22:30:12,725 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:89, took 0.583231 s
[INFO ] 2018-10-02 22:30:12,738 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:90
[INFO ] 2018-10-02 22:30:12,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:90) with 1 output partitions
[INFO ] 2018-10-02 22:30:12,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:90)
[INFO ] 2018-10-02 22:30:12,739 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:30:12,743 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:30:12,744 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:90), which has no missing parents
[INFO ] 2018-10-02 22:30:12,747 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,752 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2043.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,753 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:45415 (size: 2043.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:12,753 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:30:12,754 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:90) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:30:12,755 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-02 22:30:12,758 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:30:12,758 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-02 22:30:12,766 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Found block rdd_2_0 locally
[INFO ] 2018-10-02 22:30:12,774 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 23384 bytes result sent to driver
[INFO ] 2018-10-02 22:30:12,780 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 23 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:30:12,781 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:30:12,782 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:90) finished in 0.025 s
[INFO ] 2018-10-02 22:30:12,782 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:90, took 0.044281 s
[INFO ] 2018-10-02 22:30:12,783 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
save reviewList to hbase:3
[INFO ] 2018-10-02 22:30:12,784 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
tableName:test-flume-topic
[INFO ] 2018-10-02 22:30:12,784 utils.CommonUtil$.getWriteHbaseConfig(CommonUtil.scala:29)
zookeeperQuorum is : localhost
[WARN ] 2018-10-02 22:30:12,846 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-10-02 22:30:12,890 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: saveAsHadoopDataset at StoreMovieEssay.scala:94
[INFO ] 2018-10-02 22:30:12,891 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94) with 1 output partitions
[INFO ] 2018-10-02 22:30:12,891 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94)
[INFO ] 2018-10-02 22:30:12,891 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:30:12,891 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:30:12,891 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:94), which has no missing parents
[INFO ] 2018-10-02 22:30:12,909 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:30:12,914 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.5 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:30:12,915 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:45415 (size: 28.5 KB, free: 1406.3 MB)
[INFO ] 2018-10-02 22:30:12,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:30:12,917 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:94) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:30:12,917 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-02 22:30:12,924 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 27331 bytes)
[INFO ] 2018-10-02 22:30:12,925 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-02 22:30:13,057 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:122)
Process identifier=hconnection-0x5908aa01 connecting to ZooKeeper ensemble=localhost:2181
[WARN ] 2018-10-02 22:30:13,095 org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)
Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
[INFO ] 2018-10-02 22:30:13,115 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:376)
Scheduled snapshot period at 10 second(s).
[INFO ] 2018-10-02 22:30:13,116 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)
HBase metrics system started
[INFO ] 2018-10-02 22:30:13,132 org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:65)
Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
[INFO ] 2018-10-02 22:30:13,143 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[INFO ] 2018-10-02 22:30:13,143 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:host.name=feng
[INFO ] 2018-10-02 22:30:13,143 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.version=1.8.0_181
[INFO ] 2018-10-02 22:30:13,143 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.vendor=Oracle Corporation
[INFO ] 2018-10-02 22:30:13,143 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
[INFO ] 2018-10-02 22:30:13,144 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.class.path=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/home/feng/software/scala-2.11.11/lib/akka-actor_2.11-2.3.16.jar:/home/feng/software/scala-2.11.11/lib/config-1.2.1.jar:/home/feng/software/scala-2.11.11/lib/jline-2.14.3.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-2.11.0.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-migration_2.11-1.1.0.jar:/home/feng/software/scala-2.11.11/lib/scala-compiler.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-library_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-plugin_2.11.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-library.jar:/home/feng/software/scala-2.11.11/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/scala-2.11.11/lib/scala-reflect.jar:/home/feng/software/scala-2.11.11/lib/scala-swing_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-xml_2.11-1.0.5.jar:/home/feng/software/scala-2.11.11/lib/scalap-2.11.11.jar:/home/feng/software/code/bigdata/target/classes:/home/feng/software/code/jars/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/feng/software/code/jars/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/feng/software/code/jars/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/feng/software/code/jars/org/tukaani/xz/1.0/xz-1.0.jar:/home/feng/software/code/jars/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/feng/software/code/jars/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/home/feng/software/code/jars/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/feng/software/code/jars/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/feng/software/code/jars/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/feng/software/code/jars/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/home/feng/software/code/jars/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/feng/software/code/jars/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/feng/software/code/jars/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/feng/software/code/jars/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/feng/software/code/jars/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/feng/software/code/jars/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/home/feng/software/code/jars/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/home/feng/software/code/jars/net/java/dev/jets3t/jets3t/0.9.3/jets3t-0.9.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/home/feng/software/code/jars/commons-codec/commons-codec/1.8/commons-codec-1.8.jar:/home/feng/software/code/jars/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/feng/software/code/jars/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar:/home/feng/software/code/jars/javax/mail/mail/1.4.7/mail-1.4.7.jar:/home/feng/software/code/jars/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar:/home/feng/software/code/jars/com/jamesmurty/utils/java-xmlbuilder/1.0/java-xmlbuilder-1.0.jar:/home/feng/software/code/jars/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/feng/software/code/jars/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/feng/software/code/jars/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/feng/software/code/jars/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/feng/software/code/jars/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/feng/software/code/jars/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/feng/software/code/jars/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/feng/software/code/jars/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/feng/software/code/jars/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/feng/software/code/jars/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/feng/software/code/jars/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/feng/software/code/jars/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/feng/software/code/jars/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/feng/software/code/jars/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/feng/software/code/jars/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/feng/software/code/jars/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/feng/software/code/jars/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/feng/software/code/jars/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/feng/software/code/jars/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-xml_2.11/1.0.1/scala-xml_2.11-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/feng/software/code/jars/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/feng/software/code/jars/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/feng/software/code/jars/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/feng/software/code/jars/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/feng/software/code/jars/io/netty/netty-all/4.0.43.Final/netty-all-4.0.43.Final.jar:/home/feng/software/code/jars/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/feng/software/code/jars/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/home/feng/software/code/jars/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/home/feng/software/code/jars/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/feng/software/code/jars/oro/oro/2.0.8/oro-2.0.8.jar:/home/feng/software/code/jars/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/feng/software/code/jars/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/home/feng/software/code/jars/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/feng/software/code/jars/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/home/feng/software/code/jars/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/home/feng/software/code/jars/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/home/feng/software/code/jars/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/home/feng/software/code/jars/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/home/feng/software/code/jars/com/101tec/zkclient/0.8/zkclient-0.8.jar:/home/feng/software/code/jars/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/code/jars/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-spark/4.14.0-HBase-1.4/phoenix-spark-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-core/4.14.0-HBase-1.4/phoenix-core-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-api/0.14.0-incubating/tephra-api-0.14.0-incubating.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-core/0.14.0-incubating/tephra-core-0.14.0-incubating.jar:/home/feng/software/code/jars/com/google/inject/guice/3.0/guice-3.0.jar:/home/feng/software/code/jars/javax/inject/javax.inject/1/javax.inject-1.jar:/home/feng/software/code/jars/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/feng/software/code/jars/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/feng/software/code/jars/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/feng/software/code/jars/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/feng/software/code/jars/org/apache/twill/twill-common/0.8.0/twill-common-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-core/0.8.0/twill-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-api/0.8.0/twill-api-0.8.0.jar:/home/feng/software/code/jars/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-api/0.8.0/twill-discovery-api-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-core/0.8.0/twill-discovery-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-zookeeper/0.8.0/twill-zookeeper-0.8.0.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-hbase-compat-1.4/0.14.0-incubating/tephra-hbase-compat-1.4-0.14.0-incubating.jar:/home/feng/software/code/jars/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/feng/software/code/jars/jline/jline/2.11/jline-2.11.jar:/home/feng/software/code/jars/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/feng/software/code/jars/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/feng/software/code/jars/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/feng/software/code/jars/junit/junit/4.12/junit-4.12.jar:/home/feng/software/code/jars/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/feng/software/code/jars/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/feng/software/code/jars/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/feng/software/code/jars/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/feng/software/code/jars/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/feng/software/code/jars/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/feng/software/code/jars/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-annotations/1.4.0/hbase-annotations-1.4.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-protocol/1.4.0/hbase-protocol-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-server/1.4.0/hbase-server-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-procedure/1.4.0/hbase-procedure-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0-tests.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-prefix-tree/1.4.0/hbase-prefix-tree-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics-api/1.4.0/hbase-metrics-api-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics/1.4.0/hbase-metrics-1.4.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/feng/software/code/jars/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/feng/software/code/jars/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/feng/software/code/jars/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/feng/software/code/jars/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop-compat/1.4.0/hbase-hadoop-compat-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop2-compat/1.4.0/hbase-hadoop2-compat-1.4.0.jar:/home/feng/software/code/jars/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/feng/software/code/jars/com/salesforce/i18n/i18n-util/1.0.4/i18n-util-1.0.4.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j/60.2/icu4j-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-localespi/60.2/icu4j-localespi-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-charset/60.2/icu4j-charset-60.2.jar:/home/feng/software/code/jars/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-common/2.7.5/hadoop-common-2.7.5.jar:/home/feng/software/code/jars/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/feng/software/code/jars/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/feng/software/code/jars/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/feng/software/code/jars/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/feng/software/code/jars/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/feng/software/code/jars/asm/asm/3.1/asm-3.1.jar:/home/feng/software/code/jars/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/feng/software/code/jars/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/feng/software/code/jars/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/feng/software/code/jars/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/feng/software/code/jars/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-auth/2.7.5/hadoop-auth-2.7.5.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/feng/software/code/jars/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/feng/software/code/jars/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/feng/software/code/jars/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-client/1.4.0/hbase-client-1.4.0.jar:/home/feng/software/code/jars/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/feng/software/code/jars/io/debezium/debezium-core/0.8.1.Final/debezium-core-0.8.1.Final.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar:/home/feng/software/code/jars/com/alibaba/fastjson/1.2.47/fastjson-1.2.47.jar:/home/feng/software/code/jars/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/feng/software/idea/idea-IC-181.5281.24/lib/idea_rt.jar
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.io.tmpdir=/tmp
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.compiler=<NA>
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.name=Linux
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.arch=amd64
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.version=4.13.0-36-generic
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.name=feng
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.home=/home/feng
[INFO ] 2018-10-02 22:30:13,145 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.dir=/home/feng/software/code/bigdata
[INFO ] 2018-10-02 22:30:13,146 org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@68ce09
[INFO ] 2018-10-02 22:30:13,172 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO ] 2018-10-02 22:30:13,177 org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:852)
Socket connection established to localhost/127.0.0.1:2181, initiating session
[INFO ] 2018-10-02 22:30:13,199 org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1235)
Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1662fedb0b60057, negotiated timeout = 40000
[ERROR] 2018-10-02 22:30:13,597 org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:506)
Failed to get region location 
org.apache.hadoop.hbase.TableNotFoundException: Table 'test-flume-topic' was not found, got: test.
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1345)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-02 22:30:13,618 org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:506)
Failed to get region location 
org.apache.hadoop.hbase.TableNotFoundException: Table 'test-flume-topic' was not found, got: test.
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1345)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-02 22:30:13,631 org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:506)
Failed to get region location 
org.apache.hadoop.hbase.TableNotFoundException: Table 'test-flume-topic' was not found, got: test.
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1345)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1221)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:496)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:436)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:246)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-02 22:30:13,635 org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.closeZooKeeperWatcher(ConnectionManager.java:1757)
Closing zookeeper sessionid=0x1662fedb0b60057
[INFO ] 2018-10-02 22:30:13,648 org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684)
Session: 0x1662fedb0b60057 closed
[INFO ] 2018-10-02 22:30:13,648 org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
EventThread shut down
[ERROR] 2018-10-02 22:30:13,651 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Exception in task 0.0 in stage 2.0 (TID 2)
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 3 actions: Table 'test-flume-topic' was not found, got: test.: 3 times, servers with issues: null
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:297)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2300(AsyncProcess.java:273)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1929)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:253)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] 2018-10-02 22:30:13,806 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 3 actions: Table 'test-flume-topic' was not found, got: test.: 3 times, servers with issues: null
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:297)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2300(AsyncProcess.java:273)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1929)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:253)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] 2018-10-02 22:30:13,807 org.apache.spark.internal.Logging$class.logError(Logging.scala:70)
Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] 2018-10-02 22:30:13,816 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:30:13,819 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cancelling stage 2
[INFO ] 2018-10-02 22:30:13,821 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94) failed in 0.903 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 3 actions: Table 'test-flume-topic' was not found, got: test.: 3 times, servers with issues: null
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:297)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2300(AsyncProcess.java:273)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1929)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:253)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] 2018-10-02 22:30:13,822 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 failed: saveAsHadoopDataset at StoreMovieEssay.scala:94, took 0.932004 s
[INFO ] 2018-10-02 22:30:13,831 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490612000 ms.0 from job set of time 1538490612000 ms
[ERROR] 2018-10-02 22:30:13,837 org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
Error running job streaming job 1538490612000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 3 actions: Table 'test-flume-topic' was not found, got: test.: 3 times, servers with issues: null
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:297)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2300(AsyncProcess.java:273)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1929)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:253)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.close(BufferedMutatorImpl.java:185)
	at org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:75)
	at org.apache.spark.internal.io.SparkHadoopWriter.close(SparkHadoopWriter.scala:101)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$5.apply$mcV$sp(PairRDDFunctions.scala:1145)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1393)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2075)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1151)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)
	at spark.dataProcess.StoreMovieEssay$$anonfun$saveIntoHbase$1.apply(StoreMovieEssay.scala:94)
	at spark.dataProcess.StoreMovieEssay$$anonfun$saveIntoHbase$1.apply(StoreMovieEssay.scala:88)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] 2018-10-02 22:30:16,872 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-02 22:30:16,874 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538490616874.
[WARN ] 2018-10-02 22:30:16,875 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-02 22:30:16,876 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-02 22:30:16,876 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-02 22:30:16,877 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-02 22:30:16,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-02 22:30:16,878 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:30:16,879 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:30:24,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490624000 ms.0 from job set of time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490624000 ms to writer queue
[INFO ] 2018-10-02 22:30:24,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490624000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000'
[INFO ] 2018-10-02 22:30:24,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:30:24,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:30:24,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:30:24,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:30:24,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:30:24,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:30:24,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:30:24,036 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-02 22:30:24,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407520000
[INFO ] 2018-10-02 22:30:24,039 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:45415 (size: 2028.0 B, free: 1406.3 MB)
[INFO ] 2018-10-02 22:30:24,041 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:30:24,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:30:24,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490624000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000', took 4835 bytes and 24 ms
[INFO ] 2018-10-02 22:30:24,042 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-02 22:30:24,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:30:24,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-02 22:30:24,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:30:24,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_8_0 stored as bytes in memory (estimated size 0.0 B, free 1406.3 MB)
[INFO ] 2018-10-02 22:30:24,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_8_0 in memory on 192.168.0.100:45415 (size: 0.0 B, free: 1406.3 MB)
[INFO ] 2018-10-02 22:30:24,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 1399 bytes result sent to driver
[INFO ] 2018-10-02 22:30:24,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 21 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:30:24,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:30:24,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:89) finished in 0.022 s
[INFO ] 2018-10-02 22:30:24,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:89, took 0.043608 s
[INFO ] 2018-10-02 22:30:24,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty8
[INFO ] 2018-10-02 22:30:24,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490624000 ms.0 from job set of time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.067 s for time 1538490624000 ms (execution: 0.051 s)
[INFO ] 2018-10-02 22:30:24,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-02 22:30:24,076 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-02 22:30:24,081 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-02 22:30:24,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-02 22:30:24,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-02 22:30:24,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-02 22:30:24,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490624000 ms to writer queue
[INFO ] 2018-10-02 22:30:24,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490624000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000'
[INFO ] 2018-10-02 22:30:24,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000.bk
[INFO ] 2018-10-02 22:30:24,099 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490624000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000', took 4797 bytes and 14 ms
[INFO ] 2018-10-02 22:30:24,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490624000 ms
[INFO ] 2018-10-02 22:30:24,104 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:30:24,106 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490624106 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:30:24,107 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:30:24,107 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 22:30:36,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538490636000
[INFO ] 2018-10-02 22:30:36,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490636000 ms.0 from job set of time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,103 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,103 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,107 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:30:36,109 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:30:36,109 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:30:36,109 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:30:36,116 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:30:36,116 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,116 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:30:36,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-02 22:30:36,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:30:36,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490636000 ms to writer queue
[INFO ] 2018-10-02 22:30:36,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490636000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000'
[INFO ] 2018-10-02 22:30:36,120 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_2_piece0 on 192.168.0.100:45415 in memory (size: 28.5 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,121 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,123 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:45415 in memory (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_3_piece0 on 192.168.0.100:45415 in memory (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,131 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:45415 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:30:36,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:30:36,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_1_piece0 on 192.168.0.100:45415 in memory (size: 2043.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-02 22:30:36,134 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:30:36,134 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-02 22:30:36,140 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:30:36,143 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_11_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_11_0 in memory on 192.168.0.100:45415 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:30:36,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 1399 bytes result sent to driver
[INFO ] 2018-10-02 22:30:36,150 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538407530000
[INFO ] 2018-10-02 22:30:36,150 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490636000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000', took 4835 bytes and 33 ms
[INFO ] 2018-10-02 22:30:36,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 17 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:30:36,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:30:36,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:89) finished in 0.017 s
[INFO ] 2018-10-02 22:30:36,152 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:89, took 0.044422 s
[INFO ] 2018-10-02 22:30:36,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty11
[INFO ] 2018-10-02 22:30:36,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490636000 ms.0 from job set of time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.153 s for time 1538490636000 ms (execution: 0.137 s)
[INFO ] 2018-10-02 22:30:36,153 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-02 22:30:36,154 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-02 22:30:36,154 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-02 22:30:36,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-02 22:30:36,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-02 22:30:36,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-02 22:30:36,155 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,156 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490636000 ms to writer queue
[INFO ] 2018-10-02 22:30:36,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490636000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000'
[INFO ] 2018-10-02 22:30:36,172 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000.bk
[INFO ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490636000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000', took 4799 bytes and 16 ms
[INFO ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490636000 ms
[INFO ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490636173 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:30:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538490612000 ms
[INFO ] 2018-10-02 22:30:36,218 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:30:36,219 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-02 22:30:36,219 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-02 22:30:36,221 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-02 22:30:36,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-02 22:30:36,228 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-02 22:30:36,233 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:30:36,240 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 22:30:36,251 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 22:30:36,251 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 22:30:36,253 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 22:30:36,255 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 22:30:36,256 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 22:30:36,256 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 22:30:36,257 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-33ed9ca5-65d4-4462-9014-fbca252bdce6
[INFO ] 2018-10-02 22:32:06,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 22:32:06,550 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 22:32:06,668 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 22:32:06,668 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 22:32:06,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-02 22:32:06,710 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 22:32:06,711 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 22:32:06,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 22:32:06,712 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 22:32:06,713 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 22:32:06,942 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 36097.
[INFO ] 2018-10-02 22:32:06,960 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 22:32:06,974 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 22:32:06,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 22:32:06,976 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 22:32:06,988 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-6cc466c0-18fc-4f56-9d06-30c5b366181d
[INFO ] 2018-10-02 22:32:07,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 22:32:07,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 22:32:07,231 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 22:32:07,293 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:32:07,380 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 22:32:07,398 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40145.
[INFO ] 2018-10-02 22:32:07,399 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:40145
[INFO ] 2018-10-02 22:32:07,401 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 22:32:07,402 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 40145, None)
[INFO ] 2018-10-02 22:32:07,405 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:40145 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 40145, None)
[INFO ] 2018-10-02 22:32:07,406 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 40145, None)
[INFO ] 2018-10-02 22:32:07,407 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 40145, None)
[INFO ] 2018-10-02 22:32:07,608 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 22:32:07,609 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 22:32:08,151 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 22:32:08,156 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is : 192.168.0.100:9092
[INFO ] 2018-10-02 22:32:08,157 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
topic:test-flume-topic
[WARN ] 2018-10-02 22:32:08,163 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[INFO ] 2018-10-02 22:32:08,224 utils.CommonUtil$.getCheckpointDir(CommonUtil.scala:54)
checkpointDir is : /home/feng/software/code/bigdata/spark-warehouse
[WARN ] 2018-10-02 22:32:08,397 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-02 22:32:08,398 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-02 22:32:08,398 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-02 22:32:08,399 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-02 22:32:08,403 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-02 22:32:08,640 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-02 22:32:08,658 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:32:08,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:32:08,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:32:08,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:32:08,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-02 22:32:08,662 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:32:08,662 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:32:08,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:32:08,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:32:08,663 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@26bbe604
[INFO ] 2018-10-02 22:32:08,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:32:08,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Memory Serialized 1x Replicated
[INFO ] 2018-10-02 22:32:08,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:32:08,666 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:32:08,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-02 22:32:08,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:32:08,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:32:08,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:32:08,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:32:08,668 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-02 22:32:08,730 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:32:08,791 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:32:08,812 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 22:32:08,813 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 22:32:08,951 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-02 22:32:08,951 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:32:08,952 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:32:08,967 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 21
[INFO ] 2018-10-02 22:32:08,968 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:32:08,978 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538490732000
[INFO ] 2018-10-02 22:32:08,979 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538490732000 ms
[INFO ] 2018-10-02 22:32:08,980 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-02 22:32:08,989 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-02 22:32:12,568 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:12,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:12,569 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:12,570 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490732000 ms.0 from job set of time 1538490732000 ms
[INFO ] 2018-10-02 22:32:12,574 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:12,580 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490732000 ms to writer queue
[INFO ] 2018-10-02 22:32:12,584 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490732000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000'
[INFO ] 2018-10-02 22:32:12,614 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:32:12,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410490000
[INFO ] 2018-10-02 22:32:12,623 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490732000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000', took 4790 bytes and 39 ms
[INFO ] 2018-10-02 22:32:12,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:32:12,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:32:12,626 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:32:12,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:32:12,632 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:32:12,670 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:12,780 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2029.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:12,782 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:40145 (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:12,785 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:32:12,803 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:32:12,804 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 22:32:12,833 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:32:12,839 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 22:32:12,916 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:32:12,930 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_2_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:12,931 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_2_0 in memory on 192.168.0.100:40145 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:12,980 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 1442 bytes result sent to driver
[INFO ] 2018-10-02 22:32:13,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 176 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:32:13,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:32:13,006 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:89) finished in 0.190 s
[INFO ] 2018-10-02 22:32:13,010 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:89, took 0.395312 s
[INFO ] 2018-10-02 22:32:13,012 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty2
[INFO ] 2018-10-02 22:32:13,014 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490732000 ms.0 from job set of time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 1.013 s for time 1538490732000 ms (execution: 0.444 s)
[INFO ] 2018-10-02 22:32:13,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490732000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000'
[INFO ] 2018-10-02 22:32:13,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490732000 ms to writer queue
[INFO ] 2018-10-02 22:32:13,064 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000.bk
[INFO ] 2018-10-02 22:32:13,065 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490732000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000', took 4787 bytes and 44 ms
[INFO ] 2018-10-02 22:32:13,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490732000 ms
[INFO ] 2018-10-02 22:32:13,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-02 22:32:13,082 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 1 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538490720000: file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata/log-1538410492072-1538410552072
[INFO ] 2018-10-02 22:32:13,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 22:32:13,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538490720000
[INFO ] 2018-10-02 22:32:23,306 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-02 22:32:23,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538490743308.
[WARN ] 2018-10-02 22:32:23,309 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-02 22:32:23,309 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-02 22:32:23,310 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-02 22:32:23,311 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-02 22:32:23,311 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-02 22:32:23,312 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:32:23,312 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:32:24,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,018 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490744000 ms.0 from job set of time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,019 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490744000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490744000'
[INFO ] 2018-10-02 22:32:24,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490744000 ms to writer queue
[INFO ] 2018-10-02 22:32:24,025 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:32:24,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:32:24,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:32:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:32:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:32:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:32:24,037 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:24,051 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:24,052 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:40145 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:24,053 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410500000
[INFO ] 2018-10-02 22:32:24,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490744000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490744000', took 4825 bytes and 35 ms
[INFO ] 2018-10-02 22:32:24,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:32:24,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:32:24,056 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-02 22:32:24,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:32:24,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-02 22:32:24,062 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:32:24,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_5_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:24,066 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_5_0 in memory on 192.168.0.100:40145 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:24,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 1442 bytes result sent to driver
[INFO ] 2018-10-02 22:32:24,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 29 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:32:24,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:32:24,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (isEmpty at StoreMovieEssay.scala:89) finished in 0.031 s
[INFO ] 2018-10-02 22:32:24,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: isEmpty at StoreMovieEssay.scala:89, took 0.061851 s
[INFO ] 2018-10-02 22:32:24,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty5
[INFO ] 2018-10-02 22:32:24,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490744000 ms.0 from job set of time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.088 s for time 1538490744000 ms (execution: 0.070 s)
[INFO ] 2018-10-02 22:32:24,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-02 22:32:24,093 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-02 22:32:24,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-02 22:32:24,099 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-02 22:32:24,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-02 22:32:24,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-02 22:32:24,100 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,101 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490744000 ms to writer queue
[INFO ] 2018-10-02 22:32:24,102 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490744000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490744000'
[INFO ] 2018-10-02 22:32:24,116 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000.bk
[INFO ] 2018-10-02 22:32:24,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490744000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490744000', took 4787 bytes and 15 ms
[INFO ] 2018-10-02 22:32:24,117 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490744000 ms
[INFO ] 2018-10-02 22:32:24,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:32:24,119 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490744118 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:32:24,122 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:32:24,122 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 22:32:36,002 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538490756000
[INFO ] 2018-10-02 22:32:36,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490756000 ms.0 from job set of time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,028 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:32:36,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-02 22:32:36,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:32:36,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,029 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:32:36,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:32:36,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:32:36,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,030 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:32:36,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490756000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490756000'
[INFO ] 2018-10-02 22:32:36,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:32:36,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490756000 ms to writer queue
[INFO ] 2018-10-02 22:32:36,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:36,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:36,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:40145 (size: 2028.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:36,044 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:32:36,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:32:36,045 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-02 22:32:36,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:32:36,046 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-02 22:32:36,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538410510000
[INFO ] 2018-10-02 22:32:36,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490756000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490756000', took 4823 bytes and 18 ms
[INFO ] 2018-10-02 22:32:36,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 63 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:32:36,054 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_8_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:32:36,055 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_8_0 in memory on 192.168.0.100:40145 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:32:36,063 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 1399 bytes result sent to driver
[INFO ] 2018-10-02 22:32:36,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:32:36,067 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:32:36,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (isEmpty at StoreMovieEssay.scala:89) finished in 0.022 s
[INFO ] 2018-10-02 22:32:36,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: isEmpty at StoreMovieEssay.scala:89, took 0.039886 s
[INFO ] 2018-10-02 22:32:36,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty8
[INFO ] 2018-10-02 22:32:36,069 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490756000 ms.0 from job set of time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5 from persistence list
[INFO ] 2018-10-02 22:32:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.069 s for time 1538490756000 ms (execution: 0.045 s)
[INFO ] 2018-10-02 22:32:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 5
[INFO ] 2018-10-02 22:32:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4 from persistence list
[INFO ] 2018-10-02 22:32:36,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 4
[INFO ] 2018-10-02 22:32:36,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3 from persistence list
[INFO ] 2018-10-02 22:32:36,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 3
[INFO ] 2018-10-02 22:32:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490756000 ms to writer queue
[INFO ] 2018-10-02 22:32:36,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490756000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490756000'
[INFO ] 2018-10-02 22:32:36,083 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490612000
[INFO ] 2018-10-02 22:32:36,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490756000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490756000', took 4787 bytes and 11 ms
[INFO ] 2018-10-02 22:32:36,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490756000 ms
[INFO ] 2018-10-02 22:32:36,084 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:32:36,084 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490756084 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:32:36,085 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:32:36,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538490732000 ms
[INFO ] 2018-10-02 22:32:36,130 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:32:36,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-02 22:32:36,133 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-02 22:32:36,135 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-02 22:32:36,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-02 22:32:36,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-02 22:32:36,149 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:32:36,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 22:32:36,170 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 22:32:36,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 22:32:36,171 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 22:32:36,173 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 22:32:36,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 22:32:36,174 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 22:32:36,175 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-8c378e98-f8cd-4a6c-9853-39922b32e607
[INFO ] 2018-10-02 22:33:00,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running Spark version 2.2.0
[WARN ] 2018-10-02 22:33:00,756 org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN ] 2018-10-02 22:33:00,872 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Your hostname, feng resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface enp2s0)
[WARN ] 2018-10-02 22:33:00,872 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Set SPARK_LOCAL_IP if you need to bind to another address
[INFO ] 2018-10-02 22:33:00,907 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted application: StoreMovieEssay
[INFO ] 2018-10-02 22:33:00,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls to: feng
[INFO ] 2018-10-02 22:33:00,920 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls to: feng
[INFO ] 2018-10-02 22:33:00,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing view acls groups to: 
[INFO ] 2018-10-02 22:33:00,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Changing modify acls groups to: 
[INFO ] 2018-10-02 22:33:00,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(feng); groups with view permissions: Set(); users  with modify permissions: Set(feng); groups with modify permissions: Set()
[INFO ] 2018-10-02 22:33:01,190 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'sparkDriver' on port 34519.
[INFO ] 2018-10-02 22:33:01,217 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering MapOutputTracker
[INFO ] 2018-10-02 22:33:01,231 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManagerMaster
[INFO ] 2018-10-02 22:33:01,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] 2018-10-02 22:33:01,234 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMasterEndpoint up
[INFO ] 2018-10-02 22:33:01,241 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created local directory at /tmp/blockmgr-09611697-2e4e-4bcc-812c-a295ed87f508
[INFO ] 2018-10-02 22:33:01,296 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore started with capacity 1406.4 MB
[INFO ] 2018-10-02 22:33:01,355 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering OutputCommitCoordinator
[INFO ] 2018-10-02 22:33:01,560 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'SparkUI' on port 4040.
[INFO ] 2018-10-02 22:33:01,631 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Bound SparkUI to 0.0.0.0, and started at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:33:01,719 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting executor ID driver on host localhost
[INFO ] 2018-10-02 22:33:01,747 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44393.
[INFO ] 2018-10-02 22:33:01,748 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Server created on 192.168.0.100:44393
[INFO ] 2018-10-02 22:33:01,749 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] 2018-10-02 22:33:01,750 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering BlockManager BlockManagerId(driver, 192.168.0.100, 44393, None)
[INFO ] 2018-10-02 22:33:01,753 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registering block manager 192.168.0.100:44393 with 1406.4 MB RAM, BlockManagerId(driver, 192.168.0.100, 44393, None)
[INFO ] 2018-10-02 22:33:01,755 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered BlockManager BlockManagerId(driver, 192.168.0.100, 44393, None)
[INFO ] 2018-10-02 22:33:01,755 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized BlockManager: BlockManagerId(driver, 192.168.0.100, 44393, None)
[INFO ] 2018-10-02 22:33:01,967 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/feng/software/code/bigdata/spark-warehouse/').
[INFO ] 2018-10-02 22:33:01,968 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Warehouse path is 'file:/home/feng/software/code/bigdata/spark-warehouse/'.
[INFO ] 2018-10-02 22:33:02,498 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Registered StateStoreCoordinator endpoint
[INFO ] 2018-10-02 22:33:02,502 utils.CommonUtil$.getKafkaServers(CommonUtil.scala:45)
kafkaServers is : 192.168.0.100:9092
[INFO ] 2018-10-02 22:33:02,504 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
topic:test-flume-topic
[WARN ] 2018-10-02 22:33:02,508 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
[INFO ] 2018-10-02 22:33:02,572 utils.CommonUtil$.getCheckpointDir(CommonUtil.scala:54)
checkpointDir is : /home/feng/software/code/bigdata/spark-warehouse
[WARN ] 2018-10-02 22:33:02,713 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding enable.auto.commit to false for executor
[WARN ] 2018-10-02 22:33:02,714 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding auto.offset.reset to none for executor
[WARN ] 2018-10-02 22:33:02,714 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding executor group.id to spark-executor-StoreMovieEssayGroup
[WARN ] 2018-10-02 22:33:02,715 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO ] 2018-10-02 22:33:02,720 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
[INFO ] 2018-10-02 22:33:02,909 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Recovered 1 write ahead log files from file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata
[INFO ] 2018-10-02 22:33:02,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:33:02,919 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:33:02,920 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:33:02,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:33:02,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2b6fcb9f
[INFO ] 2018-10-02 22:33:02,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:33:02,921 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@26bbe604
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Memory Serialized 1x Replicated
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:33:02,922 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.FilteredDStream@59bbe88a
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Slide time = 12000 ms
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Storage level = Serialized 1x Replicated
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint interval = null
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Remember interval = 12000 ms
[INFO ] 2018-10-02 22:33:02,923 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e869098
[INFO ] 2018-10-02 22:33:02,978 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:33:03,068 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO ] 2018-10-02 22:33:03,090 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 22:33:03,091 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 22:33:03,226 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group StoreMovieEssayGroup.
[INFO ] 2018-10-02 22:33:03,226 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:292)
Revoking previously assigned partitions [] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:33:03,227 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:326)
(Re-)joining group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:33:03,237 org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:434)
Successfully joined group StoreMovieEssayGroup with generation 23
[INFO ] 2018-10-02 22:33:03,238 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:231)
Setting newly assigned partitions [test-flume-topic-0] for group StoreMovieEssayGroup
[INFO ] 2018-10-02 22:33:03,247 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started timer for JobGenerator at time 1538490792000
[INFO ] 2018-10-02 22:33:03,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobGenerator at 1538490792000 ms
[INFO ] 2018-10-02 22:33:03,248 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Started JobScheduler
[INFO ] 2018-10-02 22:33:03,260 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext started
[INFO ] 2018-10-02 22:33:12,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:12,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:12,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:12,088 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490792000 ms.0 from job set of time 1538490792000 ms
[INFO ] 2018-10-02 22:33:12,092 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:12,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490792000 ms to writer queue
[INFO ] 2018-10-02 22:33:12,099 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490792000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490792000'
[INFO ] 2018-10-02 22:33:12,124 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:33:12,138 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 0 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:33:12,138 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 0 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:33:12,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:33:12,142 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:33:12,144 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000.bk
[INFO ] 2018-10-02 22:33:12,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490792000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490792000', took 4789 bytes and 46 ms
[INFO ] 2018-10-02 22:33:12,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:33:12,180 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,288 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_0_piece0 stored as bytes in memory (estimated size 2029.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,289 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_0_piece0 in memory on 192.168.0.100:44393 (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,292 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:33:12,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:33:12,308 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 0.0 with 1 tasks
[INFO ] 2018-10-02 22:33:12,341 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:33:12,354 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 0.0 (TID 0)
[INFO ] 2018-10-02 22:33:12,434 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Computing topic test-flume-topic, partition 0 offsets 63 -> 70
[INFO ] 2018-10-02 22:33:12,437 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initializing cache 16 64 0.75
[INFO ] 2018-10-02 22:33:12,439 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cache miss for CacheKey(spark-executor-StoreMovieEssayGroup,test-flume-topic,0)
[INFO ] 2018-10-02 22:33:12,442 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 22:33:12,447 org.apache.kafka.common.config.AbstractConfig.logAll(AbstractConfig.java:178)
ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.0.100:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-StoreMovieEssayGroup
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO ] 2018-10-02 22:33:12,450 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:83)
Kafka version : 0.10.0.1
[INFO ] 2018-10-02 22:33:12,450 org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>(AppInfoParser.java:84)
Kafka commitId : a7a17cdec9eaa6c5
[INFO ] 2018-10-02 22:33:12,465 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Initial fetch for spark-executor-StoreMovieEssayGroup test-flume-topic 0 63
[INFO ] 2018-10-02 22:33:12,569 org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handleGroupMetadataResponse(AbstractCoordinator.java:505)
Discovered coordinator feng:9092 (id: 2147483647 rack: null) for group spark-executor-StoreMovieEssayGroup.
[INFO ] 2018-10-02 22:33:12,589 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/654562138567 
[INFO ] 2018-10-02 22:33:12,590 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/654562138567 
[INFO ] 2018-10-02 22:33:12,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/654562138567 
[INFO ] 2018-10-02 22:33:12,591 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
empty review:fileX/media/feng/资源/bigdata/test/654562138567 ajkhdkfj
[INFO ] 2018-10-02 22:33:12,593 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_2_0 stored as bytes in memory (estimated size 22.0 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,594 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_2_0 in memory on 192.168.0.100:44393 (size: 22.0 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
1 block locks were not released by TID = 0:
[rdd_2_0]
[INFO ] 2018-10-02 22:33:12,636 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0). 5921 bytes result sent to driver
[INFO ] 2018-10-02 22:33:12,651 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 0.0 (TID 0) in 321 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:33:12,653 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:33:12,656 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 0 (isEmpty at StoreMovieEssay.scala:89) finished in 0.335 s
[INFO ] 2018-10-02 22:33:12,660 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 0 finished: isEmpty at StoreMovieEssay.scala:89, took 0.534919 s
[INFO ] 2018-10-02 22:33:12,684 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: reduce at StoreMovieEssay.scala:90
[INFO ] 2018-10-02 22:33:12,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 1 (reduce at StoreMovieEssay.scala:90) with 1 output partitions
[INFO ] 2018-10-02 22:33:12,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 1 (reduce at StoreMovieEssay.scala:90)
[INFO ] 2018-10-02 22:33:12,685 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:33:12,689 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:33:12,690 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:90), which has no missing parents
[INFO ] 2018-10-02 22:33:12,692 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,695 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_0_piece0 on 192.168.0.100:44393 in memory (size: 2029.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,698 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_1_piece0 stored as bytes in memory (estimated size 2043.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,699 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_1_piece0 in memory on 192.168.0.100:44393 (size: 2043.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:12,700 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:33:12,701 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at StoreMovieEssay.scala:90) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:33:12,701 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 1.0 with 1 tasks
[INFO ] 2018-10-02 22:33:12,704 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:33:12,704 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 1.0 (TID 1)
[INFO ] 2018-10-02 22:33:12,713 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Found block rdd_2_0 locally
[INFO ] 2018-10-02 22:33:12,721 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1). 23396 bytes result sent to driver
[INFO ] 2018-10-02 22:33:12,728 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 1.0 (TID 1) in 25 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:33:12,728 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:33:12,729 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 1 (reduce at StoreMovieEssay.scala:90) finished in 0.025 s
[INFO ] 2018-10-02 22:33:12,730 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 1 finished: reduce at StoreMovieEssay.scala:90, took 0.045480 s
[INFO ] 2018-10-02 22:33:12,730 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
save reviewList to hbase:3
[INFO ] 2018-10-02 22:33:12,731 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
tableName:test
[INFO ] 2018-10-02 22:33:12,731 utils.CommonUtil$.getWriteHbaseConfig(CommonUtil.scala:29)
zookeeperQuorum is : localhost
[WARN ] 2018-10-02 22:33:12,779 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:295)
Output Path is null in setupJob()
[INFO ] 2018-10-02 22:33:12,828 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: saveAsHadoopDataset at StoreMovieEssay.scala:94
[INFO ] 2018-10-02 22:33:12,830 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94) with 1 output partitions
[INFO ] 2018-10-02 22:33:12,830 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94)
[INFO ] 2018-10-02 22:33:12,830 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:33:12,830 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:33:12,831 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:94), which has no missing parents
[INFO ] 2018-10-02 22:33:12,848 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2 stored as values in memory (estimated size 78.8 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:33:12,853 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.4 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:33:12,855 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_2_piece0 in memory on 192.168.0.100:44393 (size: 28.4 KB, free: 1406.3 MB)
[INFO ] 2018-10-02 22:33:12,857 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:33:12,860 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at StoreMovieEssay.scala:94) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:33:12,860 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 2.0 with 1 tasks
[INFO ] 2018-10-02 22:33:12,871 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 27343 bytes)
[INFO ] 2018-10-02 22:33:12,873 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 2.0 (TID 2)
[INFO ] 2018-10-02 22:33:13,036 org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:122)
Process identifier=hconnection-0x4a516677 connecting to ZooKeeper ensemble=localhost:2181
[WARN ] 2018-10-02 22:33:13,075 org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)
Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
[INFO ] 2018-10-02 22:33:13,095 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:376)
Scheduled snapshot period at 10 second(s).
[INFO ] 2018-10-02 22:33:13,095 org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)
HBase metrics system started
[INFO ] 2018-10-02 22:33:13,114 org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:65)
Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:host.name=feng
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.version=1.8.0_181
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.vendor=Oracle Corporation
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
[INFO ] 2018-10-02 22:33:13,127 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.class.path=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/home/feng/software/scala-2.11.11/lib/akka-actor_2.11-2.3.16.jar:/home/feng/software/scala-2.11.11/lib/config-1.2.1.jar:/home/feng/software/scala-2.11.11/lib/jline-2.14.3.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-2.11.0.jar:/home/feng/software/scala-2.11.11/lib/scala-actors-migration_2.11-1.1.0.jar:/home/feng/software/scala-2.11.11/lib/scala-compiler.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-library_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-continuations-plugin_2.11.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-library.jar:/home/feng/software/scala-2.11.11/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/scala-2.11.11/lib/scala-reflect.jar:/home/feng/software/scala-2.11.11/lib/scala-swing_2.11-1.0.2.jar:/home/feng/software/scala-2.11.11/lib/scala-xml_2.11-1.0.5.jar:/home/feng/software/scala-2.11.11/lib/scalap-2.11.11.jar:/home/feng/software/code/bigdata/target/classes:/home/feng/software/code/jars/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/feng/software/code/jars/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/feng/software/code/jars/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/feng/software/code/jars/org/tukaani/xz/1.0/xz-1.0.jar:/home/feng/software/code/jars/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/feng/software/code/jars/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/feng/software/code/jars/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/home/feng/software/code/jars/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/feng/software/code/jars/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/feng/software/code/jars/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/feng/software/code/jars/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/home/feng/software/code/jars/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/feng/software/code/jars/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/feng/software/code/jars/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/feng/software/code/jars/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/feng/software/code/jars/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/feng/software/code/jars/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/home/feng/software/code/jars/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/home/feng/software/code/jars/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/home/feng/software/code/jars/net/java/dev/jets3t/jets3t/0.9.3/jets3t-0.9.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/home/feng/software/code/jars/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/home/feng/software/code/jars/commons-codec/commons-codec/1.8/commons-codec-1.8.jar:/home/feng/software/code/jars/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/feng/software/code/jars/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar:/home/feng/software/code/jars/javax/mail/mail/1.4.7/mail-1.4.7.jar:/home/feng/software/code/jars/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar:/home/feng/software/code/jars/com/jamesmurty/utils/java-xmlbuilder/1.0/java-xmlbuilder-1.0.jar:/home/feng/software/code/jars/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/feng/software/code/jars/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/feng/software/code/jars/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/feng/software/code/jars/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/feng/software/code/jars/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/feng/software/code/jars/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/feng/software/code/jars/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/feng/software/code/jars/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/feng/software/code/jars/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/feng/software/code/jars/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/feng/software/code/jars/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/feng/software/code/jars/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/feng/software/code/jars/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/feng/software/code/jars/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/feng/software/code/jars/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/feng/software/code/jars/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/feng/software/code/jars/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/feng/software/code/jars/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/feng/software/code/jars/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/feng/software/code/jars/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/feng/software/code/jars/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-xml_2.11/1.0.1/scala-xml_2.11-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/feng/software/code/jars/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/feng/software/code/jars/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/feng/software/code/jars/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/feng/software/code/jars/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/feng/software/code/jars/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/feng/software/code/jars/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/feng/software/code/jars/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/feng/software/code/jars/io/netty/netty-all/4.0.43.Final/netty-all-4.0.43.Final.jar:/home/feng/software/code/jars/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/feng/software/code/jars/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/feng/software/code/jars/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/home/feng/software/code/jars/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/home/feng/software/code/jars/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/home/feng/software/code/jars/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/feng/software/code/jars/oro/oro/2.0.8/oro-2.0.8.jar:/home/feng/software/code/jars/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/feng/software/code/jars/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/home/feng/software/code/jars/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/feng/software/code/jars/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/home/feng/software/code/jars/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/home/feng/software/code/jars/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/home/feng/software/code/jars/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/home/feng/software/code/jars/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/feng/software/code/jars/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/home/feng/software/code/jars/com/101tec/zkclient/0.8/zkclient-0.8.jar:/home/feng/software/code/jars/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/feng/software/code/jars/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/feng/software/code/jars/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/home/feng/software/code/jars/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-spark/4.14.0-HBase-1.4/phoenix-spark-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/phoenix/phoenix-core/4.14.0-HBase-1.4/phoenix-core-4.14.0-HBase-1.4.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-api/0.14.0-incubating/tephra-api-0.14.0-incubating.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-core/0.14.0-incubating/tephra-core-0.14.0-incubating.jar:/home/feng/software/code/jars/com/google/inject/guice/3.0/guice-3.0.jar:/home/feng/software/code/jars/javax/inject/javax.inject/1/javax.inject-1.jar:/home/feng/software/code/jars/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/feng/software/code/jars/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/feng/software/code/jars/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/feng/software/code/jars/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/feng/software/code/jars/org/apache/twill/twill-common/0.8.0/twill-common-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-core/0.8.0/twill-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-api/0.8.0/twill-api-0.8.0.jar:/home/feng/software/code/jars/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-api/0.8.0/twill-discovery-api-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-discovery-core/0.8.0/twill-discovery-core-0.8.0.jar:/home/feng/software/code/jars/org/apache/twill/twill-zookeeper/0.8.0/twill-zookeeper-0.8.0.jar:/home/feng/software/code/jars/org/apache/tephra/tephra-hbase-compat-1.4/0.14.0-incubating/tephra-hbase-compat-1.4-0.14.0-incubating.jar:/home/feng/software/code/jars/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/feng/software/code/jars/jline/jline/2.11/jline-2.11.jar:/home/feng/software/code/jars/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/feng/software/code/jars/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/feng/software/code/jars/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/feng/software/code/jars/junit/junit/4.12/junit-4.12.jar:/home/feng/software/code/jars/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/feng/software/code/jars/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/feng/software/code/jars/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/feng/software/code/jars/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/feng/software/code/jars/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/feng/software/code/jars/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/feng/software/code/jars/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-annotations/1.4.0/hbase-annotations-1.4.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-protocol/1.4.0/hbase-protocol-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-server/1.4.0/hbase-server-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-procedure/1.4.0/hbase-procedure-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-common/1.4.0/hbase-common-1.4.0-tests.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-prefix-tree/1.4.0/hbase-prefix-tree-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics-api/1.4.0/hbase-metrics-api-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-metrics/1.4.0/hbase-metrics-1.4.0.jar:/home/feng/software/code/jars/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/feng/software/code/jars/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/feng/software/code/jars/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/feng/software/code/jars/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/feng/software/code/jars/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/feng/software/code/jars/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop-compat/1.4.0/hbase-hadoop-compat-1.4.0.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-hadoop2-compat/1.4.0/hbase-hadoop2-compat-1.4.0.jar:/home/feng/software/code/jars/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/feng/software/code/jars/com/salesforce/i18n/i18n-util/1.0.4/i18n-util-1.0.4.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j/60.2/icu4j-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-localespi/60.2/icu4j-localespi-60.2.jar:/home/feng/software/code/jars/com/ibm/icu/icu4j-charset/60.2/icu4j-charset-60.2.jar:/home/feng/software/code/jars/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-common/2.7.5/hadoop-common-2.7.5.jar:/home/feng/software/code/jars/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/feng/software/code/jars/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/feng/software/code/jars/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/feng/software/code/jars/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/feng/software/code/jars/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/feng/software/code/jars/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/feng/software/code/jars/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/feng/software/code/jars/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/feng/software/code/jars/asm/asm/3.1/asm-3.1.jar:/home/feng/software/code/jars/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/feng/software/code/jars/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/feng/software/code/jars/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/feng/software/code/jars/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/feng/software/code/jars/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/feng/software/code/jars/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/feng/software/code/jars/org/apache/hadoop/hadoop-auth/2.7.5/hadoop-auth-2.7.5.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/feng/software/code/jars/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/feng/software/code/jars/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/feng/software/code/jars/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/feng/software/code/jars/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/feng/software/code/jars/org/apache/hbase/hbase-client/1.4.0/hbase-client-1.4.0.jar:/home/feng/software/code/jars/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/feng/software/code/jars/io/debezium/debezium-core/0.8.1.Final/debezium-core-0.8.1.Final.jar:/home/feng/software/code/jars/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar:/home/feng/software/code/jars/com/alibaba/fastjson/1.2.47/fastjson-1.2.47.jar:/home/feng/software/code/jars/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/feng/software/idea/idea-IC-181.5281.24/lib/idea_rt.jar
[INFO ] 2018-10-02 22:33:13,128 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[INFO ] 2018-10-02 22:33:13,128 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.io.tmpdir=/tmp
[INFO ] 2018-10-02 22:33:13,128 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:java.compiler=<NA>
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.name=Linux
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.arch=amd64
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:os.version=4.13.0-36-generic
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.name=feng
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.home=/home/feng
[INFO ] 2018-10-02 22:33:13,129 org.apache.zookeeper.Environment.logEnv(Environment.java:100)
Client environment:user.dir=/home/feng/software/code/bigdata
[INFO ] 2018-10-02 22:33:13,130 org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@5b666263
[INFO ] 2018-10-02 22:33:13,145 org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:975)
Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO ] 2018-10-02 22:33:13,146 org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:852)
Socket connection established to localhost/127.0.0.1:2181, initiating session
[INFO ] 2018-10-02 22:33:13,169 org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1235)
Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1662fedb0b60058, negotiated timeout = 40000
[INFO ] 2018-10-02 22:33:13,588 org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.closeZooKeeperWatcher(ConnectionManager.java:1757)
Closing zookeeper sessionid=0x1662fedb0b60058
[INFO ] 2018-10-02 22:33:13,602 org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684)
Session: 0x1662fedb0b60058 closed
[INFO ] 2018-10-02 22:33:13,602 org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
EventThread shut down
[INFO ] 2018-10-02 22:33:13,604 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
No need to commit output of task because needsTaskCommit=false: attempt_20181002223312_0002_m_000000_2
[INFO ] 2018-10-02 22:33:13,610 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2). 709 bytes result sent to driver
[INFO ] 2018-10-02 22:33:13,620 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 2.0 (TID 2) in 758 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:33:13,620 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 2 (saveAsHadoopDataset at StoreMovieEssay.scala:94) finished in 0.748 s
[INFO ] 2018-10-02 22:33:13,621 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 2 finished: saveAsHadoopDataset at StoreMovieEssay.scala:94, took 0.792766 s
[WARN ] 2018-10-02 22:33:13,622 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:322)
Output Path is null in commitJob()
[INFO ] 2018-10-02 22:33:13,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty2
[INFO ] 2018-10-02 22:33:13,622 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:33:13,624 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490792000 ms.0 from job set of time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,625 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 1.623 s for time 1538490792000 ms (execution: 1.536 s)
[INFO ] 2018-10-02 22:33:13,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,627 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,628 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,629 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490792000 ms to writer queue
[INFO ] 2018-10-02 22:33:13,630 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490792000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490792000'
[INFO ] 2018-10-02 22:33:13,661 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490624000
[INFO ] 2018-10-02 22:33:13,662 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490792000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490792000', took 4786 bytes and 33 ms
[INFO ] 2018-10-02 22:33:13,665 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,667 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490792000 ms
[INFO ] 2018-10-02 22:33:13,670 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[INFO ] 2018-10-02 22:33:13,677 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Attempting to clear 0 old log files in file:/home/feng/software/code/bigdata/spark-warehouse/receivedBlockMetadata older than 1538490780000: 
[INFO ] 2018-10-02 22:33:13,679 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 22:33:21,810 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop(stopGracefully=true) from shutdown hook
[INFO ] 2018-10-02 22:33:21,812 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog shutting down at time: 1538490801812.
[WARN ] 2018-10-02 22:33:21,813 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
BatchedWriteAheadLog Writer queue interrupted.
[INFO ] 2018-10-02 22:33:21,814 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BatchedWriteAheadLog Writer thread exiting.
[INFO ] 2018-10-02 22:33:21,815 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped write ahead log manager
[INFO ] 2018-10-02 22:33:21,815 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ReceiverTracker stopped
[INFO ] 2018-10-02 22:33:21,816 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopping JobGenerator gracefully
[INFO ] 2018-10-02 22:33:21,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:33:21,817 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for all received blocks to be consumed for job generation
[INFO ] 2018-10-02 22:33:24,015 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490804000 ms.0 from job set of time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,016 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,020 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:33:24,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 3 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:33:24,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490804000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490804000'
[INFO ] 2018-10-02 22:33:24,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 3 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:33:24,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:33:24,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:33:24,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:33:24,026 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 1406.3 MB)
[INFO ] 2018-10-02 22:33:24,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_3_piece0 stored as bytes in memory (estimated size 2028.0 B, free 1406.3 MB)
[INFO ] 2018-10-02 22:33:24,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490804000 ms to writer queue
[INFO ] 2018-10-02 22:33:24,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_3_piece0 in memory on 192.168.0.100:44393 (size: 2028.0 B, free: 1406.3 MB)
[INFO ] 2018-10-02 22:33:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:33:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:33:24,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 3.0 with 1 tasks
[INFO ] 2018-10-02 22:33:24,043 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:33:24,048 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 3.0 (TID 3)
[INFO ] 2018-10-02 22:33:24,060 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 70 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:33:24,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_2_piece0 on 192.168.0.100:44393 in memory (size: 28.4 KB, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:24,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000.bk
[INFO ] 2018-10-02 22:33:24,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed broadcast_1_piece0 on 192.168.0.100:44393 in memory (size: 2043.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:24,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_8_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:24,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490804000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490804000', took 4829 bytes and 67 ms
[INFO ] 2018-10-02 22:33:24,089 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_8_0 in memory on 192.168.0.100:44393 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:24,098 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3). 1442 bytes result sent to driver
[INFO ] 2018-10-02 22:33:24,110 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 3.0 (TID 3) in 68 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:33:24,110 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:33:24,111 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 3 (isEmpty at StoreMovieEssay.scala:89) finished in 0.068 s
[INFO ] 2018-10-02 22:33:24,111 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 3 finished: isEmpty at StoreMovieEssay.scala:89, took 0.090432 s
[INFO ] 2018-10-02 22:33:24,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty8
[INFO ] 2018-10-02 22:33:24,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490804000 ms.0 from job set of time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,112 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.112 s for time 1538490804000 ms (execution: 0.097 s)
[INFO ] 2018-10-02 22:33:24,115 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2 from persistence list
[INFO ] 2018-10-02 22:33:24,118 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 2
[INFO ] 2018-10-02 22:33:24,125 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1 from persistence list
[INFO ] 2018-10-02 22:33:24,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 1
[INFO ] 2018-10-02 22:33:24,127 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0 from persistence list
[INFO ] 2018-10-02 22:33:24,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 0
[INFO ] 2018-10-02 22:33:24,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,129 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,131 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490804000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490804000'
[INFO ] 2018-10-02 22:33:24,132 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490804000 ms to writer queue
[INFO ] 2018-10-02 22:33:24,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490636000
[INFO ] 2018-10-02 22:33:24,158 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490804000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490804000', took 4785 bytes and 27 ms
[INFO ] 2018-10-02 22:33:24,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490804000 ms
[INFO ] 2018-10-02 22:33:24,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:33:24,167 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490804166 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:33:24,169 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:33:24,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 
[INFO ] 2018-10-02 22:33:36,001 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped timer for JobGenerator after time 1538490816000
[INFO ] 2018-10-02 22:33:36,017 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job streaming job 1538490816000 ms.0 from job set of time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,021 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting job: isEmpty at StoreMovieEssay.scala:89
[INFO ] 2018-10-02 22:33:36,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Got job 4 (isEmpty at StoreMovieEssay.scala:89) with 1 output partitions
[INFO ] 2018-10-02 22:33:36,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Final stage: ResultStage 4 (isEmpty at StoreMovieEssay.scala:89)
[INFO ] 2018-10-02 22:33:36,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped generation timer
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added jobs for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waiting for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:33:36,022 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Parents of final stage: List()
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,023 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Missing parents: List()
[INFO ] 2018-10-02 22:33:36,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:80), which has no missing parents
[INFO ] 2018-10-02 22:33:36,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490816000 ms to writer queue
[INFO ] 2018-10-02 22:33:36,024 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490816000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490816000'
[INFO ] 2018-10-02 22:33:36,027 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:36,031 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block broadcast_4_piece0 stored as bytes in memory (estimated size 2030.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:36,032 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added broadcast_4_piece0 in memory on 192.168.0.100:44393 (size: 2030.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:36,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] 2018-10-02 22:33:36,033 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at filter at StoreMovieEssay.scala:80) (first 15 tasks are for partitions Vector(0))
[INFO ] 2018-10-02 22:33:36,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Adding task set 4.0 with 1 tasks
[INFO ] 2018-10-02 22:33:36,034 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4714 bytes)
[INFO ] 2018-10-02 22:33:36,035 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Running task 0.0 in stage 4.0 (TID 4)
[INFO ] 2018-10-02 22:33:36,040 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Beginning offset 70 is the same as ending offset skipping test-flume-topic 0
[INFO ] 2018-10-02 22:33:36,049 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Block rdd_11_0 stored as bytes in memory (estimated size 0.0 B, free 1406.4 MB)
[INFO ] 2018-10-02 22:33:36,050 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Added rdd_11_0 in memory on 192.168.0.100:44393 (size: 0.0 B, free: 1406.4 MB)
[INFO ] 2018-10-02 22:33:36,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000.bk
[INFO ] 2018-10-02 22:33:36,057 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490816000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490816000', took 4821 bytes and 33 ms
[INFO ] 2018-10-02 22:33:36,068 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4). 1399 bytes result sent to driver
[INFO ] 2018-10-02 22:33:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished task 0.0 in stage 4.0 (TID 4) in 35 ms on localhost (executor driver) (1/1)
[INFO ] 2018-10-02 22:33:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] 2018-10-02 22:33:36,070 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
ResultStage 4 (isEmpty at StoreMovieEssay.scala:89) finished in 0.036 s
[INFO ] 2018-10-02 22:33:36,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Job 4 finished: isEmpty at StoreMovieEssay.scala:89, took 0.049252 s
[INFO ] 2018-10-02 22:33:36,071 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
rdd[list[list[Review]]] is empty11
[INFO ] 2018-10-02 22:33:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Finished job streaming job 1538490816000 ms.0 from job set of time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Total delay: 0.072 s for time 1538490816000 ms (execution: 0.056 s)
[INFO ] 2018-10-02 22:33:36,072 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8 from persistence list
[INFO ] 2018-10-02 22:33:36,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 8
[INFO ] 2018-10-02 22:33:36,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7 from persistence list
[INFO ] 2018-10-02 22:33:36,073 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 7
[INFO ] 2018-10-02 22:33:36,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6 from persistence list
[INFO ] 2018-10-02 22:33:36,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpointing graph for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updating checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,074 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Updated checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Removing RDD 6
[INFO ] 2018-10-02 22:33:36,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Submitted checkpoint of time 1538490816000 ms to writer queue
[INFO ] 2018-10-02 22:33:36,075 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Saving checkpoint for time 1538490816000 ms to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490816000'
[INFO ] 2018-10-02 22:33:36,085 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490732000
[INFO ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Checkpoint for time 1538490816000 ms saved to file 'file:/home/feng/software/code/bigdata/spark-warehouse/checkpoint-1538490816000', took 4785 bytes and 11 ms
[INFO ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Clearing checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Cleared checkpoint data for time 1538490816000 ms
[INFO ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting batches: 
[WARN ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87)
Exception thrown while writing record: BatchCleanupEvent(ArrayBuffer()) to the WriteAheadLog.
java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1538490816086 could be fulfilled.
	at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)
	at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:171)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:233)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:287)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:187)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[WARN ] 2018-10-02 22:33:36,086 org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
Failed to acknowledge batch clean up in the Write Ahead Log.
[INFO ] 2018-10-02 22:33:36,087 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
remove old batch metadata: 1538490792000 ms
[INFO ] 2018-10-02 22:33:36,124 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Waited for jobs to be processed and checkpoints to be written
[INFO ] 2018-10-02 22:33:36,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
CheckpointWriter executor terminated? true, waited for 0 ms.
[INFO ] 2018-10-02 22:33:36,126 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobGenerator
[INFO ] 2018-10-02 22:33:36,128 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped JobScheduler
[INFO ] 2018-10-02 22:33:36,138 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
StreamingContext stopped successfully
[INFO ] 2018-10-02 22:33:36,139 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Invoking stop() from shutdown hook
[INFO ] 2018-10-02 22:33:36,146 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Stopped Spark web UI at http://192.168.0.100:4040
[INFO ] 2018-10-02 22:33:36,154 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MapOutputTrackerMasterEndpoint stopped!
[INFO ] 2018-10-02 22:33:36,164 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
MemoryStore cleared
[INFO ] 2018-10-02 22:33:36,165 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManager stopped
[INFO ] 2018-10-02 22:33:36,166 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
BlockManagerMaster stopped
[INFO ] 2018-10-02 22:33:36,167 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
OutputCommitCoordinator stopped!
[INFO ] 2018-10-02 22:33:36,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Successfully stopped SparkContext
[INFO ] 2018-10-02 22:33:36,169 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Shutdown hook called
[INFO ] 2018-10-02 22:33:36,170 org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
Deleting directory /tmp/spark-77e63538-f1ff-41db-9d42-883f18c0ae85
[INFO ] 2018-10-02 23:20:56,111 sample.CommonTest$.main(CommonTest.scala:23)
Tue Oct 02 23:20:56 CST 2018
[INFO ] 2018-10-02 23:29:14,592 sample.CommonTest$.main(CommonTest.scala:27)
2018-10-02 23:29:14
