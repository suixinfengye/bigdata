
# Hadoop
[](https://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&mid=2650716244&idx=1&sn=8ae81c90633aa2deb010359782c7fa30&chksm=887da522bf0a2c34e1f4e1dd01cbed1007839cf226eb6984607fc33186a791ce1ad376ea0635&mpshare=1&scene=1&srcid=&pass_ticket=4GMBRspv7VZoP%2B4XbpwBpUc%2BhtGQ%2BqZuyRiMarCW4h0wzHuFkSshAIs0jmMVXXU6#rd)
<property>
  <name>dfs.datanode.sync.behind.writes</name>
  <value>true</value>
  <description>
        If this configuration is enabled, the datanode will instruct the
        operating system to enqueue all written data to the disk immediately
        after it is written. This differs from the usual OS policy which
        may wait for up to 30 seconds before triggering writeback.

        This may improve performance for some workloads by smoothing the
        IO profile for data written to disk.

        If the Hadoop native libraries are not available, this configuration
        has no effect.
  </description>
</property>

<property>
  <name>dfs.datanode.synconclose</name>
  <value>true</value>
</property>

<property>
  <name>dfs.namenode.avoid.read.stale.datanode</name>
  <value>true</value>
  <description>
      Indicate whether or not to avoid reading from "stale" datanodes whose heartbeat messages
      have not been received by the namenode for more than a specified time interval.
      Stale datanodes will be moved to the end of the node list returned for reading.
      See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.
  </description>
</property>

<property>
  <name>dfs.namenode.avoid.write.stale.datanode</name>
  <value>true</value>
</property>

# habse
越少HFile越利于read
越多HFile越利于write
只能优化read or write,Not both

# Tuning and Debugging in Apache Spark
https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38632
Filter input earlier in the program rather than later

#spark sql
dynamic resource allocation
启用动态分配就不用设置memory和core?

## https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/

Avoid reduceByKey When the input and output value types are different
better to use aggregateByKey

Avoid the flatMap-join-groupBy pattern
use cogroup

有观察到除了第一个job 本地化级别是any之外,其余都是process_local/node,但是task就只有一个,需要看看是不是repartition
rdd.partitions().size()

Whenever you have the power to make the decision about how data is stored on disk,
use an extensible binary format like Avro, Parquet, Thrif


内存调整
关于spark.memory.fraction  和 spark.memory.storageFraction
因为有persist 又有join 不好设置值

避免包装对象
The fastutil library provides convenient collection classes for primitive types that are compatible with the Java standard library

避免内部对象

数值/枚举优于字符串

开启指针压缩
If you have less than 32 GB of RAM, set the JVM flag -XX:+UseCompressedOops to make pointers be four bytes instead of eight.
You can add these options in spark-env.sh.

https://data-flair.training/blogs/spark-sql-performance-tuning/
i. spark.sql.codegen
ii. spark.sql.inMemorycolumnarStorage.compressed
iii. spark.sql.inMemoryColumnarStorage.batchSize
iv. spark.sql.parquet.compression.codec

#spark streaming
https://databricks.com/session/the-top-five-mistakes-made-when-writing-streaming-applications
client模式下,client die,driver die
cluster模式下,client die, driver don't die

receiver based sources的数据,要启用checkpoint和wal:spark.streaing.receiver.writeAheadLog.enable

# Everyday I’m Shuffling - Tips for Writing Better Spark Programs
https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38391
reduceByKey, aggregateByKey, foldByKey, and combineByKey,  preferred over groupByKey

jmc
spark.executor.extraJavaOptions    -XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=filename=executor.jfr,dumponexit=true,settings=profile

https://databricks.com/session/scalable-monitoring-using-prometheus-with-apache-spark-clusters
1.run explain plan:df.explain / spark ui
2.interpret plan
3.tune plan

A Deep Dive into the Catalyst Optimizer (Herman van Hovell)
https://www.youtube.com/watch?v=GDeePbbCz2g

A Deep Dive into the Catalyst Optimizer-Hands on Lab
https://databricks.com/session/a-deep-dive-into-the-catalyst-optimizer-hands-on-lab?utm_campaign=Spark%20Summit%20EU%202016&utm_content=34985851&utm_medium=social&utm_source=twitter


https://kernel.rubikloud.com/data-platform/how-to-improve-spark-sql-query-times/