# jvm
#visaulVm
cd /home/feng/software/visualvm_142/
./bin/visualvm

cd /usr/local/userlib/conf/jvm
jstatd -J-Djava.security.policy=/usr/local/userlib/conf/jvm/jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.0.101
 -J-Djava.rmi.server.logCalls=true &
jstatd -J-Djava.security.policy=/usr/local/userlib/conf/jvm/jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.0.107
 -J-Djava.rmi.server.logCalls=true
jstatd -J-Djava.security.policy=/usr/local/userlib/conf/jvm/jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.0.108
  -J-Djava.rmi.server.logCalls=true

add jms
-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9450 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false


SPARK_MASTER_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9448 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
SPARK_WORKER_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9450 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://spark1:9000/spark-logs -Dspark.history.fs.cleaner.enabled=true -Dspark.history.fs.cleaner.interval=2d -Dspark.history.fs.cleaner.maxAge=8d -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9452 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"

jmap -dump:format=b,file=/usr/local/userlib/spark-2.2/logs/heap_dump2.hprof 32623

# python
source /home/feng/software/python/venv/bin/activate
cd /media/feng/资源/bigdata/doubanSpider/doubanSpider/spiders
scrapy crawl doubanmovies -s JOBDIR=/media/feng/资源/bigdata/doubanSpider/file/job4

# flume
cd /home/feng/software/flume/
bin/flume-ng agent -n logser -c conf -f conf/flume_movie_eassy.conf


# kafka
cd /usr/local/userlib/kafka/bin
./kafka-server-start.sh -daemon ../config/server.properties
./kafka-topics.sh --create --zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181 --replication-factor 2
--partitions 2 --topic test11
./kafka-console-producer.sh --broker-list 192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092 --topic test1
./kafka-topics.sh --list --zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181
./kafka-console-consumer.sh --zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181 --topic mysql-clusterc.bigdata.movie_base_info --from-beginning
./kafka-topics.sh -zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181 --topic connect-configs-mysqld --describe
# ./kafka-topics.sh --list --zookeeper 192.168.0.100:2181
# ./kafka-console-producer.sh --broker-list 192.168.0.100:9092 --topic mysqlfullfillment.test.doulist
# ./kafka-console-consumer.sh --zookeeper 192.168.0.100:2181 --topic mysqlfullfillment.test.doulist
./kafka-topics.sh --delete --zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181 --topic movie-essay-topic


./kafka-topics.sh --create --zookeeper 192.168.0.101:2181,192.168.0.107:2181,192.168.0.108:2181 --replication-factor 2  --partitions 1 --topic connect-configs-mysqld1

# Debezium
cd /usr/local/userlib/kafka
bin/connect-distributed.sh config/connect-distributed-mysql.properties
bin/connect-standalone.sh config/connect-standalone.properties config/mysql-local.properties



http://192.168.0.101:8083/connectors
 {
   	"name": "mysql-clusterd1",
	"config": {
		"connector.class": "io.debezium.connector.mysql.MySqlConnector",
		"database.hostname": "192.168.0.100",
		"database.port": "3306",
		"database.user": "debezium",
		"database.password": "feng",
		"database.server.id": "18410",
		"database.server.name": "mysql-clusterd1",
		"database.whitelist": "bigdata",
		"table.blacklist":"bigdata.trick_url,bigdata.steaming_record",
		"database.history.kafka.bootstrap.servers": "192.168.0.101:9092,192.168.0.107:9092,192.168.0.108:9092",
		"database.history.kafka.topic": "cluster-bigdatad1",
		"include.schema.changes": "true",
		"transforms": "unwrap",
		"transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
      "decimal.handling.modestring":"double"
    }
  }

# mysql
mysql -h192.168.0.100 -p3306 -udebezium -p

# bigdata
zkServer.sh start
zkCli.sh -server 192.168.0.101

start-dfs.sh
start-yarn.sh
start-hbase.sh
cd /usr/local/userlib/spark-2.2/
./sbin/start-history-server.sh
./sbin/start-all.sh

hdfs haadmin -getServiceState nn1
hdfs getconf -confKey dfs.namenode.avoid.read.stale.datanode
hbase-daemons.sh start regionserver
hbase-daemon.sh stop regionserver RegionServer
# phoenix
/usr/local/userlib/phoenix-4.14.0-HBase-1.4/bin/sqlline.py spark1:2181


# hbase
truncate 'tablename'
# redis
ps -ef | grep redis
netstat -tnlp | grep redis
cd /usr/local/userlib/redis-3.2.12
src/redis-cli -h 192.168.0.101 -p 7000 shutdown
src/redis-cli -h 192.168.0.101 -p 7001 shutdown
src/redis-cli -h 192.168.0.107 -p 7000 shutdown
src/redis-cli -h 192.168.0.107 -p 7001 shutdown
src/redis-cli -h 192.168.0.108 -p 7000 shutdown
src/redis-cli -h 192.168.0.108 -p 7001 shutdown

#每个节点
cd /usr/local/userlib/redis-3.2.12
src/redis-server cluster-redis/7000/redis.conf
src/redis-server cluster-redis/7001/redis.conf

src/redis-cli -h 192.168.0.101 -c -p 7000
cluster nodes
cluster info
info


source /home/feng/software/code/python/RedisLive/venv/bin/activate
cd /home/feng/software/code/python/RedisLive/redis-monitor/src/
./redis_monitor.py

yum install curl
curl -L get.rvm.io | bash -s stable
usermod -a -G rvm root
source /usr/local/rvm/scripts/rvm
rvm list known
yum -y update nss
rvm install 2.4.4
rvm use 2.4.4
rvm remove 1.8.7
ruby --version

src/redis-trib.rb  create  --replicas  1 192.168.0.101:7000 192.168.0.101:7001 192.168.0.107:7000 192.168.0.107:7001 192.168.0.108:7000 192.168.0.108:7001

https://blog.csdn.net/chenxinchongcn/article/details/78666374?utm_source=blogxgwz0
https://www.cnblogs.com/carryping/p/7447823.html
https://blog.csdn.net/lsxf_xin/article/details/79442198
https://blog.csdn.net/huwh_/article/details/79242625

#指定 redis 只接收来自于该 IP 地址的请求，如果不进行设置，那么将处理所有请求
bind 127.0.0.1


#spark
start-shuffle-service.sh #动态资源分配用到外部服务进程

grep -rin "23:12:18" executor.log
cat executor.log | tail -n +60000 | head -n 10000  > temp
